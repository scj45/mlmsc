{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrLGCab5VQmw",
        "cell_id": "c668c72769fc424ebe33c3a783ab2706",
        "deepnote_cell_type": "markdown"
      },
      "source": "# Get English Data",
      "block_group": "d521c08dfce748c3a7ac572bb3cc37b7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "c6a7029592bd4255aab06b2cdc9c14ef",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Text preprocessing functions",
      "block_group": "eea96532e88045e1b083ae389c5e41c7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRxUfmoPU9yM",
        "cell_id": "7c7d6ce3ea32453ebd7bfe2012ad921e",
        "deepnote_cell_type": "code"
      },
      "source": "# download and preprocess text data from Project Gutenberg via Kaggle\nimport requests\nimport re\nimport glob\nimport os\nimport pandas as pd\n\n# function for text extraction\ndef extraction(raw):\n    START = \"*** START\"\n    END = \"*** END\"\n    start = raw.find(START)\n    end = raw.find(END)\n    if start != -1 and end != -1 and end > start:\n        return raw[start:end]\n    else:\n        return raw  # Fallback if tags are not found\n\n# strip the Gutenberg header and footer\ndef strip(text):\n    start_pattern = r\"\\*{3} START OF THIS PROJECT GUTENBERG EBOOK .* \\*{3}\"\n    end_pattern = r\"\\*{3} END OF THIS PROJECT GUTENBERG EBOOK .* \\*{3}\"\n    start = re.search(start_pattern, text)\n    end = re.search(end_pattern, text)\n    if start and end:\n        return text[start.end():end.start()]\n    # Removes only leading/trailing whitespaces, preserves all internal whitespace\n    return text.strip()\n\n# preprocess the text data, returning list of characters for char mode, else list of words\ndef preprocess(text, mode='char'):\n    text = text.lower()\n    text = re.sub(r'[^a-z0-9\\s.,;!?\\'\"-]', ' ', text)\n    if mode == 'char':\n        return [c for c in text if c.isalpha() or c in ' .,;!?\\'\"-']\n    elif mode == 'word':\n        return re.findall(r'\\b\\w+\\b', text)\n    else:\n        raise ValueError(\"mode must be 'char' or 'word'\")",
      "block_group": "39eb5da3c7b54688a1f973d5261b36e5",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "29a9946b0e9446f1bc7ca1f7641a8aa5",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Get data and preprocess",
      "block_group": "bc0a1fde4d43437093ce23c70be40825"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcnG4fqPGyza",
        "executionInfo": {
          "user": {
            "userId": "04808690080138473040",
            "displayName": "Stephanie Jat"
          },
          "status": "ok",
          "elapsed": 4770,
          "user_tz": 0,
          "timestamp": 1762806123373
        },
        "cell_id": "10954d04744248128c05099382b7d483",
        "deepnote_cell_type": "code"
      },
      "source": "# download and preprocess text data from Project Gutenberg\nimport requests\nimport re\nimport glob\nimport os\nimport pandas as pd\n\ngutenberg_top20 = [\n    (1342, \"Pride_and_Prejudice\"),\n    (84, \"Frankenstein\"),\n    (1661, \"Adventures_of_Sherlock_Holmes\"),\n    (11, \"Alices_Adventures_in_Wonderland\"),\n    (98, \"A_Tale_of_Two_Cities\"),\n    (2701, \"Moby_Dick\"),\n    (76, \"Adventures_of_Huckleberry_Finn\"),\n    (2542, \"A_Dolls_House\"),\n    (5200, \"Metamorphosis\"),\n    (120, \"Peter_Pan\"),\n    (74, \"The_Adventures_of_Tom_Sawyer\"),\n    (1400, \"Great_Expectations\"),\n    (160, \"The_Yellow_Wallpaper\"),\n    (23, \"Narrative_of_Frederick_Douglass\"),\n    (219, \"The_Picture_of_Dorian_Gray\"),\n    (4300, \"Ulysses\"),\n    (46, \"A_Christmas_Carol\"),\n    (166, \"The_Iliad\"),\n    (1013, \"Anna_Karenina\"),\n    (1952, \"War_and_Peace\"),\n]\n\n# function for text extraction\ndef extraction(raw):\n    START = \"*** START\"\n    END = \"*** END\"\n    start = raw.find(START)\n    end = raw.find(END)\n    if start != -1 and end != -1 and end > start:\n        return raw[start:end]\n    else:\n        return raw  # Fallback if tags are not found\n\n# strip the Gutenberg header and footer\ndef strip(text):\n    start_pattern = r\"\\*{3} START OF THIS PROJECT GUTENBERG EBOOK .* \\*{3}\"\n    end_pattern = r\"\\*{3} END OF THIS PROJECT GUTENBERG EBOOK .* \\*{3}\"\n    start = re.search(start_pattern, text)\n    end = re.search(end_pattern, text)\n    if start and end:\n        return text[start.end():end.start()]\n    # Removes only leading/trailing whitespaces, preserves all internal whitespace\n    return text.strip()\n\n# preprocess the text data, returning list of characters for char mode, else list of words\ndef preprocess(text, mode='char'):\n    text = text.lower()\n    text = re.sub(r'[^a-z0-9\\s.,;!?\\'\"-]', ' ', text)\n    if mode == 'char':\n        return [c for c in text if c.isalpha() or c in ' .,;!?\\'\"-']\n    elif mode == 'word':\n        return re.findall(r'\\b\\w+\\b', text)\n    else:\n        raise ValueError(\"mode must be 'char' or 'word'\")\n\n# --- Data loading ----- #\nbooks_dir = \"gutenberg_top20\"\nos.makedirs(books_dir, exist_ok=True)\ncorpus = []\n\n# download texts\nfor book_id, book_name in gutenberg_top20:\n    fname = os.path.join(books_dir, f\"{book_id}_{book_name}.txt\")\n    if not os.path.exists(fname):\n        url = f\"https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt\"\n        try:\n            response = requests.get(url)\n            if response.status_code == 200:\n                with open(fname, \"w\", encoding=\"utf-8\") as f:\n                    f.write(response.text)\n            else:\n                print(f\"Download failed for {book_name} ({book_id}) status {response.status_code}\")\n        except Exception as e:\n            print(f\"Error downloading {book_name} ({book_id}): {e}\")\n\n# extraction and preprocessing\nfor book_id, book_name in gutenberg_top20:\n    fname = os.path.join(books_dir, f\"{book_id}_{book_name}.txt\")\n    try:\n        with open(fname, encoding=\"utf-8\") as f:\n            raw = f.read()\n            content = extraction(raw)\n            stripped = strip(content)\n            chars = preprocess(stripped, mode='char')\n            corpus.append({\"id\": book_id, \"name\": book_name, \"chars\": chars})\n    except Exception as e:\n        print(f\"Error with {fname}: {e}\")\n",
      "block_group": "dc54a807ab6547719cbb8e488f13e3a7",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdcWrT2MVWsC",
        "cell_id": "401ee09b4fc94d8aaacce588d042ceab",
        "deepnote_cell_type": "markdown"
      },
      "source": "# Probability Distributions",
      "block_group": "8143f504a46f4296b1d49a7b08213f34"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCU13V1zVc2k",
        "colab": {
          "height": 1000,
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9c54406-b308-4a55-a55a-b3060c8580b6",
        "executionInfo": {
          "user": {
            "userId": "04808690080138473040",
            "displayName": "Stephanie Jat"
          },
          "status": "ok",
          "elapsed": 10294,
          "user_tz": 0,
          "timestamp": 1762806196624
        },
        "cell_id": "040edac8a9324ae6a0dd2b38de3570c4",
        "deepnote_cell_type": "code"
      },
      "source": "# calculate the stationary distribution and transition matrix for the English language\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# get the data from the engtextdata script (contains the text data from the Project Gutenberg books)\ndata = corpus\n# data is a list of dicts from corpus build\nif isinstance(data, list) and isinstance(data[0], dict) and \"chars\" in data[0]:\n    # concatenate all character lists into one big string\n    text = ''.join(''.join(book['chars']) for book in data)\nelif isinstance(data, list):\n    # List of strings (full book texts) â€” not your current corpus structure, but useful elsewhere\n    text = ''.join(data)\nelif isinstance(data, dict):\n    # Dict of {'id_or_name': {'chars': [...]}} mappings\n    text = ''.join(''.join(v['chars']) for v in data.values())\nelse:\n    # Already a string or unknown\n    text = str(data)\n\nchars = list(text)\n\n# get the states and counts\nwith open('/datasets/t1cw-data/symbols.txt', 'r') as f:\n    states = [line.rstrip('\\n') for line in f if line.rstrip('\\n') != ''] # create a list of unique states\nsequence = [char for char in text if char in states]\ncounts = {beta: defaultdict(int) for beta in states} # create a dictionary to store the counts of each state\n# loop through the sequence and count the number of times each state appears after the previous state\nfor i in range(len(sequence)-1):\n    alpha, beta = sequence[i], sequence[i+1]\n    counts[beta][alpha] += 1\n\n# initialise probabilities as vector and matrix\nn = len(states)\npi = np.zeros((n,)) # stationary distribution is a row vector of size (1xn)\npsi = np.zeros((n, n)) # transition matrix: psi[j, i] is the probability of beta (j) given alpha (i), i.e. P(states[j]|states[i])\n\nepsilon = 0.5           # Laplace smoothing to avoid zeros\n\nfor j, beta in enumerate(states):         # for each \"next\" state\n    total = sum(counts[beta][alpha] for alpha in states) + n * epsilon\n    for i, alpha in enumerate(states):    # for each \"previous\" state\n        # Use integer indices for numpy arrays!\n        psi[j, i] = (counts[beta][alpha] + epsilon) / total\n\n# calculate the stationary distribution\nevals, evecs = np.linalg.eig(psi) # find the eigenvectors and eigenvalues of the transition matrix\npi = np.real(evecs[:, np.isclose(evals, 1)]) # find the eigenvector corresponding to eigenvalue 1\npi = pi.squeeze() # squeeze the stationary distribution to remove any extra dimensions\npi = pi / pi.sum()  # normalize the stationary distribution\n\n# print the stationary distribution\nstationary_df = pd.DataFrame({'State': states, 'Stationary probability': pi})\nprint(stationary_df.to_string(index=False)) # print as a readable table\n\n# plot the stationary distribution\nplt.figure(figsize=(12, 2))\nplt.imshow(pi[np.newaxis, :], cmap='hot', aspect='auto')\nplt.colorbar(label='Stationary Probability')\nplt.xticks(np.arange(len(states)), states, rotation=90)\nplt.yticks([0], ['Stationary'])\nplt.title('Q.5(a) Stationary Distribution Heatmap')\nplt.tight_layout()\nplt.show()\n\n# print the transition matrix as a readable table\ntransition_df = pd.DataFrame(psi, index=states, columns=states) # create a dataframe from the transition matrix\nprint(transition_df.round(4).to_string()) # print the transition table with 4 decimals\n\n# plot the transition matrix\nplt.figure(figsize=(10, 8))\nplt.imshow(psi, cmap='hot', aspect='auto') # plot the transition matrix as a heatmap\nplt.colorbar(label='Transition Probability')\nplt.xticks(np.arange(len(states)), states, rotation=90) # set the x-axis labels to the states\nplt.yticks(np.arange(len(states)), states) # set the y-axis labels to the states\nplt.title('Q.5(a) English Language Transition Matrix Heatmap (Hot Colormap)') # set the title of the plot\nplt.xlabel('Beta State') # set the x-axis label\nplt.ylabel('Alpha State') # set the y-axis label\nplt.tight_layout() # adjust the layout of the plot\nplt.show() # show the plot",
      "block_group": "e9d32758bfa945348141a295b08cd28b",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQH3-hvQViws",
        "cell_id": "5f39b119b2464e7d91e0b0f224545eff",
        "deepnote_cell_type": "markdown"
      },
      "source": "# MH",
      "block_group": "3c7beae0f2044822b01675b716b44f4f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww1C1c91VmhE",
        "colab": {
          "height": 1000,
          "base_uri": "https://localhost:8080/",
          "background_save": true
        },
        "cell_id": "5068db1b9bbb4d81974f83f8d76a9012",
        "deepnote_cell_type": "code"
      },
      "source": "# Metropolis-Hastings Implementation\n\nimport numpy as np\nimport random\nimport sys\nimport matplotlib.pyplot as plt\n\n# get the states from symbols.txt\nwith open('/datasets/t1cw-data/symbols.txt', 'r') as f:\n    states = [line.rstrip('\\n') for line in f if line.rstrip('\\n') != ''] # create a list of unique states\nstate_to_idx = {s: i for i, s in enumerate(states)} # state-to-index mapping for ease\nidx_to_state = {i: s for i, s in enumerate(states)}\n\n# get the encrypted text from message.txt\nwith open('/datasets/t1cw-data/message.txt', 'r') as g:\n    encrypted_text = g.read() # Read the entire file as a string\nencrypted_sequence = [char for char in encrypted_text if char in states] # make it a list of permitted characters\nencrypted_idx_seq = [state_to_idx[c] for c in encrypted_sequence] # index\n\n\ndef log_likelihood(decoded_sequence, psi, state_to_idx): # Calculate log likelihood\n    total = 0.0\n    for prev, curr in zip(decoded_sequence[:-1], decoded_sequence[1:]):\n        i = state_to_idx[prev]\n        j = state_to_idx[curr]\n        p = psi[j, i]\n        if p > 0:\n            total += np.log(p)\n        else:\n            # smoothing: assign large negative if impossible transition to protect ergodicity\n            total += -15  # Penalize impossible transitions\n    return total\n\ndef decrypt_idx_sequence(seq_idx, best_perm, idx_to_state):\n    # For each symbol's index in encrypted sequence, get mapped plaintext index, then symbol\n    return [idx_to_state[best_perm[i]] for i in seq_idx]\n\nchains = 10\nsteps = 50000\nburn_in = 400 #set burn-in based on where log graph tapers when steps = 1000\n\nall_best_logs = []\nall_best_deciphers = []\nbest_overall_log = float('-inf')\nbest_overall_perm = None\n\nn = len(states)\nfor chain in range(chains):\n    # Random or stationary-matching initialization\n    current_perm_idx = random.sample(range(n), n)\n    log_trace, trace = [], []\n    best_chain_perm, best_chain_log = None, float('-inf')\n\n    for step in range(steps):\n        i, j = random.sample(range(n), 2)\n        proposed_perm = current_perm_idx.copy()\n        proposed_perm[i], proposed_perm[j] = proposed_perm[j], proposed_perm[i]\n        decoded_current = decrypt_idx_sequence(encrypted_idx_seq, current_perm_idx, idx_to_state)\n        decoded_proposed = decrypt_idx_sequence(encrypted_idx_seq, proposed_perm, idx_to_state)\n        log_current = log_likelihood(decoded_current, psi, state_to_idx)\n        log_proposed = log_likelihood(decoded_proposed, psi, state_to_idx)\n        # Metropolis-Hastings acceptance\n        accept_prob = 1 if (log_proposed - log_current) > 100 else min(1, np.exp(log_proposed - log_current))\n        if random.random() < accept_prob:\n            current_perm_idx = proposed_perm\n        # Save best perm for this chain\n        if log_current > best_chain_log:\n            best_chain_log = log_current\n            best_chain_perm = current_perm_idx.copy()\n        trace.append(current_perm_idx.copy())\n        log_trace.append(log_current)\n        # PRINT decrypted first 60 chars every 100 steps\n        if step % 100 == 0:\n            print(f\"Chain {chain+1}, Step {step}: {''.join(decoded_current)[:60]}\")\n\n    # ===== AFTER ALL STEPS FOR THIS CHAIN: =====\n    log_trace_burned = log_trace[burn_in:]  # Remove burn-in if you want\n\n    # After each chain: print, plot, save\n    if best_chain_perm is not None:\n        deciphered = decrypt_idx_sequence(encrypted_idx_seq, best_chain_perm, idx_to_state)\n        all_best_logs.append(best_chain_log)\n        all_best_deciphers.append(''.join(deciphered))\n        if best_chain_log > best_overall_log:\n            best_overall_log = best_chain_log\n            best_overall_perm = best_chain_perm.copy()\n        # Now plot trace/output for this chain\n        plt.plot(log_trace)\n        plt.xlabel('Iteration')\n        plt.ylabel('Log-Likelihood')\n        plt.title(f'MH Log-Likelihood Trace Plot (Chain {chain+1})')\n        plt.show()\n        with open('/datasets/t1cw-data/deciphered.txt', 'w') as out_file:\n            out_file.write(''.join(deciphered))\n\n# After all chains: print/save best of best\nif best_overall_perm is not None:\n    final_deciphered = decrypt_idx_sequence(encrypted_idx_seq, best_overall_perm, idx_to_state)\n    print('Final best decipher:', ''.join(final_deciphered[:100]))\n",
      "block_group": "f492addd96d1429883266934675c17c0",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=cb182644-878e-48cb-992b-68a78a5afe3d' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_full_width": true,
    "deepnote_notebook_id": "3ac313f6019b4b498b1acade2a0bd268"
  }
}