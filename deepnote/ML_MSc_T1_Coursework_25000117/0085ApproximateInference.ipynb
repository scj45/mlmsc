{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "b673c98b4c77485a856e6ae377e769ea",
        "deepnote_cell_type": "text-cell-h1"
      },
      "source": "# Bayesian linear and GP regression",
      "block_group": "723398ad4a7347c68776d4d37059a961"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "99b5f6effc984a5ba30bfcd8bb943256",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Load the NOAA CO2 file",
      "block_group": "bef54408f1574b6099d19f65e464003f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "36e73098",
        "execution_start": 1768601894165,
        "execution_millis": 382,
        "execution_context_id": "9f69f681-c681-435c-ac60-bb3a595e8f6f",
        "cell_id": "deb731e4b61f487db48899262beba31b",
        "deepnote_cell_type": "code"
      },
      "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Read all lines, then drop comment lines starting with '#'\ndf = pd.read_fwf(\n    \"/work/scj-gdrive/co2.txt\",\n    widths=[6, 6, 12, 10, 10],\n    header=None,\n    comment=\"#\",\n    names=[\"year\", \"month\", \"t\", \"y\", \"trend\"]\n)\n\n# Drop any non-numeric rows (e.g. leftover 'year month decimal ...' line)\ndf = df[pd.to_numeric(df[\"t\"], errors=\"coerce\").notna()].copy()\ndf[\"t\"] = df[\"t\"].astype(float)\ndf[\"y\"] = df[\"y\"].astype(float)\n\n# Time and data\nt = df[\"t\"].values\ny = df[\"y\"].values\n\n# Training mask and splits\nmask_train = t <= 2007 + 8/12         # shape (N,)\nt_train = t[mask_train]               # shape (N_train,)",
      "block_group": "a747119fb3374be5899752651d719452",
      "execution_count": 146,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "b9be2a6d91ef485b979419fd69df1820",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Mean and covariance",
      "block_group": "44cfe6639279437fb4c2e35b40ef46e5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "b7cd635a",
        "execution_start": 1768601896479,
        "execution_millis": 1,
        "execution_context_id": "9f69f681-c681-435c-ac60-bb3a595e8f6f",
        "cell_id": "83cff7c24af64e459287295ca99ba031",
        "deepnote_cell_type": "code"
      },
      "source": "Phi = np.c_[t,np.ones(len(t))]\nm0, S0 = np.array([0.,360.]), np.diag([1e4,1e4])\nbeta = 1.0\nSN=np.linalg.inv(np.linalg.inv(S0)+beta*Phi.T@Phi) \nmN=SN@(np.linalg.inv(S0)@m0 + beta*Phi.T@y)\n\nprint('Mean:',mN)\nprint('Cov:',SN)\nprint('Std:',np.sqrt(np.diag(SN)))",
      "block_group": "39eb49f221b245a6af0df7fae93b2fe6",
      "execution_count": 149,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "c1b68efc85b342b5ade1d166c1f4424d",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## MAP estimates ",
      "block_group": "e52a621cde374b16be50e4f2c58f3f40"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "952722a7",
        "execution_start": 1768601899119,
        "execution_millis": 1,
        "execution_context_id": "9f69f681-c681-435c-ac60-bb3a595e8f6f",
        "cell_id": "95a71d8e1e234587a225e98280d0e56e",
        "deepnote_cell_type": "code"
      },
      "source": "sigma2 = 1.0\nbeta = 1.0 / sigma2\n\na_MAP, b_MAP = mN\nprint(\"a_MAP:\", a_MAP)\nprint(\"b_MAP:\", b_MAP)\n\n# Compute residuals g_obs(t)\ng_obs = y - (a_MAP * t + b_MAP)\ng_train = g_obs[mask_train]               # shape (N_train,)\n\n# Quick diagnostics (optional)\nprint(\"Residual mean:\", np.mean(g_obs))\nprint(\"Residual std:\", np.std(g_obs))\n\n# Plot residuals over time\nplt.figure(figsize=(10, 4))\nplt.plot(df[\"t\"].values, g_obs, \"b-\", markersize=3)\nplt.axhline(0.0, color=\"r\", linestyle=\"--\", linewidth=1)\nplt.xlabel(\"Calendar time (decimal year)\")\nplt.ylabel(\"Residual g_obs(t) [ppm]\")\nplt.title(\"CO2 residuals g_obs(t) = y(t) - (a_MAP t + b_MAP)\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()",
      "block_group": "76db5ab28fdc47ed9b34b3e35821b776",
      "execution_count": 152,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "3b1dfe90",
        "execution_start": 1768602133015,
        "execution_millis": 109,
        "execution_context_id": "9f69f681-c681-435c-ac60-bb3a595e8f6f",
        "cell_id": "3a31a696bc0144c58cb67bbc6db37ef2",
        "deepnote_cell_type": "code"
      },
      "source": "from scipy.stats import norm\n\nres = g_train  # or g_obs\n\n# histogram of residuals\nplt.figure(figsize=(6, 4))\ncount, bins, _ = plt.hist(\n    res,\n    bins=30,\n    density=True,\n    alpha=0.7,\n    color=\"C0\",\n    label=\"residuals\",\n)\n\n# grid for normal pdf\nxs = np.linspace(bins[0], bins[-1], 400)\n\n# standard normal N(0,1); or fit mean/std of residuals instead\npdf_std = norm.pdf(xs, loc=0.0, scale=1.0)\nplt.plot(xs, pdf_std, \"r-\", linewidth=2, label=\"e(t) ~ N(0,1)\")\n\nplt.xlabel(\"residual bin\")\nplt.ylabel(\"density\")\nplt.title(\"CO2 Residuals Density\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()",
      "block_group": "60879803926c43bcad4f43f948345a7f",
      "execution_count": 161,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "3f618ac716374f73ae686a36aa1ee6d1",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Draw samples from a GP",
      "block_group": "7a100d1189f044a780022ddadcf1c43d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "8da02ee9",
        "execution_start": 1768644783935,
        "execution_millis": 1,
        "execution_context_id": "480cf8cf-251f-406e-a5c8-fe90e9f8b0fe",
        "cell_id": "7ac4d640f69348fa82baeee21debae62",
        "deepnote_cell_type": "code"
      },
      "source": "def sample_gp(x, kernel, params, jitter=1e-6, random_state=None):\n    \"\"\"\n    Draw one sample f(x) from a zero-mean GP with covariance kernel k.\n\n    Parameters\n    ----------\n    x : array_like, shape (N,)\n        Input points where the function is evaluated.\n    kernel : callable\n        Covariance function k(s, t, *kernel_params) -> scalar.\n    *kernel_params :\n        Hyperparameters passed to the kernel.\n    jitter : float, optional\n        Small diagonal term for numerical stability.\n    random_state : int or None, optional\n        Seed for reproducibility.\n\n    Returns\n    -------\n    f : ndarray, shape (N,)\n        Sampled function values f(x) at the given inputs.\n    \"\"\"\n    x = np.asarray(x)\n    N = x.shape[0]\n\n    # build covariance matrix K_ij = k(x_i, x_j)\n    K = kernel(x, x, **params)\n    \n    # add jitter to ensure positive definiteness\n    K += jitter * np.eye(N)\n\n    rng = np.random.default_rng(random_state)\n    f = rng.multivariate_normal(mean=np.zeros(N), cov=K)\n\n    return f",
      "block_group": "76368c1760eb4380aad6389112805e93",
      "execution_count": 22,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "b9b77812cc0e4ff283baf207f1a6e965",
        "deepnote_cell_type": "text-cell-h3"
      },
      "source": "### Samples from example RBF kernel",
      "block_group": "13c94f791f9c4259acc07ef1f541fa0d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "dfbb3baf",
        "execution_start": 1768644844335,
        "execution_millis": 9250,
        "execution_context_id": "480cf8cf-251f-406e-a5c8-fe90e9f8b0fe",
        "cell_id": "da339d908a6240de81f91928db18fb36",
        "deepnote_cell_type": "code"
      },
      "source": "def rbf_kernel(s, t, lengthscale, variance):\n    s = np.asarray(s)\n    t = np.asarray(t)\n    S, T = np.meshgrid(s, t, indexing=\"ij\")  # both (N,M)\n    diff = S - T\n    return variance * np.exp(-0.5 * diff**2 / lengthscale**2)\n\nx = np.linspace(0, 10, 200)\n\nparams1 = dict(lengthscale=1.0, variance=1.0)\nparams2 = dict(lengthscale=1.0, variance=1.0)\nparams3 = dict(lengthscale=0.3, variance=1.0)  # shorter lengthscale\n\nf1 = sample_gp(x, rbf_kernel, params1, random_state=0)\nf2 = sample_gp(x, rbf_kernel, params2, random_state=1)\nf3 = sample_gp(x, rbf_kernel, params3, random_state=2)\n\nplt.figure(figsize=(8, 4))\nplt.plot(x, f1, label=\"ℓ = 1.0, sample 1\")\nplt.plot(x, f2, label=\"ℓ = 1.0, sample 2\")\nplt.plot(x, f3, label=\"ℓ = 0.3, sample 3\")\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.title(\"Samples from a zero-mean GP (example kernel)\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()",
      "block_group": "ff711e872cd54f5c9b89bdcc616f18e8",
      "execution_count": 28,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "f043d5a981b447d2ba7ab5d660ca30b1",
        "deepnote_cell_type": "text-cell-h3"
      },
      "source": "### Samples from matern periodic kernel",
      "block_group": "66da3d6bed4f432c8365d7697a622fa4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "a243f067",
        "execution_start": 1768644587035,
        "execution_millis": 1,
        "execution_context_id": "480cf8cf-251f-406e-a5c8-fe90e9f8b0fe",
        "cell_id": "c72c6c2887e44c988b45005134320a40",
        "deepnote_cell_type": "code"
      },
      "source": "def kst(s, t, theta, tau, sigma, phi, eta, zeta):\n    \"\"\"\n    s, t : arrays of shape (N,) and (M,)\n    returns K of shape (N, M)\n    \"\"\"\n    s = np.asarray(s)\n    t = np.asarray(t)\n\n    # pairwise differences\n    S, T = np.meshgrid(s, t, indexing=\"ij\") # S, T have shape (N, M)\n    diff = S - T\n\n    # periodic part\n    periodic = np.exp(-2 * np.sin(np.pi * diff / tau) ** 2 / sigma ** 2)\n    # squared‑exponential part\n    se = phi**2 * np.exp(-diff**2 / (2 * eta**2))\n    \n    # nugget\n    K = theta**2 * (periodic + se)\n\n    if zeta != 0.0:\n        N = s.shape[0]\n        if s.shape[0] == t.shape[0] and np.allclose(s, t):\n            K = K + zeta**2 * np.eye(s.shape[0])\n    return K",
      "block_group": "34e0a49763214bf1aca11217fff585cd",
      "execution_count": 10,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "76a7ae37",
        "execution_start": 1768644892235,
        "execution_millis": 44527,
        "execution_context_id": "480cf8cf-251f-406e-a5c8-fe90e9f8b0fe",
        "cell_id": "93db1f22fd10461795edd87682279a28",
        "deepnote_cell_type": "code"
      },
      "source": "xs = np.linspace(0, 10, 300)\n\nparam_sets = [\n    dict(theta=1.0, sigma=0.7, tau=1.0, phi=0.5, eta=2.0, zeta=0.0),\n    dict(theta=1.0, sigma=0.2, tau=1.0, phi=0.5, eta=2.0, zeta=0.0),  # sharper periodic\n    dict(theta=1.0, sigma=0.7, tau=2.0, phi=0.5, eta=2.0, zeta=0.0),  # longer period\n    dict(theta=1.0, sigma=0.7, tau=1.0, phi=1.5, eta=0.5, zeta=0.0),  # more local SE\n]\n\nplt.figure(figsize=(10, 8))\n\nfor i, params in enumerate(param_sets, 1):\n    f = sample_gp(xs, kst, params, random_state= i)\n    plt.subplot(len(param_sets), 1, i)\n    plt.plot(xs, f)\n    plt.grid(True)\n    plt.ylabel(\"f(t)\")\n    txt = \", \".join(f\"{k}={v}\" for k, v in params.items())\n    plt.title(\"Samples from matern periodic kernel, hyperparameters:\" + txt)\n\nplt.xlabel(\"t\")\nplt.tight_layout()\nplt.show()",
      "block_group": "59d0da22368e430a8fa3bc4362533693",
      "execution_count": 31,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "ddedca3d63b64aff95b8c637f81f22f9",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Test hyperparameters",
      "block_group": "6789637708a4418e9e804e08f34f2511"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "12cae4fe7e344b0c8fedd7cf8e7e963a",
        "deepnote_cell_type": "text-cell-h3"
      },
      "source": "### Minimise the negative log marginal likelihood to optimise hyperparameters",
      "block_group": "38dddd19027f4f68aac226b162877c57"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "6b004f53",
        "execution_start": 1768642417065,
        "execution_millis": 1950446,
        "execution_context_id": "480cf8cf-251f-406e-a5c8-fe90e9f8b0fe",
        "cell_id": "44f6102bb3ec453798c9cd76b28d30e8",
        "deepnote_cell_type": "code"
      },
      "source": "def nlml(p, t_train, g_train, jitter=1e-6):\n    log_theta, log_tau, log_sigma, log_phi, log_eta, log_zeta = p\n\n    # build params dict correctly\n    params = dict(\n        theta=np.exp(log_theta),\n        tau=np.exp(log_tau),\n        sigma=np.exp(log_sigma),\n        phi=np.exp(log_phi),\n        eta=np.exp(log_eta),\n        zeta=np.exp(log_zeta),\n    )\n\n    # kst must have signature kst(s, t, **params)\n    K = kst(t_train, t_train, **params)\n    K = K + jitter * np.eye(len(t_train))\n\n    # Cholesky for stability: K = L L^T\n    L = np.linalg.cholesky(K)\n    # Solve K^{-1} g via two triangular solves\n    alpha = np.linalg.solve(L.T, np.linalg.solve(L, g_train))\n\n    data_fit = 0.5 * g_train @ alpha\n    complexity = np.sum(np.log(np.diag(L)))  # 0.5 * log|K|\n    const = 0.5 * len(t_train) * np.log(2 * np.pi)\n    return data_fit + complexity + const\n\nfrom scipy.optimize import minimize   # for minimising the negative log marginal likelihood\n\n# initial guess in log-space (hand-tuned)\np0 = np.log([2.0, 1.0, 0.5, 0.5, 3.0, 0.4])  # [theta, tau, sigma, phi, eta, zeta]\n\nres = minimize(\n    nlml,\n    p0,\n    args=(t_train, g_train),\n    method=\"L-BFGS-B\",\n    options={\"maxiter\": 100}\n)\n\np_opt = res.x\ntheta_opt, tau_opt, sigma_opt, phi_opt, eta_opt, zeta_opt = np.exp(p_opt)\nprint(\"Optimized hyperparameters:\")\nprint(\"theta, tau, sigma, phi, eta, zeta =\", theta_opt, tau_opt, sigma_opt, phi_opt, eta_opt, zeta_opt)",
      "block_group": "02184febb2ae4ba3b3cd2ed998de5de1",
      "execution_count": 1,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "b68bffda6eda498e8289637bc89471c9",
        "deepnote_cell_type": "text-cell-p"
      },
      "source": "Optimized hyperparameters:\ntheta, tau, sigma, phi, eta, zeta = 2.589086891839971 0.9990816088980068 1.7718435610262389 0.6563166765210973 1.2616660220940161 0.33558552328185726",
      "block_group": "7acc80c7461048a4833f283e93f51497"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "32d1e752",
        "execution_start": 1768644946936,
        "execution_millis": 41847,
        "execution_context_id": "480cf8cf-251f-406e-a5c8-fe90e9f8b0fe",
        "cell_id": "f56fab61348e4cd18849c829d59cae03",
        "deepnote_cell_type": "code"
      },
      "source": "params_opt = dict(\n    theta=2.589086891839971,\n    tau=0.9990816088980068,\n    sigma=1.7718435610262389,\n    phi=0.6563166765210973,\n    eta=1.2616660220940161,\n    zeta=0.33558552328185726,\n)\n\n# GP prior samples\nfprior1 = sample_gp(t_train, kst, params_opt, random_state=0)\nfprior2 = sample_gp(t_train, kst, params_opt, random_state=1)\nfprior3 = sample_gp(t_train, kst, params_opt, random_state=2)\n\nplt.figure(figsize=(10, 4))\n\n# residuals\nplt.plot(t_train, g_train, \"b.\", markersize=3, label=\"residuals $g(t)$\")\n\n# GP prior sample paths\nplt.plot(t_train, fprior1, \"g-\", alpha=0.8, label=\"GP prior sample 1\")\nplt.plot(t_train, fprior2, \"c-\", alpha=0.8, label=\"GP prior sample 2\")\nplt.plot(t_train, fprior3, \"m-\", alpha=0.8, label=\"GP prior sample 3\")\n\nplt.xlabel(\"time $t$\")\nplt.ylabel(\"$g(t)$ / $f(t)$\")\nplt.title(\"Residuals vs GP prior samples with optimised hyperparameters\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()",
      "block_group": "e508d62627884e40a08fa27809b5c5a4",
      "execution_count": 34,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "27837ca84767492280e4c1283833e1d3",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Extrapolation",
      "block_group": "33bdfb7678f4490395deeba88cc04e25"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "9aba49af",
        "execution_start": 1768645032566,
        "execution_millis": 0,
        "execution_context_id": "480cf8cf-251f-406e-a5c8-fe90e9f8b0fe",
        "cell_id": "80e6f9e375ba450885ffa99efeea9060",
        "deepnote_cell_type": "code"
      },
      "source": "def fit_gp_residuals(t_train, g_train, theta, tau, sigma, phi, eta, zeta, jitter=1e-6):\n    \"\"\"Precompute training covariance inverse etc. for residual GP.\"\"\"\n    K = kst(t_train, t_train, theta, tau, sigma, phi, eta, zeta)\n    K = K + jitter * np.eye(len(t_train))\n    K_inv = np.linalg.inv(K)\n    alpha = K_inv @ g_train  # used for predictive mean\n    return {\"theta\": theta, \"tau\": tau, \"sigma\": sigma,\n            \"phi\": phi, \"eta\": eta, \"zeta\": zeta,\n            \"t_train\": t_train, \"K_inv\": K_inv, \"alpha\": alpha}\n\ndef gp_mean(t_test, gp_params):\n    \"\"\"Predictive mean of residual g(t) at test points.\"\"\"\n    th = gp_params\n    t_train = th[\"t_train\"]\n    K_star = kst(t_test, t_train,\n                     th[\"theta\"], th[\"tau\"], th[\"sigma\"],\n                     th[\"phi\"], th[\"eta\"], th[\"zeta\"])\n    # m_g = K_* K^{-1} g = K_* alpha\n    return K_star @ th[\"alpha\"]\n\ndef gp_std(t_test, gp_params):\n    \"\"\"Predictive std of residual g(t) at test points.\"\"\"\n    th = gp_params\n    t_train = th[\"t_train\"]\n    K_inv = th[\"K_inv\"]\n\n    K_star = kst(t_test, t_train,\n                     th[\"theta\"], th[\"tau\"], th[\"sigma\"],\n                     th[\"phi\"], th[\"eta\"], th[\"zeta\"])\n    K_starstar = kst(t_test, t_test,\n                         th[\"theta\"], th[\"tau\"], th[\"sigma\"],\n                         th[\"phi\"], th[\"eta\"], th[\"zeta\"])\n    # Σ_g = K** - K_* K^{-1} K_*^T\n    cov = K_starstar - K_star @ K_inv @ K_star.T\n    var = np.clip(np.diag(cov), 0.0, np.inf)   # numerical safety\n    return np.sqrt(var)",
      "block_group": "2e5971e3f805402eabb331d424390a97",
      "execution_count": 37,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "c56b1e3",
        "execution_start": 1768645034827,
        "execution_millis": 22894,
        "execution_context_id": "480cf8cf-251f-406e-a5c8-fe90e9f8b0fe",
        "cell_id": "7fd0c508e9bf4ff982814a4562a89fea",
        "deepnote_cell_type": "code"
      },
      "source": "# Define test times: Sept 2007 to Dec 2020 (monthly)\nt_test = np.arange(2007 + 8/12, 2020 + 12/12 + 1e-6, 1/12)\n\n# Compute predictive mean / std of residual g(t) at t_test\ngp_params = fit_gp_residuals(t_train, g_train,\n                             theta_opt, tau_opt, sigma_opt, phi_opt, eta_opt, zeta_opt)\nm_g = gp_mean(t_test, gp_params)\ns_g = gp_std(t_test, gp_params)\n\n# Turn residual predictions into CO2 predictions\nf_mean = a_MAP * t_test + b_MAP + m_g\nf_std  = s_g                   # var[f] = var[g] under this model\n\n# Plot observed CO2 and extrapolated mean ± 1 std\nplt.figure(figsize=(10, 4))\nplt.plot(t, y, \"m-\", markersize=2, label=\"Observed CO$_2$\")\n\n# extrapolated GP mean\nplt.plot(t_test, f_mean, \"C0-\", label=\"GP extrapolated mean\")\n\n# 1 std error bars as a shaded band\nplt.fill_between(\n    t_test,\n    f_mean - f_std,\n    f_mean + f_std,\n    color=\"C0\",\n    alpha=0.2,\n    label=\"±1 std (GP)\"\n)\n\nplt.xlabel(\"Year\")\nplt.ylabel(\"CO$_2$ concentration (ppm)\")\nplt.title(\"CO$_2$ with linear trend + GP residual extrapolation\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()",
      "block_group": "5f66c82464754ed7ae1a7d932ffabbe5",
      "execution_count": 40,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "a79ccfd9",
        "execution_start": 1768571526118,
        "execution_millis": 32690,
        "execution_context_id": "924faa36-a91e-493f-9f17-08755441c44d",
        "deepnote_to_be_reexecuted": true,
        "cell_id": "c275bfbc58514e4fa169af855e54b2c9",
        "deepnote_cell_type": "code"
      },
      "source": "# =========================\n# Define untrained vs trained hyperparameters\n# =========================\n# hand-chosen \"untrained\" values\ntheta_u, tau_u, sigma_u, phi_u, eta_u, zeta_u = 2.0, 1.0, 0.5, 0.5, 3.0, 0.4\n\n# \"trained\" values = optimised hyperparameters\n\n# =========================\n# Fit GP for untrained and trained cases\n# =========================\ngp_un = fit_gp_residuals(t_train, g_train,\n                         theta_u, tau_u, sigma_u, phi_u, eta_u, zeta_u)\ngp_tr = fit_gp_residuals(t_train, g_train,\n                         theta_opt, tau_opt, sigma_opt, phi_opt, eta_opt, zeta_opt)\n\n# Predict residuals at test times\nm_g_un = gp_mean(t_test, gp_un)\ns_g_un = gp_std(t_test, gp_un)\n\nm_g_tr = gp_mean(t_test, gp_tr)\ns_g_tr = gp_std(t_test, gp_tr)\n\n# Convert to CO2\nf_un = a_MAP * t_test + b_MAP + m_g_un\nf_tr = a_MAP * t_test + b_MAP + m_g_tr\n\n# =========================\n# Plot: trained vs untrained extrapolation in one block\n# =========================\nfig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n\n# LEFT: untrained hyperparameters\nax = axes[0]\nax.plot(t, y, \"b.\", ms=2, label=\"Observed CO$_2$\")\nax.plot(t_test, f_un, \"g-\", label=\"Untrained mean\")\nax.fill_between(t_test, f_un - s_g_un, f_un + s_g_un,\n                color=\"C0\", alpha=0.2, label=\"±1 std\")\nax.set_title(\"Untrained hyperparameters\")\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"CO$_2$ (ppm)\")\nax.grid(True)\nax.legend()\n\n# RIGHT: trained hyperparameters\nax = axes[1]\nax.plot(t, y, \"b.\", ms=2, label=\"Observed CO$_2$\")\nax.plot(t_test, f_tr, \"m-\", label=\"Trained mean\")\nax.fill_between(t_test, f_tr - s_g_tr, f_tr + s_g_tr,\n                color=\"C1\", alpha=0.2, label=\"±1 std\")\nax.set_title(\"Trained hyperparameters\")\nax.set_xlabel(\"Year\")\nax.grid(True)\nax.legend()\n\nplt.tight_layout()\nplt.show()",
      "block_group": "e3cb55ae4c054f9daf7f04d9bcdd5070",
      "execution_count": 12,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "6c112c2ba42b4d628e2eda8af26acf90",
        "deepnote_cell_type": "text-cell-h1"
      },
      "source": "# Mean-field learning",
      "block_group": "6bcb5b51703f451d96a0cb63c643e0d9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "7d0db27a133f4323bc82d4fba41124e8",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## E-Step",
      "block_group": "2c47e482dd8645de851583609336970a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "248049fe",
        "execution_start": 1768586788468,
        "execution_millis": 2,
        "execution_context_id": "9f69f681-c681-435c-ac60-bb3a595e8f6f",
        "cell_id": "24bc0dbef01b451197d28a1859b88480",
        "deepnote_cell_type": "code"
      },
      "source": "def MeanField(X, mu, sigma2, pi, lambda0, maxsteps=100, eps=1e-6):\n    \"\"\"\n    Mean-field variational E-step for the binary latent factor model.\n\n    X       : (N,D) data matrix\n    mu      : (K,D) matrix, rows μ_k^T\n    sigma2  : scalar noise variance\n    pi      : (K,) Bernoulli priors π_k\n    lambda0 : (N,K) initial λ_{nk}\n    maxsteps: maximum iterations\n    eps     : convergence threshold on |F_t - F_{t-1}|\n\n    Returns\n    -------\n    lam : (N,K) mean-field parameters λ_{nk}\n    F   : (T,) free energy values per iteration\n    \"\"\"\n    X = np.asarray(X)\n    N, D = X.shape\n\n    mu = np.asarray(mu)\n    if mu.shape[0] != X.shape[1] and mu.shape[1] == X.shape[1]:\n        mu = mu  # assume (K,D)\n    elif mu.shape[0] == X.shape[1] and mu.shape[1] != X.shape[1]:\n        mu = mu.T  # convert (D,K) → (K,D)\n    K = mu.shape[0]\n\n    pi = np.asarray(pi).reshape(-1)\n    assert pi.shape[0] == K\n\n    lam = np.array(lambda0, dtype=float)\n    assert lam.shape == (N, K)\n\n    # Clip pi away from {0,1} before taking logs\n    pi = np.clip(pi, 1e-3, 1-1e-3)\n    logit_pi = np.log(pi) - np.log(1.0 - pi)\n\n    F_vals = []\n\n    for it in range(maxsteps):\n        # ----- update λ for all data points -----\n        for n in range(N):\n            x_n = X[n]          # (D,)\n            lam_n = lam[n]      # (K,)\n\n            # precompute Σ_k λ_{nk} μ_k\n            sum_lam_mu = lam_n @ mu  # (D,)\n\n            new_lam_n = np.empty_like(lam_n)\n\n            for k in range(K):\n                mu_k = mu[k]               # (D,)\n                sum_other = sum_lam_mu - lam_n[k] * mu_k\n\n                # your derived term:\n                # μ_k^T ( x_n - μ_k/2 - Σ_{k'≠k} λ_{nk'} μ_{k'} ) / σ²\n                term = (mu_k @ (x_n - 0.5 * mu_k - sum_other)) / sigma2\n                raw_logit = term + logit_pi[k]\n                logit = np.clip(raw_logit, -20, 20)   # much tighter\n                new_lam_n[k] = 1.0 / (1.0 + np.exp(-logit))\n\n            lam[n] = new_lam_n\n\n        # ----- compute free energy F(q,θ) using your expression -----\n        # common precomputes\n        mu_T = mu.T              # (D,K)\n        # E[s_n] = λ_n, so E[sum_k s_nk μ_k] = lam @ mu\n        M = lam @ mu             # (N,D)\n\n        # first big bracket in F\n        xTx = np.sum(X * X)      # ∑_n x_n^T x_n\n        xT_M = np.sum(X * M)     # ∑_n x_n^T M_n\n        # double sum over k,k'\n        lam_mu = lam @ mu        # same as M\n        # ||∑_k λ_nk μ_k||² = M_n^T M_n\n        MM = np.sum(M * M)       # ∑_n M_n^T M_n\n\n        const1 = -0.5 * D * X.shape[0] * np.log(2 * np.pi * sigma2)\n        quad = -0.5 / sigma2 * (xTx - 2 * xT_M + MM)\n\n        # prior term ∑_{n,k} [ λ_{nk} log π_k + (1-λ_{nk}) log(1-π_k) ]\n        log_pi = np.log(pi + 1e-12)\n        log_1m_pi = np.log(1 - pi + 1e-12)\n        prior = np.sum(lam * log_pi + (1 - lam) * log_1m_pi)\n\n        # entropy term −∑_{n,k} [ λ_{nk} log λ_{nk} + (1-λ_{nk}) log(1-λ_{nk}) ]\n        log_lam = np.log(lam + 1e-12)\n        log_1m_lam = np.log(1 - lam + 1e-12)\n        entropy = -np.sum(lam * log_lam + (1 - lam) * log_1m_lam)\n\n        F = const1 + quad + prior + entropy\n        F_vals.append(F)\n        \n        # convergence criterion\n        if it > 0 and abs(F_vals[-1] - F_vals[-2]) < eps:\n            break\n\n    return lam, np.array(F_vals)",
      "block_group": "3639e1c6bc354c3d93bd0760c2af7d08",
      "execution_count": 70,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "d726a30c59ed4835aa4253193dbcd799",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## M-Step",
      "block_group": "5447f599c1cd47a49b99396ad0efa045"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "b3ab9e32",
        "execution_start": 1768586799175,
        "execution_millis": 1,
        "execution_context_id": "9f69f681-c681-435c-ac60-bb3a595e8f6f",
        "cell_id": "ad8d7d3cf32145c4a6e17080e5a9405f",
        "deepnote_cell_type": "code"
      },
      "source": "def MStep(X, ES, ESS, eps=1e-6):\n    \"\"\"\n    mu, sigma, pie = MStep(X,ES,ESS)\n\n    Inputs:\n    -----------------\n           X: shape (N, D) data matrix\n          ES: shape (N, K) E_q[s]\n         ESS: shape (K, K) sum over data points of E_q[ss'] (N, K, K)\n                           if E_q[ss'] is provided, the sum over N is done for you.\n\n    Outputs:\n    --------\n          mu: shape (D, K) matrix of means in p(y|{s_i},mu,sigma)\n       sigma: shape (,)    standard deviation in same\n         pi: shape (1, K) vector of parameters specifying generative distribution for s\n    \"\"\"\n    N, D = X.shape\n    if ES.shape[0] != N:\n        raise TypeError(\"ES must have the same number of rows as X\")\n    K = ES.shape[1]\n\n    if ESS.shape == (N, K, K):\n        ESS = np.sum(ESS, axis=0)\n    if ESS.shape != (K, K):\n        raise TypeError(\"ESS must be square and have the same number of columns as ES\")\n\n    # Add small ridge to avoid singularity\n    ESS_reg = ESS + eps * np.eye(K)\n\n    # μ = (ESS^-1 ES^T X)^T  -> use solve instead of explicit inverse\n    mu = np.linalg.solve(ESS_reg, ES.T @ X).T   # (D,K)\n\n    # σ update (safe version)\n    num = (\n        np.trace(X.T @ X) +\n        np.trace(mu.T @ mu @ ESS) -\n        2.0 * np.trace(ES.T @ X @ mu)\n    )\n    sigma2 = max(num / (N * D), 1e-12)\n    sigma = float(np.sqrt(sigma2))\n\n    # π update\n    pi = np.clip(ES.mean(axis=0), 0.05, 0.95)   # (K,)\n\n    return mu, sigma, pi",
      "block_group": "6c4683f168c2433c9be986fe4256eb70",
      "execution_count": 73,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "ea84b8fa549e41839038900db5589ec0",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Combined EM Function",
      "block_group": "d677163bd7884d8eb8857c87d9a18db1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "2bf645af",
        "execution_start": 1768586806954,
        "execution_millis": 2,
        "execution_context_id": "9f69f681-c681-435c-ac60-bb3a595e8f6f",
        "cell_id": "45b24693eb5f425eb2b6ab420e54c798",
        "deepnote_cell_type": "code"
      },
      "source": "import numpy as np\n\ndef LearnBinFactors(X, K, iterations=50, eps_F=1e-6):\n    \"\"\"\n    mu, sigma2, pi, F_hist = LearnBinFactors(X,K,iterations)\n\n    X          : (N,D) data\n    K          : number of binary factors\n    iterations : max EM iterations\n    eps_F      : convergence threshold on |F_t - F_{t-1}|\n    \"\"\"\n    N, D = X.shape\n\n    # ----- initialise parameters -----\n    rng = np.random.default_rng()\n    mu = rng.normal(0, 1, size=(D, K))\n    sigma2 = 1.0\n    pi = np.full(K, 0.5)\n\n    # initial variational parameters λ\n    lam = rng.uniform(0.25, 0.75, size=(N, K))\n\n    F_hist = []\n\n    for it in range(iterations):\n        # ===== E-step: mean-field over s, holding θ fixed =====\n        lam, F_vals = MeanField(\n            X, mu.T, sigma2, pi, lam,\n            maxsteps=100, eps=1e-6\n        )\n        F_E = F_vals[-1]      # free energy at end of inner loop\n\n        # ===== M-step: update μ, σ, π using current expectations =====\n        ES = lam                     # (N,K) = E_q[s]\n        # E_q[ss'] factorises: diag = λ_k, off-diag = λ_k λ_{k'}\n        ESS = np.zeros((K, K))\n        ESS += ES.T @ ES             # includes λ_k λ_{k'}, and λ_k^2 on diag\n\n        mu, sigma, pi_row = MStep(X, ES, ESS)\n        mu = mu                      # (D,K)\n        sigma2 = sigma**2\n        pi = pi_row.ravel()\n\n        # recompute ELBO with updated θ as a debugging check (optional)\n        # here we reuse F_E as our monitored quantity\n        F_hist.append(F_E)\n\n        # check monotonic increase\n        if it > 0 and F_hist[-1] < F_hist[-2] - 1e-8:\n            print(f\"Warning: F decreased at EM iter {it}: \"\n                  f\"{F_hist[-2]:.6f} -> {F_hist[-1]:.6f}\")\n\n        # convergence on F\n        if it > 0 and abs(F_hist[-1] - F_hist[-2]) < eps_F:\n            break\n\n    return mu, sigma2, pi, np.array(F_hist)",
      "block_group": "863bc0b7f18d4c929ba8d55ab1ad277b",
      "execution_count": 76,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "003094e7dc8d44e58d903203615bc1de",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Run on genimages.py",
      "block_group": "0a188abd9bdb4dae8e08d13a5fb7d68e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "b9bb2fd5",
        "execution_start": 1768586350845,
        "execution_millis": 567,
        "execution_context_id": "9f69f681-c681-435c-ac60-bb3a595e8f6f",
        "cell_id": "b8eea80042274341a2b79380019457e9",
        "deepnote_cell_type": "code"
      },
      "source": "# Load the data\n%run /work/genimages.py\nX = Y     # shape (N, 16)\nN, D = X.shape                   # D should be 16\nK = 8                            # number of latent factors",
      "block_group": "685d63d7a1984649a99d3c4cac18085d",
      "execution_count": 64,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "3b35fe3f",
        "execution_start": 1768603747165,
        "execution_millis": 183721,
        "is_output_hidden": false,
        "execution_context_id": "9f69f681-c681-435c-ac60-bb3a595e8f6f",
        "deepnote_app_is_output_hidden": true,
        "cell_id": "2bb631a375be4098b0b390778f90448c",
        "deepnote_cell_type": "code"
      },
      "source": "mu, sigma2, pi, F_hist = LearnBinFactors(X, K=8, iterations=50)\n\nprint(\"F_hist (first 5):\", F_hist[:5])\nprint(\"mu range:\", mu.min(), mu.max())\nprint(\"sigma2:\", sigma2)\nprint(\"pi:\", pi)\n\nfor k in range(8):\n    plt.subplot(2,4,k+1)\n    plt.imshow(mu[:,k].reshape(4,4), cmap=\"gray\")\n    plt.axis(\"off\")\nplt.show()",
      "block_group": "46d6fa7c2d244a4a9b435560577cbd62",
      "execution_count": 209,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "3faacb751f7e4f0e9fe85366b0bb1222",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## N=1",
      "block_group": "860502b427ea421d8bc59e2cb2f792ae"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "a7e84058",
        "execution_start": 1768603930945,
        "execution_millis": 305,
        "execution_context_id": "9f69f681-c681-435c-ac60-bb3a595e8f6f",
        "cell_id": "da5d6f78e6c84b2691dbab8bf83bd5a4",
        "deepnote_cell_type": "code"
      },
      "source": "# Extract the first datapoint\nx1 = Y[0:1, :]      # shape (1,D)\nN1, D = x1.shape\nK = mu.shape[1]     # (D,K)\n\n# initial λ for this point \nlam0 = np.full((1, K), 0.5)\n\n# Different sigmas\nsigmas = [0.1, 1.0, 5.0]\nF_hist_all = []\n\nfor sigma in sigmas:\n    sigma2 = sigma**2\n    lam, F_hist = MeanField(\n        x1,           # X with N=1\n        mu.T,         # (K,D) inside function\n        sigma2,\n        pi,\n        lam0,\n        maxsteps=100,\n        eps=1e-10     # tight so you see more iterations\n    )\n    F_hist_all.append(F_hist)\n\nplt.figure(figsize=(10,4))\n\n# F(t)\nplt.subplot(1,2,1)\nfor sigma, F_hist in zip(sigmas, F_hist_all):\n    plt.plot(F_hist, label=f\"σ={sigma}\")\nplt.xlabel(\"Iteration t\")\nplt.ylabel(\"F\")\nplt.title(\"Free energy vs iteration (N=1)\")\nplt.legend()\nplt.grid(True)\n\n# log(F(t)-F(t-1))\nplt.subplot(1,2,2)\nfor sigma, F_hist in zip(sigmas, F_hist_all):\n    F_diff = np.diff(F_hist)\n    # keep positive diffs; clip very small negatives due to numerics\n    F_diff = np.clip(F_diff, 1e-15, None)\n    plt.plot(np.log(F_diff), label=f\"σ={sigma}\")\nplt.xlabel(\"Iteration t\")\nplt.ylabel(\"log(F_t - F_{t-1})\")\nplt.title(\"Convergence rate (log increment)\")\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()",
      "block_group": "7ec6963537624cc0ae4750da2b7e85b9",
      "execution_count": 210,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "11817b22ee244acbbfad390fa23433bb",
        "deepnote_cell_type": "text-cell-h1"
      },
      "source": "# Variational Bayes for binary factors",
      "block_group": "f25d5c7735fc406caaa2a9f66696e24d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "a800fffd071a46f28fdb07fd53fbaba5",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "##  Hyperparameter optimisation algorithm",
      "block_group": "9f7d7b376f2a430195ebe77ccbcf7321"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "614c1a8a",
        "execution_start": 1768588565695,
        "execution_millis": 1,
        "execution_context_id": "9f69f681-c681-435c-ac60-bb3a595e8f6f",
        "cell_id": "76f18da185814fe894d188f684ee6dfd",
        "deepnote_cell_type": "code"
      },
      "source": "def ard(X, K_max, max_iters=200, tol=1e-4, a0=1e-3, b0=1e-3):\n    N, D = X.shape\n    K = K_max\n\n    lam = np.clip(0.5 + 0.1*np.random.randn(N, K), 1e-3, 1-1e-3)\n    m   = 0.1 * np.random.randn(K, D)          # rows m_k\n    sigma2 = np.var(X)\n    pi  = 0.3 * np.ones(K)\n\n    # ARD hyperparameters: q(α_k) = Gamma(a_k, b_k)\n    a = a0 * np.ones(K)\n    b = b0 * np.ones(K)\n\n    F_hist, Keff_hist = [], []\n\n    for t in range(max_iters):\n        # ===== E-step: update λ =====\n        mu_kd = m.copy()                       # K×D\n        for n in range(N):\n            x_n = X[n]\n            for k in range(K):\n                lam_minus = lam[n].copy(); lam_minus[k] = 0.0\n                E_mu_minus = lam_minus @ mu_kd\n                resid = x_n - E_mu_minus\n                mu_k = mu_kd[k]\n                log_prior_odds = np.log(pi[k] / (1-pi[k]))\n                llr = (resid @ mu_k) / sigma2 - 0.5 * (mu_k @ mu_k) / sigma2\n                logit = np.clip(log_prior_odds + llr, -20, 20)\n                lam[n, k] = 1.0 / (1.0 + np.exp(-logit))\n\n        # ===== M-step: q(w_d) for all d, then collect moments for each k =====\n        # we’ll store posterior means/covariances of w_d\n        mu_wd = np.zeros((D, K))\n        Sigma_wd_diag = np.zeros((D, K))       # only diag needed for ARD\n\n        alpha = a / b                          # E[α_k]\n        A = np.diag(alpha)                     # K×K\n\n        ES  = lam                              # N×K\n        ESS = ES.T @ ES                        # K×K\n\n        for d in range(D):\n            x_d = X[:, d]                      # N\n            # Σ_wd^{-1} = A + (1/σ²) E[ssᵀ]\n            Sigma_inv = A + ESS / sigma2       # K×K\n            Sigma = np.linalg.inv(Sigma_inv)\n            # μ_wd = Σ (1/σ²) E[s]ᵀ x_d\n            mu = Sigma @ (ES.T @ x_d) / sigma2 # K\n            mu_wd[d] = mu\n            Sigma_wd_diag[d] = np.diag(Sigma)\n\n        # stack rows m_k: k-th row collects wd,k over d\n        m = mu_wd.T                            # K×D\n\n        # ===== Hyper-M: update q(α_k) =====\n        # a_k = a0 + (D/2)\n        # b_k = b0 + 0.5 * Σ_d E[w_{d,k}²]\n        Ew2 = (m**2 + Sigma_wd_diag.T).sum(axis=1)    # length K\n        a = a0 + 0.5 * D\n        b = b0 + 0.5 * Ew2\n        alpha = a / b\n\n        # ===== Update π from λ =====\n        pi = np.clip(lam.mean(axis=0), 1e-3, 1-1e-3)\n\n        # ===== (optional) σ² update using current m, λ =====\n        X_hat = lam @ m                        # N×D\n        sigma2 = np.mean((X - X_hat)**2)\n\n        # ===== Track ELBO & K_eff =====\n        lam_tmp, F_vals = MeanField(X, m, sigma2, pi, lam, maxsteps=1, eps=0.0)\n        F_t = F_vals[-1]\n        F_hist.append(F_t)\n        \n        Keff = np.sum(alpha < 10.0)            # e.g. “active” if E[α_k] < 10\n        Keff_hist.append(Keff)\n\n        if t > 5 and abs(F_hist[-1] - F_hist[-2]) < tol:\n            break\n\n    params = dict(lambda_=lam, m=m, alpha=alpha, pi=pi, sigma2=sigma2)\n    return np.array(F_hist), np.array(Keff_hist), params",
      "block_group": "0c9a1aac590443af9bf9a5593beccce4",
      "execution_count": 100,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "69aa040cd5284402add1da9bdcc3cc5a",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Results",
      "block_group": "5dea6babd88f4e9898f880af9f328706"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "3de34e4a",
        "execution_start": 1768769881676,
        "execution_millis": 797230,
        "execution_context_id": "efb87bf6-96d8-4cd9-b1d6-3e7b5d8a129a",
        "cell_id": "f8b039b85a1c43a3a8e353537e06067a",
        "deepnote_cell_type": "code"
      },
      "source": "K_values = [2, 4, 5, 8, 10, 11, 13, 15, 16, 18, 20]\n\nresults = {}\nfor K in K_values:\n    F_hist, Keff_hist, params = ard(X, K_max=K, max_iters=200, tol=1e-4)\n    results[K] = dict(F=np.asarray(F_hist),\n                      Keff=np.asarray(Keff_hist),\n                      params=params)\n\n# ================================\n# ELBO / Keff vs iteration (plot only a few representative Ks)\n# ================================\nK_values = [2, 5, 8, 15, 20]\ncolors = {\n    2: \"purple\",\n    5: \"tab:blue\",\n    8: \"tab:red\",\n    15: \"tab:green\",\n    20: \"tab:orange\"\n}\n\nplt.figure(figsize=(10, 4))\n\n# ----- ELBO vs iteration -----\nplt.subplot(1, 2, 1)\nfor K in K_values:\n    F = results[K][\"F\"]\n    style = \"-\" if K == 8 else \"--\"  # highlight K=10\n    plt.plot(F, linestyle=style, color=colors[K], label=f\"K={K}\")\nplt.xlabel(\"VB iteration\")\nplt.ylabel(\"Free energy F\")\nplt.title(\"VB free energy vs iteration\")\nplt.grid(True, alpha=0.3)\nplt.legend(frameon=False)\n\n# ----- Effective K vs iteration -----\nplt.subplot(1, 2, 2)\nfor K in K_values:\n    Keff = results[K][\"Keff\"]\n    style = \"-\" if K == 8 else \"--\"\n    plt.plot(Keff, linestyle=style, color=colors[K], label=f\"K={K}\")\nplt.xlabel(\"VB iteration\")\nplt.ylabel(\"Effective K\")\nplt.title(\"Effective number of factors vs iteration\")\nplt.grid(True, alpha=0.3)\nplt.legend(frameon=False)\n\nplt.tight_layout()\nplt.show()\n\n# ================================\n# Final F and Keff vs initial K\n# ================================\nfinal_F = []\nfinal_Keff = []\nfor K in K_values:\n    final_F.append(results[K][\"F\"][-1])\n    final_Keff.append(results[K][\"Keff\"][-1])\n\nplt.figure(figsize=(8, 3))\n\nplt.subplot(1, 2, 1)\nplt.plot(K_values, final_F, \"o-\")\nplt.xlabel(\"Initial K\")\nplt.ylabel(\"Final free energy F\")\nplt.title(\"Final ELBO vs initial K\")\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.plot(K_values, final_Keff, \"o-\")\nplt.xlabel(\"Initial K\")\nplt.ylabel(\"Final effective K\")\nplt.title(\"Final effective K vs initial K\")\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()",
      "block_group": "fb119bfb82e14333a6fce03186012466",
      "execution_count": 12,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "is_collapsed": false,
        "formattedRanges": [],
        "deepnote_app_block_visible": false,
        "cell_id": "1782ec176e144081aca07861ae6e5e94",
        "deepnote_cell_type": "text-cell-h1"
      },
      "source": "# EP",
      "block_group": "fa548da2458e4529b4d715c48c22c111"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "454a3e11",
        "execution_start": 1768600844995,
        "execution_millis": 0,
        "execution_context_id": "9f69f681-c681-435c-ac60-bb3a595e8f6f",
        "cell_id": "fe4060dc7a8b4b33a01f76c1840e1339",
        "deepnote_cell_type": "code"
      },
      "source": "def EP_single(x, mu, sigma2, pi, max_iter=50, tol=1e-4, eps=1e-9):\n    x = np.asarray(x)\n    K, D = mu.shape\n\n    logit_pi = np.log(pi) - np.log(1 - pi)\n\n    eta_site = np.zeros(K)\n    eta_post = logit_pi + eta_site\n    lam = expit(eta_post)\n\n    for it in range(max_iter):\n        lam_old = lam.copy()\n\n        for k in range(K):\n            eta_cav = eta_post[k] - eta_site[k]\n            lam_cav = expit(eta_cav)\n\n            s_mean_others = lam.copy()\n            s_mean_others[k] = 0.0\n            mean_contrib_others = s_mean_others @ mu\n            mu_k = mu[k]\n\n            m0 = mean_contrib_others\n            m1 = mean_contrib_others + mu_k\n\n            ll0 = -0.5 / sigma2 * np.sum((x - m0) ** 2)\n            ll1 = -0.5 / sigma2 * np.sum((x - m1) ** 2)\n\n            log_cav0 = 0.0\n            log_cav1 = eta_cav\n\n            log_p0 = log_cav0 + ll0\n            log_p1 = log_cav1 + ll1\n            mmax = max(log_p0, log_p1)\n            p0 = np.exp(log_p0 - mmax)\n            p1 = np.exp(log_p1 - mmax)\n            Z = p0 + p1\n\n            lam_tilt = p1 / Z\n\n            # clamp to avoid 0 or 1\n            lam_tilt = np.clip(lam_tilt, eps, 1 - eps)\n\n            eta_post_new_k = np.log(lam_tilt) - np.log(1 - lam_tilt)\n            eta_site[k] = eta_post_new_k - eta_cav\n\n        eta_post = logit_pi + eta_site\n        lam = expit(eta_post)\n\n        if np.max(np.abs(lam - lam_old)) < tol:\n            break\n\n    return lam",
      "block_group": "eaf82d2da3624664ba3e65333bc05602",
      "execution_count": 133,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "36504702",
        "execution_start": 1768600848721,
        "execution_millis": 1,
        "execution_context_id": "9f69f681-c681-435c-ac60-bb3a595e8f6f",
        "cell_id": "0f7364b66a2f48d1a47208f6664fd376",
        "deepnote_cell_type": "code"
      },
      "source": "def EP(X, mu, sigma2, pi, max_iter_ep=50, tol=1e-4):\n    N, D = X.shape\n    K = mu.shape[0]\n    lam_all = np.zeros((N, K))\n    for n in range(N):\n        lam_all[n] = EP_single(X[n], mu, sigma2, pi,\n                                         max_iter=max_iter_ep, tol=tol)\n    return lam_all",
      "block_group": "9757e21cedf440538208bcd41081d854",
      "execution_count": 136,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "aea125df240e4c61bc09ac3742f085bf",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Loopy BP",
      "block_group": "a07e16e837e446809f1b2a498711b381"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "4236037a",
        "execution_start": 1768595066185,
        "execution_millis": 1,
        "execution_context_id": "9f69f681-c681-435c-ac60-bb3a595e8f6f",
        "cell_id": "e98a4b566abb42499518ecf8344c5893",
        "deepnote_cell_type": "code"
      },
      "source": "def loopyBP_single(x, mu, sigma2, pi, max_iter=50, tol=1e-4):\n    \"\"\"\n    Approximate loopy BP for p(s | x) in the binary factor model.\n\n    x   : (D,)\n    mu  : (K,D) rows mu_k\n    pi  : (K,)\n    \"\"\"\n    x = np.asarray(x)\n    K, D = mu.shape\n\n    # log-odds messages from prior and likelihood to each s_k\n    logit_prior = np.log(pi) - np.log(1-pi)\n    # initialise variable beliefs\n    logit_bel = logit_prior.copy()\n    lam = expit(logit_bel)\n\n    for it in range(max_iter):\n        lam_old = lam.copy()\n\n        # \"factor → variable\" messages from the (approximated) likelihood\n        logit_like = np.zeros(K)\n\n        # mean-field over others to make factor update tractable\n        mean_others = lam @ mu          # (D,)\n\n        for k in range(K):\n            mu_k = mu[k]\n\n            # remove contribution of s_k from mean_others\n            mean_minus = mean_others - lam[k] * mu_k\n\n            # Likelihood for s_k = 0 and 1 using Gaussian with current mean field:\n            m0 = mean_minus\n            m1 = mean_minus + mu_k\n\n            ll0 = -0.5/sigma2 * np.sum((x - m0)**2)\n            ll1 = -0.5/sigma2 * np.sum((x - m1)**2)\n\n            logit_like[k] = ll1 - ll0    # factor→variable log-odds\n\n        # variable beliefs: prior ⊕ all factor messages\n        logit_bel = logit_prior + logit_like\n        logit_bel = np.clip(logit_bel, -20, 20)\n        lam = expit(logit_bel)\n\n        if np.max(np.abs(lam - lam_old)) < tol:\n            break\n\n    return lam   # approximate marginals q(s_k=1|x)",
      "block_group": "eeefa292091b437a8b5132c028afe12d",
      "execution_count": 112,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "5161dd2a",
        "execution_start": 1768595069612,
        "execution_millis": 1,
        "execution_context_id": "9f69f681-c681-435c-ac60-bb3a595e8f6f",
        "cell_id": "dd0f924384c64a08a1ce7c6c041c73e1",
        "deepnote_cell_type": "code"
      },
      "source": "def loopyBP(X, mu, sigma2, pi, max_iter=50, tol=1e-4):\n    N, D = X.shape\n    K = mu.shape[0]\n    lam_all = np.zeros((N, K))\n    for n in range(N):\n        lam_all[n] = loopyBP_single(X[n], mu, sigma2, pi,\n                                               max_iter=max_iter, tol=tol)\n    return lam_all",
      "block_group": "977fdae237ed4bcd87ac8be5ddbeb8f8",
      "execution_count": 115,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "80da3ab2204e4fa0b6f60ad2d569a653",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## VB vs EP vs BP",
      "block_group": "db87bc04d8704556ae27fbd8a28d8f87"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "f0728578",
        "execution_start": 1768601512675,
        "execution_millis": 364444,
        "execution_context_id": "9f69f681-c681-435c-ac60-bb3a595e8f6f",
        "cell_id": "d5dae99ddb984ccab2bf71560d7852bf",
        "deepnote_cell_type": "code"
      },
      "source": "# Learn theta\nmu_vb, sigma2_vb, pi_vb, F_vb = LearnBinFactors(X, K=8, iterations=100)\n\n# VB mean-field λ\nlam_vb, _ = MeanField(X, mu_vb.T, sigma2_vb, pi_vb, \n                      lambda0=0.5*np.ones((N,8)), maxsteps=50)\n\n# Reconstruction errors per datapoint under VB\nX_hat_vb = lam_vb @ mu_vb.T          # (N,16)\nerr_per_x = np.mean((X - X_hat_vb)**2, axis=1)\n\n# Choose indices\nclean_idx = np.argmin(err_per_x)         # best reconstructed\nnoisy_idx = np.argmax(err_per_x)         # worst reconstructed\nindices = [clean_idx, noisy_idx]\n\n# EP λ\nlam_ep = EP(X, mu_vb.T, sigma2_vb, pi_vb)\n\n# Loopy BP λ\nlam_bp = loopyBP(X, mu_vb.T, sigma2_vb, pi_vb)\n\nK = 8\n\n# Average squared reconstruction error\ndef recon_error(X, lam, mu):\n    X_hat = lam @ mu.T\n    return np.mean((X - X_hat)**2, axis=1)\n\nerr_vb = recon_error(X, lam_vb, mu_vb)\nerr_ep = recon_error(X, lam_ep, mu_vb)\nerr_bp = recon_error(X, lam_bp, mu_vb)\n\nfor n in indices:\n    x = X[n].reshape(4,4)\n\n    # reconstructions\n    x_vb = (lam_vb[n] @ mu_vb.T).reshape(4,4)\n    x_ep = (lam_ep[n] @ mu_vb.T).reshape(4,4)\n    x_bp = (lam_bp[n] @ mu_vb.T).reshape(4,4)\n    \n    # label best / worst\n    if n == clean_idx:\n        rec_type = \"best reconstruction\"\n    elif n == noisy_idx:\n        rec_type = \"worst reconstruction\"\n    else:\n        rec_type = \"\"\n\n    plt.figure(figsize=(12, 5))\n\n    plt.subplot(1, 5, 1)\n    width = 0.3\n    ks = np.arange(K)\n    plt.bar(ks - width, lam_vb[n], width, alpha=0.7, label=\"VB\")\n    plt.bar(ks,         lam_ep[n], width, alpha=0.7, label=\"EP\")\n    plt.bar(ks + width, lam_bp[n], width, alpha=0.7, label=\"BP\")\n    plt.xticks(ks)\n    plt.ylim(0, 1)\n\n    title_main = f\"λ, n={n}\"\n    title_errs = (\n        f\"err(VB)={err_vb[n]:.3e}, \"\n        f\"err(EP)={err_ep[n]:.3e}, \"\n        f\"err(BP)={err_bp[n]:.3e}\"\n    )\n    if rec_type:\n        plt.title(f\"{title_main} ({rec_type})\\n{title_errs}\")\n    else:\n        plt.title(f\"{title_main}\\n{title_errs}\")\n\n    plt.legend()\n\n    plt.subplot(1, 5, 2)\n    plt.imshow(x, cmap=\"gray\")\n    plt.axis(\"off\")\n    plt.title(\"Original\")\n\n    plt.subplot(1, 5, 3)\n    plt.imshow(x_vb, cmap=\"gray\")\n    plt.axis(\"off\")\n    plt.title(\"VB\")\n\n    plt.subplot(1, 5, 4)\n    plt.imshow(x_ep, cmap=\"gray\")\n    plt.axis(\"off\")\n    plt.title(\"EP\")\n\n    plt.subplot(1, 5, 5)\n    plt.imshow(x_bp, cmap=\"gray\")\n    plt.axis(\"off\")\n    plt.title(\"BP\")\n\n    plt.tight_layout()\n    plt.show()",
      "block_group": "b39ec4028c7c4faea386da705b7a2f36",
      "execution_count": 142,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=cb182644-878e-48cb-992b-68a78a5afe3d' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_full_width": true,
    "deepnote_notebook_id": "b771f1a76c094c60b897e28dd5e44013"
  }
}