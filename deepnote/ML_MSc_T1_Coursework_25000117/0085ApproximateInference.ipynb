{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "b673c98b4c77485a856e6ae377e769ea",
        "deepnote_cell_type": "text-cell-h1"
      },
      "source": "# Bayesian linear and GP regression",
      "block_group": "723398ad4a7347c68776d4d37059a961"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "99b5f6effc984a5ba30bfcd8bb943256",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Load the NOAA CO2 file",
      "block_group": "bef54408f1574b6099d19f65e464003f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "36e73098",
        "execution_start": 1768518416816,
        "execution_millis": 1803,
        "execution_context_id": "8157a301-8199-4441-bb0b-2acaaf88337b",
        "cell_id": "deb731e4b61f487db48899262beba31b",
        "deepnote_cell_type": "code"
      },
      "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Read all lines, then drop comment lines starting with '#'\ndf = pd.read_fwf(\n    \"/work/scj-gdrive/co2.txt\",\n    widths=[6, 6, 12, 10, 10],\n    header=None,\n    comment=\"#\",\n    names=[\"year\", \"month\", \"t\", \"y\", \"trend\"]\n)\n\n# Drop any non-numeric rows (e.g. leftover 'year month decimal ...' line)\ndf = df[pd.to_numeric(df[\"t\"], errors=\"coerce\").notna()].copy()\ndf[\"t\"] = df[\"t\"].astype(float)\ndf[\"y\"] = df[\"y\"].astype(float)\n\n# Time and data\nt = df[\"t\"].values\ny = df[\"y\"].values\n\n# Training mask and splits\nmask_train = t <= 2007 + 8/12         # shape (N,)\nt_train = t[mask_train]               # shape (N_train,)\n",
      "block_group": "a747119fb3374be5899752651d719452",
      "execution_count": 1,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "b9be2a6d91ef485b979419fd69df1820",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Mean and covariance",
      "block_group": "44cfe6639279437fb4c2e35b40ef46e5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "b7cd635a",
        "execution_start": 1768518420785,
        "execution_millis": 17,
        "execution_context_id": "8157a301-8199-4441-bb0b-2acaaf88337b",
        "cell_id": "83cff7c24af64e459287295ca99ba031",
        "deepnote_cell_type": "code"
      },
      "source": "Phi = np.c_[t,np.ones(len(t))]\nm0, S0 = np.array([0.,360.]), np.diag([1e4,1e4])\nbeta = 1.0\nSN=np.linalg.inv(np.linalg.inv(S0)+beta*Phi.T@Phi) \nmN=SN@(np.linalg.inv(S0)@m0 + beta*Phi.T@y)\n\nprint('Mean:',mN)\nprint('Cov:',SN)\nprint('Std:',np.sqrt(np.diag(SN)))",
      "block_group": "39eb49f221b245a6af0df7fae93b2fe6",
      "execution_count": 4,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "c1b68efc85b342b5ade1d166c1f4424d",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## MAP estimates ",
      "block_group": "e52a621cde374b16be50e4f2c58f3f40"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "952722a7",
        "execution_start": 1768518426010,
        "execution_millis": 314,
        "execution_context_id": "8157a301-8199-4441-bb0b-2acaaf88337b",
        "cell_id": "95a71d8e1e234587a225e98280d0e56e",
        "deepnote_cell_type": "code"
      },
      "source": "sigma2 = 1.0\nbeta = 1.0 / sigma2\n\na_MAP, b_MAP = mN\nprint(\"a_MAP:\", a_MAP)\nprint(\"b_MAP:\", b_MAP)\n\n# Compute residuals g_obs(t)\ng_obs = y - (a_MAP * t + b_MAP)\ng_train = g_obs[mask_train]               # shape (N_train,)\n\n# Quick diagnostics (optional)\nprint(\"Residual mean:\", np.mean(g_obs))\nprint(\"Residual std:\", np.std(g_obs))\n\n# Plot residuals over time\nplt.figure(figsize=(10, 4))\nplt.plot(df[\"t\"].values, g_obs, \"b-\", markersize=3)\nplt.axhline(0.0, color=\"r\", linestyle=\"--\", linewidth=1)\nplt.xlabel(\"Calendar time (decimal year)\")\nplt.ylabel(\"Residual g_obs(t) [ppm]\")\nplt.title(\"CO2 residuals g_obs(t) = y(t) - (a_MAP t + b_MAP)\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()",
      "block_group": "76db5ab28fdc47ed9b34b3e35821b776",
      "execution_count": 7,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "3f618ac716374f73ae686a36aa1ee6d1",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Generate samples drawn from a GP",
      "block_group": "7a100d1189f044a780022ddadcf1c43d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "b247792e",
        "execution_start": 1768518429120,
        "execution_millis": 1,
        "execution_context_id": "8157a301-8199-4441-bb0b-2acaaf88337b",
        "cell_id": "7ac4d640f69348fa82baeee21debae62",
        "deepnote_cell_type": "code"
      },
      "source": "def sample_gp(x, kernel, jitter=1e-6, random_state=None):\n    \"\"\"\n    Sample from a zero-mean Gaussian process prior on given inputs x.\n\n    Parameters\n    ----------\n    x : array-like, shape (N,) or (N, d)\n        Input locations.\n    kernel : callable\n        Function k(x1, x2) returning the covariance between x1 and x2.\n        Must accept two arrays of shape (N, d) and (M, d) and return (N, M).\n    jitter : float\n        Small diagonal term added for numerical stability.\n    random_state : int or np.random.RandomState, optional\n\n    Returns\n    -------\n    f : ndarray, shape (N,)\n        One sample of function values f(x) from GP(0, k).\n    \"\"\"\n    x = np.atleast_2d(x)\n    if x.shape[0] == 1 and x.shape[1] > 1:\n        x = x.T   # ensure shape (N, d)\n\n    N = x.shape[0]\n\n    # 1. Build covariance matrix K_ij = k(x_i, x_j)\n    K = kernel(x, x)          # shape (N, N)\n\n    # 2. Add jitter for numerical stability\n    K = K + jitter * np.eye(N)\n\n    # 3. Sample from multivariate normal with mean 0 and covariance K\n    rng = np.random if random_state is None else np.random.RandomState(random_state)\n    f = rng.multivariate_normal(mean=np.zeros(N), cov=K)\n\n    return f",
      "block_group": "76368c1760eb4380aad6389112805e93",
      "execution_count": 10,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "ddedca3d63b64aff95b8c637f81f22f9",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Test hyperparameters",
      "block_group": "6789637708a4418e9e804e08f34f2511"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "2611fb62",
        "execution_start": 1768518432435,
        "execution_millis": 58110,
        "execution_context_id": "8157a301-8199-4441-bb0b-2acaaf88337b",
        "cell_id": "928b7ce060f64df1b8b5d7937a6a3298",
        "deepnote_cell_type": "code"
      },
      "source": "def kernel_st(s, t, theta, tau, sigma, phi, eta, zeta):\n    \"\"\"\n    s, t : arrays of shape (N,) and (M,) (1D inputs)\n    returns K of shape (N,M)\n    \"\"\"\n    s = np.atleast_1d(s)\n    t = np.atleast_1d(t)\n    S, T = np.meshgrid(s, t, indexing=\"ij\")   # S_ij = s_i, T_ij = t_j\n    diff = S - T\n\n    # Periodic part\n    arg_periodic = np.pi * diff / tau\n    K_per = np.exp(-2.0 * np.sin(arg_periodic)**2 / sigma**2)\n\n    # RBF part\n    K_rbf = np.exp(-0.5 * (diff**2) / eta**2)\n\n    # Combine with scales\n    K = theta**2 * (K_per + phi**2 * K_rbf)\n\n    # Add noise on the diagonal if shapes match (square matrix when s==t)\n    if s.shape == t.shape and np.allclose(s, t):\n        K = K + (zeta**2) * np.eye(len(s))\n\n    return K\n\ndef make_kernel(theta, tau, sigma, phi, eta, zeta):\n    return lambda s, t: kernel_st(s, t, theta, tau, sigma, phi, eta, zeta)\n\n# ----- plot several sample functions -----\nx = np.linspace(0, 5.0, 400)  # input grid\n\ntheta = 1.0\ntau   = 1.0    # period\nsigma = 0.3    # periodic lengthscale\nphi   = 0.5    # weight of RBF term\neta   = 1.0    # RBF lengthscale\nzeta  = 0.1    # noise scale\n\nker = make_kernel(theta, tau, sigma, phi, eta, zeta)\n\nplt.figure(figsize=(8, 4))\nfor m in range(5):\n    f = sample_gp(x, ker)\n    plt.plot(x, f, alpha=0.7)\n\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.title(\"Samples from GP with periodic + RBF kernel\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()",
      "block_group": "b38671a102fa4857aca5a85bdbb96c17",
      "execution_count": 13,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "12cae4fe7e344b0c8fedd7cf8e7e963a",
        "deepnote_cell_type": "text-cell-h3"
      },
      "source": "### Minimise the negative log marginal likelihood to optimise hyperparameters",
      "block_group": "38dddd19027f4f68aac226b162877c57"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "998f843e",
        "execution_start": 1768518490606,
        "execution_millis": 18987960,
        "execution_context_id": "8157a301-8199-4441-bb0b-2acaaf88337b",
        "cell_id": "44f6102bb3ec453798c9cd76b28d30e8",
        "deepnote_cell_type": "code"
      },
      "source": "def nlml(p, t_train, g_train, jitter=1e-6):\n    log_theta, log_tau, log_sigma, log_phi, log_eta, log_zeta = p\n    theta = np.exp(log_theta)\n    tau   = np.exp(log_tau)\n    sigma = np.exp(log_sigma)\n    phi   = np.exp(log_phi)\n    eta   = np.exp(log_eta)\n    zeta  = np.exp(log_zeta)\n\n    K = kernel_st(t_train, t_train, theta, tau, sigma, phi, eta, zeta)\n    K = K + jitter * np.eye(len(t_train))\n\n    # Cholesky for stability: K = L L^T\n    L = np.linalg.cholesky(K)\n    # Solve K^{-1} g via two triangular solves\n    alpha = np.linalg.solve(L.T, np.linalg.solve(L, g_train))\n\n    data_fit = 0.5 * g_train @ alpha\n    complexity = np.sum(np.log(np.diag(L)))  # 0.5 * log|K|, up to factor\n    const = 0.5 * len(t_train) * np.log(2 * np.pi)\n    return data_fit + complexity + const\n\nfrom scipy.optimize import minimize  # if available in your environment\n\n# initial guess in log-space (hand-tuned)\np0 = np.log([2.0, 1.0, 0.5, 0.5, 3.0, 0.4])  # [theta, tau, sigma, phi, eta, zeta]\n\nres = minimize(\n    nlml,\n    p0,\n    args=(t_train, g_train),\n    method=\"L-BFGS-B\",\n    options={\"maxiter\": 100}\n)\n\np_opt = res.x\ntheta_opt, tau_opt, sigma_opt, phi_opt, eta_opt, zeta_opt = np.exp(p_opt)\nprint(\"Optimized hyperparameters:\")\nprint(\"theta, tau, sigma, phi, eta, zeta =\", theta_opt, tau_opt, sigma_opt, phi_opt, eta_opt, zeta_opt)",
      "block_group": "02184febb2ae4ba3b3cd2ed998de5de1",
      "execution_count": 14,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "b68bffda6eda498e8289637bc89471c9",
        "deepnote_cell_type": "text-cell-p"
      },
      "source": "Optimized hyperparameters:\ntheta, tau, sigma, phi, eta, zeta = 2.589086891839971 0.9990816088980068 1.7718435610262389 0.6563166765210973 1.2616660220940161 0.33558552328185726",
      "block_group": "7acc80c7461048a4833f283e93f51497"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "55bb06055132467e8a64d7b7fc9a8ffe",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## 0-mean GP",
      "block_group": "a8764714525342e69b38e4b121a8741d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "ec89f27c",
        "execution_start": 1768537478618,
        "execution_millis": 0,
        "execution_context_id": "8157a301-8199-4441-bb0b-2acaaf88337b",
        "cell_id": "80e6f9e375ba450885ffa99efeea9060",
        "deepnote_cell_type": "code"
      },
      "source": "import numpy as np\n\ndef build_K(t1, t2, theta, tau, sigma, phi, eta, zeta):\n    \"\"\"Covariance matrix using your kernel k(s,t).\"\"\"\n    t1 = np.atleast_1d(t1)\n    t2 = np.atleast_1d(t2)\n    S, T = np.meshgrid(t1, t2, indexing=\"ij\")\n    diff = S - T\n\n    # periodic term\n    K_per = np.exp(-2.0 * np.sin(np.pi * diff / tau)**2 / sigma**2)\n    # RBF term\n    K_rbf = np.exp(-0.5 * diff**2 / eta**2)\n\n    K = theta**2 * (K_per + phi**2 * K_rbf)\n    if t1.shape == t2.shape and np.allclose(t1, t2):\n        K = K + zeta**2 * np.eye(len(t1))\n    return K\n\ndef fit_gp_residuals(t_train, g_train, theta, tau, sigma, phi, eta, zeta, jitter=1e-6):\n    \"\"\"Precompute training covariance inverse etc. for residual GP.\"\"\"\n    K = build_K(t_train, t_train, theta, tau, sigma, phi, eta, zeta)\n    K = K + jitter * np.eye(len(t_train))\n    K_inv = np.linalg.inv(K)\n    alpha = K_inv @ g_train  # used for predictive mean\n    return {\"theta\": theta, \"tau\": tau, \"sigma\": sigma,\n            \"phi\": phi, \"eta\": eta, \"zeta\": zeta,\n            \"t_train\": t_train, \"K_inv\": K_inv, \"alpha\": alpha}\n\ndef gp_mean(t_test, gp_params):\n    \"\"\"Predictive mean of residual g(t) at test points.\"\"\"\n    th = gp_params\n    t_train = th[\"t_train\"]\n    K_star = build_K(t_test, t_train,\n                     th[\"theta\"], th[\"tau\"], th[\"sigma\"],\n                     th[\"phi\"], th[\"eta\"], th[\"zeta\"])\n    # m_g = K_* K^{-1} g = K_* alpha\n    return K_star @ th[\"alpha\"]\n\ndef gp_std(t_test, gp_params):\n    \"\"\"Predictive std of residual g(t) at test points.\"\"\"\n    th = gp_params\n    t_train = th[\"t_train\"]\n    K_inv = th[\"K_inv\"]\n\n    K_star = build_K(t_test, t_train,\n                     th[\"theta\"], th[\"tau\"], th[\"sigma\"],\n                     th[\"phi\"], th[\"eta\"], th[\"zeta\"])\n    K_starstar = build_K(t_test, t_test,\n                         th[\"theta\"], th[\"tau\"], th[\"sigma\"],\n                         th[\"phi\"], th[\"eta\"], th[\"zeta\"])\n    # Σ_g = K** - K_* K^{-1} K_*^T\n    cov = K_starstar - K_star @ K_inv @ K_star.T\n    var = np.clip(np.diag(cov), 0.0, np.inf)   # numerical safety\n    return np.sqrt(var)",
      "block_group": "2e5971e3f805402eabb331d424390a97",
      "execution_count": 15,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "5187d22",
        "execution_start": 1768537478669,
        "execution_millis": 18637,
        "execution_context_id": "8157a301-8199-4441-bb0b-2acaaf88337b",
        "cell_id": "5a05f7d69fb54305a56741d172c69ad5",
        "deepnote_cell_type": "code"
      },
      "source": "ker = lambda s, u: kernel_st(s, u, theta_opt, tau_opt, sigma_opt, phi_opt, eta_opt, zeta_opt)\n\n# zero-mean prior sample\nfig, axes = plt.subplots(1, 3, figsize=(12,4), sharey=True)\n\nfor ax, (t0, t1) in zip(\n    axes,\n    [(1980,1990), (1990,2000), (2000,2010)]\n):\n    mask = (t >= t0) & (t < t1)\n    t_win = t[mask]\n    for _ in range(3):\n        ax.plot(t_win, sample_gp(t_win, ker), alpha=0.7)\n    ax.axhline(0, color=\"g\", ls=\"--\")\n    ax.set_title(f\"{t0}-{t1}\")\n    ax.grid(True)\n\naxes[0].set_ylabel(\"g(t)\")\nfor ax in axes:\n    ax.set_xlabel(\"Year\")\nplt.tight_layout()\nplt.show()\n",
      "block_group": "aa1b315e82cc4821a39699a49da118f5",
      "execution_count": 16,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "27837ca84767492280e4c1283833e1d3",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Extrapolation",
      "block_group": "e7e43b66e3e945c988ba70ee569110cc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "107151a0",
        "execution_start": 1768500911448,
        "execution_millis": 12881,
        "execution_context_id": "6ea1e252-0b74-48b9-8e05-76e9963ad014",
        "cell_id": "7fd0c508e9bf4ff982814a4562a89fea",
        "deepnote_cell_type": "code"
      },
      "source": "# Define test times: Sept 2007 to Dec 2020 (monthly)\nt_test = np.arange(2007 + 8/12, 2020 + 12/12 + 1e-6, 1/12)\n\n# Compute predictive mean / std of residual g(t) at t_test\ngp_params = fit_gp_residuals(t_train, g_train,\n                             theta_opt, tau_opt, sigma_opt, phi_opt, eta_opt, zeta_opt)\nm_g = gp_mean(t_test, gp_params)\ns_g = gp_std(t_test, gp_params)\n\n# Turn residual predictions into CO2 predictions\nf_mean = a_MAP * t_test + b_MAP + m_g\nf_std  = s_g                   # var[f] = var[g] under this model\n\n# Plot observed CO2 and extrapolated mean ± 1 std\nplt.figure(figsize=(10, 4))\nplt.plot(t, y, \"m-\", markersize=2, label=\"Observed CO$_2$\")\n\n# extrapolated GP mean\nplt.plot(t_test, f_mean, \"C0-\", label=\"GP extrapolated mean\")\n\n# 1 std error bars as a shaded band\nplt.fill_between(\n    t_test,\n    f_mean - f_std,\n    f_mean + f_std,\n    color=\"C0\",\n    alpha=0.2,\n    label=\"±1 std (GP)\"\n)\n\nplt.xlabel(\"Year\")\nplt.ylabel(\"CO$_2$ concentration (ppm)\")\nplt.title(\"CO$_2$ with linear trend + GP residual extrapolation\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()",
      "block_group": "5f66c82464754ed7ae1a7d932ffabbe5",
      "execution_count": 48,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "c275bfbc58514e4fa169af855e54b2c9",
        "deepnote_cell_type": "code"
      },
      "source": "# =========================\n# Define untrained vs trained hyperparameters\n# =========================\n# hand-chosen \"untrained\" values\ntheta_u, tau_u, sigma_u, phi_u, eta_u, zeta_u = 2.0, 1.0, 0.5, 0.5, 3.0, 0.4\n\n# \"trained\" values = optimised hyperparameters\n\n# =========================\n# Fit GP for untrained and trained cases\n# =========================\ngp_un = fit_gp_residuals(t_train, g_train,\n                         theta_u, tau_u, sigma_u, phi_u, eta_u, zeta_u)\ngp_tr = fit_gp_residuals(t_train, g_train,\n                         theta_opt, tau_opt, sigma_opt, phi_opt, eta_opt, zeta_opt)\n\n# Predict residuals at test times\nm_g_un = gp_mean(t_test, gp_un)\ns_g_un = gp_std(t_test, gp_un)\n\nm_g_tr = gp_mean(t_test, gp_tr)\ns_g_tr = gp_std(t_test, gp_tr)\n\n# Convert to CO2\nf_un = a_MAP * t_test + b_MAP + m_g_un\nf_tr = a_MAP * t_test + b_MAP + m_g_tr\n\n# =========================\n# Plot: trained vs untrained extrapolation in one block\n# =========================\nfig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n\n# LEFT: untrained hyperparameters\nax = axes[0]\nax.plot(t, y, \"b.\", ms=2, label=\"Observed CO$_2$\")\nax.plot(t_test, f_un, \"c-\", label=\"Untrained mean\")\nax.fill_between(t_test, f_un - s_g_un, f_un + s_g_un,\n                color=\"C0\", alpha=0.2, label=\"±1 std\")\nax.set_title(\"Untrained hyperparameters\")\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"CO$_2$ (ppm)\")\nax.grid(True)\nax.legend()\n\n# RIGHT: trained hyperparameters\nax = axes[1]\nax.plot(t, y, \"b.\", ms=2, label=\"Observed CO$_2$\")\nax.plot(t_test, f_tr, \"m-\", label=\"Trained mean\")\nax.fill_between(t_test, f_tr - s_g_tr, f_tr + s_g_tr,\n                color=\"C1\", alpha=0.2, label=\"±1 std\")\nax.set_title(\"Trained hyperparameters\")\nax.set_xlabel(\"Year\")\nax.grid(True)\nax.legend()\n\nplt.tight_layout()\nplt.show()",
      "block_group": "e3cb55ae4c054f9daf7f04d9bcdd5070",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "6c112c2ba42b4d628e2eda8af26acf90",
        "deepnote_cell_type": "text-cell-h1"
      },
      "source": "# Mean-field learning",
      "block_group": "6bcb5b51703f451d96a0cb63c643e0d9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "43feb81",
        "execution_start": 1768499234767,
        "execution_millis": 111,
        "execution_context_id": "6ea1e252-0b74-48b9-8e05-76e9963ad014",
        "cell_id": "24bc0dbef01b451197d28a1859b88480",
        "deepnote_cell_type": "code"
      },
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\nN=400  # number of data points - you can increase this if you want to\n       # learn better features (but it will take longer).\nD=16   # dimensionality of the data\n\nnp.random.seed(0)\n\n# Define the basic shapes of the features\n\nm1 = [0, 0, 1, 0,\n      0, 1, 1, 1,\n      0, 0, 1, 0,\n      0, 0, 0, 0]\n\nm2 = [0, 1, 0, 0,\n      0, 1, 0, 0,\n      0, 1, 0, 0,\n      0, 1, 0, 0]\n\nm3 = [1, 1, 1, 1,\n      0, 0, 0, 0,\n      0, 0, 0, 0,\n      0, 0, 0, 0]\n\nm4 = [1, 0, 0, 0,\n      0, 1, 0, 0,\n      0, 0, 1, 0,\n      0, 0, 0, 1] \n\nm5 = [0, 0, 0, 0,\n      0, 0, 0, 0,\n      1, 1, 0, 0,\n      1, 1, 0, 0] \n\nm6 = [1, 1, 1, 1,\n      1, 0, 0, 1,\n      1, 0, 0, 1,\n      1, 1, 1, 1] \n\nm7 = [0, 0, 0, 0,\n      0, 1, 1, 0,\n      0, 1, 1, 0,\n      0, 0, 0, 0]\n\nm8 = [0, 0, 0, 1,\n      0, 0, 0, 1,\n      0, 0, 0, 1,\n      0, 0, 0, 1]\n\nnfeat = 8 # number of features\nrr = 0.5 + np.random.rand(nfeat, 1) * 0.5 # weight of each feature between 0.5 and 1\nmut = np.array([rr[0] * m1, rr[1] * m2, rr[2] * m3, rr[3] * m4, rr[4] * m5,\n                rr[5] * m6, rr[6] * m7, rr[7] * m8])\ns = np.random.rand(N, nfeat) < 0.3 # each feature occurs with prob 0.3 independently \n\n# Generate Data - The Data is stored in Y\n\nY = np.dot(s, mut) + np.random.randn(N, D) * 0.1 # some Gaussian noise is added \n\nnrows = 13\nfor k in range(16):\n    plt.subplot(4, 4, k + 1)\n    plt.imshow(np.reshape(Y[k], (4, 4)), cmap=plt.gray(), interpolation='none')\n    plt.axis('off')\n\nplt.show()",
      "block_group": "3639e1c6bc354c3d93bd0760c2af7d08",
      "execution_count": 6,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "7d0db27a133f4323bc82d4fba41124e8",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## E-Step",
      "block_group": "d741a4506e9e40b4b8788cd9a01f3347"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "845e02c5",
        "execution_start": 1768499234938,
        "execution_millis": 0,
        "execution_context_id": "6ea1e252-0b74-48b9-8e05-76e9963ad014",
        "cell_id": "c93b2c0019a341de926877b16d12c6c4",
        "deepnote_cell_type": "code"
      },
      "source": "def log_gauss_pdf(x, mu, sigma2):\n    \"\"\"Log multivariate Gaussian density N(x | mu, sigma2 * I)\"\"\"\n    D = len(x)\n    return -0.5 * D * np.log(2 * np.pi * sigma2) - 0.5 / sigma2 * np.sum((x - mu)**2)\n\ndef compute_lower_bound(lambda_n, x_n, mu_kd, sigma2, pie):\n    \"\"\"\n    Compute variational lower bound F for one data point\n    mu_kd: K x D matrix of factor means\n    \"\"\"\n    K, D = mu_kd.shape\n    \n    # Entropy term H[q(s)] = -E[log q(s)]\n    entropy = np.sum(lambda_n * np.log(lambda_n + 1e-12) + \n                     (1-lambda_n) * np.log(1-lambda_n + 1e-12))\n    \n    # E[log p(s|pie)]\n    log_prior_exp = np.sum(lambda_n * np.log(pie + 1e-12) + \n                          (1-lambda_n) * np.log(1-pie + 1e-12))\n    \n    # E[mu_s] = lambda_n @ mu_kd  (K -> D)\n    E_mu_s = np.dot(lambda_n, mu_kd)\n    \n    # Var(mu_s_d) = sum_k lambda_k*(1-lambda_k)*mu_kd[k,d]^2\n    var_mu_s = np.sum((lambda_n * (1 - lambda_n))[:, np.newaxis] * mu_kd**2, axis=0)\n    \n    # E[log p(x|s)] using total variance sigma2 + Var(mu_s)\n    log_like_exp = log_gauss_pdf(x_n, E_mu_s, sigma2 + np.mean(var_mu_s))\n    \n    return log_prior_exp + log_like_exp + entropy\n\ndef mean_field_update(lambda_n, x_n, mu_kd, sigma2, pie):\n    \"\"\"Fixed-point update lambda_i <- argmax F[lambda]\"\"\"\n    K, D = mu_kd.shape\n    new_lambda = np.zeros(K)\n\n    for i in range(K):\n        # pie[i] must be scalar, not array of shape (1,)\n        log_prior_odds = float(np.log(pie[i] / (1.0 - pie[i] + 1e-12)))\n\n        lambda_minus_i = lambda_n.copy()\n        lambda_minus_i[i] = 0.0\n        E_mu_minus_i = np.dot(lambda_minus_i, mu_kd)      # (D,)\n\n        residual = x_n - E_mu_minus_i                    # (D,)\n        mui = mu_kd[i]                                   # (D,)\n\n        llr = (residual @ mui) / float(sigma2) \\\n              - 0.5 * (mui @ mui) / float(sigma2)\n\n        logit = float(log_prior_odds + llr)\n        new_lambda[i] = 1.0 / (1.0 + np.exp(-logit))\n\n    return new_lambda\n\n\ndef EStep(X, mu, sigma2, pie, lambda0, maxsteps=100, tol=1e-8):\n    \"\"\"\n    Mean-field variational inference (E-step of variational EM).\n\n    Args:\n        X: N x D data matrix\n        mu: D x K matrix of factor means\n        sigma2: scalar noise variance\n        pie: 1 x K vector of Bernoulli priors π_i\n        lambda0: N x K initial variational parameters\n        maxsteps: maximum coordinate ascent iterations\n        tol: convergence tolerance on change in total F\n\n    Returns:\n        lambda: N x K final variational parameters q_n(s_i=1) = λ_{n,i}\n        F: array of lower bound values over iterations\n    \"\"\"\n    N, D = X.shape\n    K = mu.shape[1]\n\n    # Internally use mu_kd: K x D\n    mu_kd = mu.T\n\n    # Initialize\n    lambda_n = lambda0.copy().astype(np.float64)\n    F_history = np.zeros(maxsteps)\n\n    for step in range(maxsteps):\n        # Old lower bound\n        F_old = sum(\n            compute_lower_bound(lambda_n[n], X[n], mu_kd, sigma2, pie)\n            for n in range(N)\n        )\n\n        # Coordinate ascent: cycle through data points\n        for n in range(N):\n            x_n = X[n]  # n is defined here\n            lambda_n[n] = mean_field_update(lambda_n[n], x_n, mu_kd, sigma2, pie)\n\n        # New lower bound\n        F_new = sum(\n            compute_lower_bound(lambda_n[n], X[n], mu_kd, sigma2, pie)\n            for n in range(N)\n        )\n\n        F_history[step] = F_new\n\n        # Convergence check\n        delta_F = abs(F_new - F_old)\n        if delta_F < tol:\n            print(f\"Converged after {step+1} steps (ΔF = {delta_F:.2e})\")\n            break\n\n        if step % 20 == 0 or step < 5:\n            print(f\"Step {step}: F = {F_new:.4f} (ΔF = {delta_F:.2e})\")\n\n    return lambda_n, F_history[:step+1]",
      "block_group": "c93b2c0019a341de926877b16d12c6c4",
      "execution_count": 7,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "d726a30c59ed4835aa4253193dbcd799",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## M-Step",
      "block_group": "5447f599c1cd47a49b99396ad0efa045"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "d8e9b47c",
        "execution_start": 1768499234998,
        "execution_millis": 0,
        "execution_context_id": "6ea1e252-0b74-48b9-8e05-76e9963ad014",
        "cell_id": "ad8d7d3cf32145c4a6e17080e5a9405f",
        "deepnote_cell_type": "code"
      },
      "source": "def MStep(X, ES, ESS, eps=1e-6):\n    \"\"\"\n    mu, sigma, pie = MStep(X,ES,ESS)\n\n    Inputs:\n    -----------------\n           X: shape (N, D) data matrix\n          ES: shape (N, K) E_q[s]\n         ESS: shape (K, K) sum over data points of E_q[ss'] (N, K, K)\n                           if E_q[ss'] is provided, the sum over N is done for you.\n\n    Outputs:\n    --------\n          mu: shape (D, K) matrix of means in p(y|{s_i},mu,sigma)\n       sigma: shape (,)    standard deviation in same\n         pie: shape (1, K) vector of parameters specifying generative distribution for s\n    \"\"\"\n    N, D = X.shape\n    if ES.shape[0] != N:\n        raise TypeError(\"ES must have the same number of rows as X\")\n    K = ES.shape[1]\n\n    if ESS.shape == (N, K, K):\n        ESS = np.sum(ESS, axis=0)\n    if ESS.shape != (K, K):\n        raise TypeError(\"ESS must be square and have the same number of columns as ES\")\n\n    # Add small ridge to avoid singularity\n    ESS_reg = ESS + eps * np.eye(K)\n\n    # μ = (ESS^-1 ES^T X)^T  -> use solve instead of explicit inverse\n    mu = np.linalg.solve(ESS_reg, ES.T @ X).T   # (D,K)\n\n    # σ update (safe version)\n    num = (\n        np.trace(X.T @ X) +\n        np.trace(mu.T @ mu @ ESS) -\n        2.0 * np.trace(ES.T @ X @ mu)\n    )\n    sigma2 = max(num / (N * D), 1e-12)\n    sigma = float(np.sqrt(sigma2))\n\n    pie = ES.mean(axis=0)                       # (K,)\n\n    return mu, sigma, pie",
      "block_group": "6c4683f168c2433c9be986fe4256eb70",
      "execution_count": 8,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "a495ac342c5d4668b165ac079c3604d0",
        "deepnote_cell_type": "text-cell-p"
      },
      "source": "Write two or three sentences discussing how the solution relates to linear regression and why.",
      "block_group": "36aba2a021a9454da19d358e9e816b17"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "ea84b8fa549e41839038900db5589ec0",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Combined EM Function",
      "block_group": "d677163bd7884d8eb8857c87d9a18db1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "9c1ade46",
        "execution_start": 1768499235068,
        "execution_millis": 0,
        "execution_context_id": "6ea1e252-0b74-48b9-8e05-76e9963ad014",
        "cell_id": "45b24693eb5f425eb2b6ab420e54c798",
        "deepnote_cell_type": "code"
      },
      "source": "def LearnBinFactors(X, K, iterations=100, tol=1e-8, verbose=True):\n    \"\"\"\n    Full variational EM for binary latent factor model.\n    Returns:\n        mu : (D, K)\n        sigma : scalar\n        pie : (K,)\n        F_history : ELBO over EM iterations\n    \"\"\"\n    N, D = X.shape\n\n    # Initialise parameters\n    mu = 0.1 * np.random.randn(D, K)\n    sigma = 1.0\n    pie = 0.5 * np.ones(K)\n    lambda_n = 0.5 * np.ones((N, K))\n\n    F_history = np.zeros(iterations)\n    em_done = 0  # how many EM steps actually run\n\n    for em_iter in range(iterations):\n        # === E-STEP via EStep() ===\n        lambda_n, F_path = EStep(\n            X, mu, sigma**2, pie, lambda_n,\n            maxsteps=50, tol=1e-8\n        )\n        F_new = F_path[-1]                 # current ELBO\n        F_history[em_iter] = F_new\n\n        # === M-STEP ===\n        ESS_diag = np.sum(lambda_n * (1 - lambda_n), axis=0)\n        ESS = np.diag(ESS_diag)\n\n        mu_old, sigma_old, pie_old = mu.copy(), sigma, pie.copy()\n        mu, sigma, pie = MStep(X, lambda_n, ESS)\n\n        # === Monotonicity check ===\n        if em_iter > 0 and F_new < F_history[em_iter - 1]:\n            if verbose:\n                print(f\"WARNING: F decreased from {F_history[em_iter-1]:.4f} to {F_new:.4f}\")\n                print(\"  Reverting to previous parameters\")\n            mu, sigma, pie = mu_old, sigma_old, pie_old\n            F_history[em_iter] = F_history[em_iter - 1]\n        else:\n            if em_iter > 0:\n                delta_F = abs(F_new - F_history[em_iter - 1])\n                if delta_F < tol and verbose:\n                    print(f\"Converged at EM iteration {em_iter+1}: ΔF = {delta_F:.2e}\")\n                    em_done = em_iter + 1\n                    break\n\n        if verbose and em_iter % 10 == 0:\n            print(f\"EM {em_iter}: F={F_history[em_iter]:.4f}, σ={sigma:.4f}, π={pie.round(3)}\")\n\n        em_done = em_iter + 1  # update if we didn't break\n\n    if verbose:\n        print(\"\\n=== Final Results ===\")\n        print(f\"Converged in {em_done} EM iterations\")\n        print(f\"Final F: {F_history[em_done-1]:.4f}\")\n        print(f\"σ: {sigma:.4f}\")\n        print(f\"π: {pie}\")\n        print(f\"μ shape: {mu.shape}\")\n\n    return mu, sigma, pie, F_history[:em_done]",
      "block_group": "863bc0b7f18d4c929ba8d55ab1ad277b",
      "execution_count": 9,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "003094e7dc8d44e58d903203615bc1de",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Run on genimages.py",
      "block_group": "0a188abd9bdb4dae8e08d13a5fb7d68e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "c6a53971",
        "execution_start": 1768499235138,
        "execution_millis": 1009,
        "is_output_hidden": true,
        "execution_context_id": "6ea1e252-0b74-48b9-8e05-76e9963ad014",
        "deepnote_to_be_reexecuted": true,
        "deepnote_app_is_output_hidden": true,
        "cell_id": "2bb631a375be4098b0b390778f90448c",
        "deepnote_cell_type": "code"
      },
      "source": "X = Y     # your data matrix, shape (N, 16)\nK_true = nfeat\nN, D = X.shape                    # D should be 16\nK = 8                            \n\n# 2. Learn binary latent factor model\nmu, sigma, pie, F_history = LearnBinFactors(X, K, iterations=100)\n\n# 3. Visualise each μ as a 4×4 image\nfor k in range(K):\n    plt.subplot(2, K//2, k+1)\n    plt.imshow(mu[:, k].reshape(4, 4), cmap=\"gray\", vmin=mu.min(), vmax=mu.max())\n    plt.axis(\"off\")\nplt.suptitle(\"Learned μ features (4×4)\")\nplt.tight_layout()\nplt.show()",
      "block_group": "46d6fa7c2d244a4a9b435560577cbd62",
      "execution_count": 10,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "e23d751f2a754b3c97a06a364de5343e",
        "deepnote_cell_type": "text-cell-p"
      },
      "source": "Step 0: F = -12808.4334 (ΔF = 2.31e+02)\nStep 1: F = -12814.6780 (ΔF = 6.24e+00)\nStep 2: F = -12814.4817 (ΔF = 1.96e-01)\nStep 3: F = -12814.4878 (ΔF = 6.17e-03)\nStep 4: F = -12814.4876 (ΔF = 2.49e-04)\nConverged after 9 steps (ΔF = 6.58e-10)\nEM 0: F=-12814.4876, σ=0.0000, π=[0.436 0.542 0.318 0.509 0.494 0.526 0.458 0.351]\nStep 0: F = -2448358160419534.5000 (ΔF = 2.45e+15)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\n/tmp/ipykernel_853/4288592019.py:52: RuntimeWarning: overflow encountered in exp\n  new_lambda[i] = 1.0 / (1.0 + np.exp(-logit))\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nEM 10: F=-12814.4876, σ=0.0000, π=[0.436 0.542 0.318 0.509 0.494 0.526 0.458 0.351]\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nEM 20: F=-12814.4876, σ=0.0000, π=[0.436 0.542 0.318 0.509 0.494 0.526 0.458 0.351]\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nEM 30: F=-12814.4876, σ=0.0000, π=[0.436 0.542 0.318 0.509 0.494 0.526 0.458 0.351]\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nEM 40: F=-12814.4876, σ=0.0000, π=[0.436 0.542 0.318 0.509 0.494 0.526 0.458 0.351]\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nEM 50: F=-12814.4876, σ=0.0000, π=[0.436 0.542 0.318 0.509 0.494 0.526 0.458 0.351]\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nEM 60: F=-12814.4876, σ=0.0000, π=[0.436 0.542 0.318 0.509 0.494 0.526 0.458 0.351]\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nEM 70: F=-12814.4876, σ=0.0000, π=[0.436 0.542 0.318 0.509 0.494 0.526 0.458 0.351]\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nEM 80: F=-12814.4876, σ=0.0000, π=[0.436 0.542 0.318 0.509 0.494 0.526 0.458 0.351]\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nEM 90: F=-12814.4876, σ=0.0000, π=[0.436 0.542 0.318 0.509 0.494 0.526 0.458 0.351]\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters\nStep 0: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 1: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 2: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 3: F = -99868699963097168.0000 (ΔF = 9.74e+16)\nStep 4: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 20: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nStep 40: F = -2439559147201384.0000 (ΔF = 9.74e+16)\nWARNING: F decreased from -12814.4876 to -99868699963097168.0000\n  Reverting to previous parameters",
      "block_group": "8b56a61512464d2f9736837eace3fd5f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "8861482a2a2a43bab839d7b49dbd363b",
        "deepnote_cell_type": "text-cell-p"
      },
      "source": "=== Final Results ===\nConverged in 100 EM iterations\nFinal F: -12814.4876\nσ: 0.0000\nπ: [0.43637231 0.54201043 0.31785741 0.50949573 0.4943884  0.52550078\n 0.45752202 0.35095216]\nμ shape: (16, 8)",
      "block_group": "70a7b4bcccbe4668984238966561d26f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "3faacb751f7e4f0e9fe85366b0bb1222",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Variational approximation for just the first data point",
      "block_group": "860502b427ea421d8bc59e2cb2f792ae"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "664ccd20",
        "execution_start": 1767456370587,
        "execution_millis": 0,
        "execution_context_id": "0743e833-a8b8-4afe-ad03-721ce7570625",
        "deepnote_to_be_reexecuted": true,
        "cell_id": "da5d6f78e6c84b2691dbab8bf83bd5a4",
        "deepnote_cell_type": "code"
      },
      "source": "def stable_sigmoid(x):\n        # handles large ±x safely\n        return np.where(\n            x >= 0,\n            1.0 / (1.0 + np.exp(-x)),\n            np.exp(x) / (1.0 + np.exp(x))\n        )\n\ndef MeanField_single(x, mu, sigma, pie, lambda0, maxsteps=200, tol=1e-10):\n    \"\"\"\n    Run mean-field VI for a single data point x (N=1).\n    Returns:\n        lambda_hist: T x K\n        F_hist     : length-T array of ELBO values\n    \"\"\"\n    X = x.reshape(1, -1)   # 1 x D\n    N, D = X.shape\n    K = mu.shape[1]\n    sigma2 = max(sigma**2, 1e-6)\n    \n    # Internal helpers\n    def log_gauss_pdf(x, m, s2):\n        D = len(x)\n        return -0.5*D*np.log(2*np.pi*s2) - 0.5/s2 * np.sum((x - m)**2)\n\n    def compute_F(lmbda):\n        mu_kd = mu.T                     # K x D\n        # Entropy\n        H = np.sum(lmbda*np.log(lmbda+1e-12) +\n                   (1-lmbda)*np.log(1-lmbda+1e-12))\n        # Prior\n        log_prior = np.sum(lmbda*np.log(pie+1e-12) +\n                           (1-lmbda)*np.log(1-pie+1e-12))\n        # Likelihood term\n        E_mu = lmbda @ mu_kd             # D\n        var_mu = np.sum((lmbda*(1-lmbda))[:, None] * mu_kd**2, axis=0)\n        log_like = log_gauss_pdf(x, E_mu, sigma2 + np.mean(var_mu))\n        return log_prior + log_like + H\n\n    def mf_update(lmbda):\n        mu_kd = mu.T\n        new_lmbda = np.zeros_like(lmbda)\n        for i in range(K):\n            log_prior_odds = np.log(pie[i] / (1-pie[i] + 1e-12))\n            lmbda_minus = lmbda.copy()\n            lmbda_minus[i] = 0.0\n            E_mu_minus = lmbda_minus @ mu_kd\n            resid = x - E_mu_minus\n            llr = (resid @ mu_kd[i]) / sigma2 - 0.5*(mu_kd[i] @ mu_kd[i]) / sigma2\n            logit = log_prior_odds + llr\n            new_lmbda[i] = stable_sigmoid(logit)\n        return new_lmbda\n\n    # Coordinate ascent\n    lmbda = lambda0.copy()\n    F_hist = []\n    lambda_hist = []\n    for t in range(maxsteps):\n        F_old = compute_F(lmbda)\n        lambda_hist.append(lmbda.copy())\n        F_hist.append(F_old)\n        new_lmbda = mf_update(lmbda)\n        if np.max(np.abs(new_lmbda - lmbda)) < tol:\n            lmbda = new_lmbda\n            F_hist.append(compute_F(lmbda))\n            lambda_hist.append(lmbda.copy())\n            break\n        lmbda = new_lmbda\n\n    return np.array(lambda_hist), np.array(F_hist)",
      "block_group": "7ec6963537624cc0ae4750da2b7e85b9",
      "execution_count": 145,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "5897038a",
        "execution_start": 1767456121902,
        "execution_millis": 127,
        "execution_context_id": "0743e833-a8b8-4afe-ad03-721ce7570625",
        "deepnote_to_be_reexecuted": true,
        "cell_id": "eac817d8036b4f37b305ed32736399c3",
        "deepnote_cell_type": "code"
      },
      "source": "x1 = X[0]                          # first data point from genimages\nK  = mu.shape[1]\nlambda0 = 0.5 * np.ones(K)\n\nlambda_hist, F_hist = MeanField_single(\n    x1, mu, sigma, pie,\n    lambda0=lambda0, maxsteps=200\n)\n\nT = len(F_hist)\nFt = F_hist\ndF = Ft[1:] - Ft[:-1]              # F(t) - F(t-1)\nlog_dF = np.log(np.clip(dF, 1e-12, None))\n\nplt.figure(figsize=(10,4))\n\nplt.subplot(1,2,1)\nplt.plot(range(T), Ft, 'o-')\nplt.xlabel('Iteration t')\nplt.ylabel('F_t')\nplt.title('ELBO F for single data point')\nplt.grid(True)\n\nplt.subplot(1,2,2)\nplt.plot(range(1, T), log_dF, 'o-')\nplt.xlabel('Iteration t')\nplt.ylabel('log(F_t − F_{t-1})')\nplt.title('Log increment of ELBO')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()",
      "block_group": "f47d164a3a8d4c1cbbdb54132f5de3e4",
      "execution_count": 133,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "f13b35752ea74a34a0b8559824df3e67",
        "deepnote_cell_type": "text-cell-p"
      },
      "source": "new_lmbda[i] = 1.0 / (1.0 + np.exp(-logit))",
      "block_group": "afce24ea23654a07aff3ad4fe5252c7d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "19f84ff4d61745cf9784f8dda9d4e79b",
        "deepnote_cell_type": "text-cell-h3"
      },
      "source": "### Effects of sigmas",
      "block_group": "95716e5fd7e94e1a9cfd4c4d5146bbdf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "8a5823f1",
        "execution_start": 1767456148817,
        "execution_millis": 46,
        "execution_context_id": "0743e833-a8b8-4afe-ad03-721ce7570625",
        "deepnote_to_be_reexecuted": true,
        "cell_id": "3e6c12b3a0774146b5e6abbfb88f5920",
        "deepnote_cell_type": "code"
      },
      "source": "sigmas = [0.05, 0.2, 1.0] \nall_F = []\n\nfor s in sigmas:\n    _, Fh = MeanField_single(\n        x1, mu, s, pie, lambda0, maxsteps=200\n    )\n    all_F.append(Fh)\n\nplt.figure(figsize=(6,4))\nfor s, Fh in zip(sigmas, all_F):\n    plt.plot(Fh, label=f\"sigma={s:.3f}\")\nplt.xlabel(\"Iteration t\")\nplt.ylabel(\"F_t\")\nplt.title(\"ELBO F for different σ\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()",
      "block_group": "a129edadefcb4dbea857fe0083e13950",
      "execution_count": 136,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "11817b22ee244acbbfad390fa23433bb",
        "deepnote_cell_type": "text-cell-h1"
      },
      "source": "# Variational Bayes for binary factors",
      "block_group": "f25d5c7735fc406caaa2a9f66696e24d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "a800fffd071a46f28fdb07fd53fbaba5",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "##  Hyperparameter optimisation algorithm",
      "block_group": "9f7d7b376f2a430195ebe77ccbcf7321"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "953f206d",
        "execution_start": 1767455864377,
        "execution_millis": 0,
        "execution_context_id": "0743e833-a8b8-4afe-ad03-721ce7570625",
        "deepnote_to_be_reexecuted": true,
        "cell_id": "76f18da185814fe894d188f684ee6dfd",
        "deepnote_cell_type": "code"
      },
      "source": "def vb_binary_factors(X, K_max, max_iters=200, tol=1e-4, a0=1e-3, b0=1e-3):\n    \"\"\"\n    VB (no ARD shrinkage in m) for binary latent factor model, starting with K_max factors.\n\n    Returns:\n      F_hist    : array of ELBO values over iterations\n      Keff_hist : array of effective factor counts over iterations\n      params    : dict of final variational params (lambda, m, tau, pie, sigma2)\n    \"\"\"\n    N, D = X.shape\n    K = K_max\n\n    # --- Initialise variational parameters ---\n    lam = np.clip(0.5 + 0.1*np.random.randn(N, K), 1e-3, 1-1e-3)  # N x K\n    m   = 0.1 * np.random.randn(K, D)                              # K x D\n    tau = np.ones(K)                                               # precision\n    sigma2 = np.var(X)                                             # fixed noise\n    pie = 0.3 * np.ones(K)                                         # Bernoulli prior\n\n    F_hist    = []\n    Keff_hist = []\n\n    # --- ELBO (simplified) ---\n    def compute_F():\n        mu_kd = m\n        ll = 0.0\n        ent = 0.0\n        prior_s = 0.0\n        for n in range(N):\n            ln = lam[n]                    # (K,)\n            E_mu_s = ln @ mu_kd            # (D,)\n\n            # Likelihood term\n            ll -= 0.5 / sigma2 * np.sum((X[n] - E_mu_s)**2)\n\n            # Entropy of q(S)\n            ent -= np.sum(\n                ln * np.log(ln + 1e-12) +\n                (1 - ln) * np.log(1 - ln + 1e-12)\n            )\n\n            # Prior over S\n            prior_s += np.sum(\n                ln * np.log(pie + 1e-12) +\n                (1 - ln) * np.log(1 - pie + 1e-12)\n            )\n        return ll + prior_s + ent\n\n    # --- VB iterations ---\n    for t in range(max_iters):\n        mu_kd = m.copy()                  # K x D\n\n        # === E-step: update lambda (mean-field for S) ===\n        for n in range(N):\n            x_n = X[n]\n            for i in range(K):\n                lam_minus = lam[n].copy()\n                lam_minus[i] = 0.0\n                E_mu_minus = lam_minus @ mu_kd\n                resid = x_n - E_mu_minus\n\n                E_mu_i = m[i]\n                log_prior_odds = np.log(pie[i] / (1 - pie[i] + 1e-12))\n                llr = (resid @ E_mu_i) / sigma2 - 0.5 * (E_mu_i @ E_mu_i) / sigma2\n                logit = log_prior_odds + llr\n                lam[n, i] = 1.0 / (1.0 + np.exp(-logit))\n\n        # === M-step: update m, tau (NO ARD on m) ===\n        for i in range(K):\n            r_i = lam[:, i]                     # (N,)\n            R_i = np.sum(r_i)\n\n            contrib_others = lam @ mu_kd        # N x D\n            contrib_others -= r_i[:, None] * m[i][None, :]\n            resid_i = X - contrib_others        # N x D\n\n            tau_i = 1.0 + R_i / sigma2          # no alpha term\n            tau[i] = tau_i\n            m[i] = (r_i[:, None] * resid_i).sum(axis=0) / (sigma2 * max(tau_i, 1e-6))\n\n        # Update π from lam\n        pie = lam.mean(axis=0)\n\n        # === ELBO and effective K ===\n        F_t = compute_F()\n        F_hist.append(F_t)\n\n        norm_m = np.linalg.norm(m, axis=1)\n        Keff_t = np.sum(norm_m > 1e-3)\n        Keff_hist.append(Keff_t)\n\n        if t > 5 and abs(F_hist[-1] - F_hist[-2]) < tol:\n            break\n\n    params = dict(lambda_=lam, m=m, tau=tau, pie=pie, sigma2=sigma2)\n    return np.array(F_hist), np.array(Keff_hist), params",
      "block_group": "0c9a1aac590443af9bf9a5593beccce4",
      "execution_count": 121,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "69aa040cd5284402add1da9bdcc3cc5a",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Results",
      "block_group": "5dea6babd88f4e9898f880af9f328706"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "b0258582",
        "execution_start": 1767455875763,
        "execution_millis": 12180,
        "execution_context_id": "0743e833-a8b8-4afe-ad03-721ce7570625",
        "deepnote_to_be_reexecuted": true,
        "cell_id": "f8b039b85a1c43a3a8e353537e06067a",
        "deepnote_cell_type": "code"
      },
      "source": "K_values = [2, 4, 8, 16]   # half to 4× true\n\nresults = {}\nfor Kmax in K_values:\n    F_hist, Keff_hist, params = vb_binary_factors(\n        X, K_max=Kmax, max_iters=200, tol=1e-4\n    )\n    results[Kmax] = dict(F=F_hist, Keff=Keff_hist, params=params)",
      "block_group": "fb119bfb82e14333a6fce03186012466",
      "execution_count": 124,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "f95419d7",
        "execution_start": 1767455895435,
        "execution_millis": 322,
        "execution_context_id": "0743e833-a8b8-4afe-ad03-721ce7570625",
        "deepnote_to_be_reexecuted": true,
        "cell_id": "8a928783eb094343a043ec5502f49e5d",
        "deepnote_cell_type": "code"
      },
      "source": "plt.figure(figsize=(10,4))\n\n# Left: ELBO vs iteration\nplt.subplot(1,2,1)\nfor Kmax in K_values:\n    F = results[Kmax][\"F\"]\n    plt.plot(range(len(F)), F, label=f\"Kmax={Kmax}\")\nplt.xlabel(\"VB iteration\")\nplt.ylabel(\"Free energy F\")\nplt.title(\"VB free energy vs iteration\")\nplt.legend()\nplt.grid(True)\n\n# Right: effective number of factors vs iteration\nplt.subplot(1,2,2)\nfor Kmax in K_values:\n    Keff = results[Kmax][\"Keff\"]\n    plt.plot(range(len(Keff)), Keff, label=f\"Kmax={Kmax}\")\nplt.xlabel(\"VB iteration\")\nplt.ylabel(\"Effective K\")\nplt.title(\"Effective number of factors vs iteration\")\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()",
      "block_group": "bdaf88780b274d2cafd5cb676e9e2a13",
      "execution_count": 127,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "is_collapsed": false,
        "formattedRanges": [],
        "deepnote_app_block_visible": false,
        "cell_id": "1782ec176e144081aca07861ae6e5e94",
        "deepnote_cell_type": "text-cell-h1"
      },
      "source": "# EP & Loopy BP",
      "block_group": "fa548da2458e4529b4d715c48c22c111"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "4fb6af77",
        "execution_start": 1767456310407,
        "execution_millis": 0,
        "execution_context_id": "0743e833-a8b8-4afe-ad03-721ce7570625",
        "deepnote_to_be_reexecuted": true,
        "cell_id": "d47ca25e9db347a18219d3e8eefcdf85",
        "deepnote_cell_type": "code"
      },
      "source": "def loopyBP(h, J, max_iters=50, tol=1e-6):\n    \"\"\"\n    Loopy BP on binary Ising model with fields h and couplings J.\n\n    h: shape (K,)\n    J: shape (K, K), with zeros on the diagonal\n\n    Returns:\n        m  : shape (K,) approximate node marginals E[s_i]\n        nu : shape (K, K) message natural params nu_{i->j}\n    \"\"\"\n    K = len(h)\n    nu = np.zeros((K, K))  # messages in natural-parameter form\n\n    sigma = lambda x: 1.0 / (1.0 + np.exp(-x))\n\n    for it in range(max_iters):\n        nu_old = nu.copy()\n        for i in range(K):\n            for j in range(K):\n                if i == j:\n                    continue\n                # cavity field at i excluding j\n                eta_i_ex = h[i] + sum(nu[k, i] for k in range(K) if k != j)\n                m_i_ex = stable_sigmoid(eta_i_ex)\n\n                # BP message update i -> j for factor exp(J_ij s_i s_j)\n                J_ij = np.clip(J[i, j], -20.0, 20.0)\n                num = np.exp(J_ij) * m_i_ex + (1.0 - m_i_ex)\n                den = m_i_ex + (1.0 - m_i_ex)\n                nu[i, j] = np.log(num / den)\n\n        if np.max(np.abs(nu - nu_old)) < tol:\n            # print(f\"Loopy BP converged in {it+1} iterations\")\n            break\n\n    # Node beliefs from local fields + incoming messages\n    eta = h + np.sum(nu, axis=0)\n    m = sigma(eta)\n    return m, nu\n",
      "block_group": "c66cb96937b24e2bb80454e7e6830cea",
      "execution_count": 139,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "c3d2498cfbfb4ce28adf948dc9f4772e",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Run on data",
      "block_group": "360fb3d1c8cc4f28ab13b3b000378cca"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "784db286",
        "execution_start": 1767456407817,
        "execution_millis": 4669,
        "execution_context_id": "0743e833-a8b8-4afe-ad03-721ce7570625",
        "deepnote_to_be_reexecuted": true,
        "cell_id": "53e2767743c947c4848809ad8c2fc3c9",
        "deepnote_cell_type": "code"
      },
      "source": "N, D = X.shape\nK = mu.shape[1]\nsigma2 = sigma**2\n\nposterior_mf = np.zeros((N, K))\nposterior_bp = np.zeros((N, K))\n\nfor n in range(N):\n    x_n = X[n]\n\n    # Fields and couplings for this data point\n    # h_i = μ_i^T x / σ² − ½||μ_i||²/σ² + log(π_i / (1-π_i))\n    h_n = (mu.T @ x_n) / sigma2 \\\n          - 0.5 * np.sum(mu**2, axis=0) / sigma2 \\\n          + np.log(pie / (1 - pie))\n\n    # J_ij = μ_i^T μ_j / σ², diagonal = 0\n    J = (mu.T @ mu) / sigma2\n    np.fill_diagonal(J, 0.0)\n\n    # Mean-field for this point\n    lambda0 = 0.5 * np.ones(K)\n    lambda_hist, F_hist = MeanField_single(\n        x_n, mu, sigma, pie, lambda0,\n        maxsteps=200, tol=1e-10\n    )\n    posterior_mf[n] = lambda_hist[-1]  # final λ_n\n\n    # Loopy BP for this point\n    m_bp, _ = loopyBP(h_n, J, max_iters=50, tol=1e-6)\n    posterior_bp[n] = m_bp\n",
      "block_group": "48692193cd0b47cb9e0b4afbbdfbb9b6",
      "execution_count": 151,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "80da3ab2204e4fa0b6f60ad2d569a653",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## MF vs BP",
      "block_group": "db87bc04d8704556ae27fbd8a28d8f87"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "8423467a",
        "execution_start": 1767456420905,
        "execution_millis": 54,
        "execution_context_id": "0743e833-a8b8-4afe-ad03-721ce7570625",
        "deepnote_to_be_reexecuted": true,
        "cell_id": "4285e4bf89cb470e835deaf0030c306d",
        "deepnote_cell_type": "code"
      },
      "source": "# Mean absolute difference per factor\nmad_per_factor = np.mean(np.abs(posterior_mf - posterior_bp), axis=0)\nprint(\"Mean |MF - BP| per factor:\", mad_per_factor)\n\n# Overall mean absolute difference\nmad_overall = np.mean(np.abs(posterior_mf - posterior_bp))\nprint(\"Overall mean |MF - BP|:\", mad_overall)\n\n# Scatter plot over all N×K entries\nplt.figure(figsize=(4,4))\nplt.scatter(\n    posterior_mf.flatten(),\n    posterior_bp.flatten(),\n    alpha=0.3, s=10\n)\nplt.plot([0, 1], [0, 1], 'k--', linewidth=1)\nplt.xlabel('Mean-field E[s_i | x_n]')\nplt.ylabel('Loopy BP E[s_i | x_n]')\nplt.title('MF vs BP posterior means (all points)')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()",
      "block_group": "c09027d1439f4f6696f891a19f889381",
      "execution_count": 154,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=cb182644-878e-48cb-992b-68a78a5afe3d' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_persisted_session": {
      "createdAt": "2026-01-16T07:11:01.157Z"
    },
    "deepnote_full_width": true,
    "deepnote_notebook_id": "b771f1a76c094c60b897e28dd5e44013"
  }
}