{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxaf1HXo2iLB",
        "cell_id": "75a5fa3eefea49c5b148728e4bf55d36",
        "deepnote_cell_type": "markdown"
      },
      "source": "# EM Algorithm",
      "block_group": "1adad255803048b68e7aa54ce06fc2dd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zRa9MQiL1QW",
        "colab": {
          "height": 1000,
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15778fa3-ac6b-4ac0-9d82-8c39b0ef0679",
        "collapsed": true,
        "executionInfo": {
          "user": {
            "userId": "04808690080138473040",
            "displayName": "Stephanie Jat"
          },
          "status": "ok",
          "elapsed": 1018,
          "user_tz": 0,
          "timestamp": 1763774757891
        },
        "cell_id": "62f36ca99fa242d69a83e9b61471a0f6",
        "deepnote_cell_type": "code"
      },
      "source": "EM Algorithm\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Step 1: Load in data\n# Values are space-separated and there are 100 lines in the file:\nX = np.loadtxt('/datasets/t1cw-data/binarydigits.txt', dtype=int)\nN, D = X.shape # Number of samples and number of variables\nS = np.sum(X, axis=0) # Sum of ones per variable\n\n# Step 2: Define the log-Bernoulli PDF\ndef log_bernoulli_pdf(X, P):\n    # X: NxD data, P: KxD Bernoulli parameter matrix\n    # Returns NxK log pdf matrix\n    # Avoid log(0) by clipping P away from 0/1\n    eps = 1e-8\n    P = np.clip(P, eps, 1-eps)\n    log_p = np.log(P)\n    log_1mp = np.log(1-P)\n    return (X @ log_p.T) + ((1-X) @ log_1mp.T) # NxK\n\n# Step 3: EM Algorithm with weak symmetric priors\ndef em_map(X, K, max_iter=1000, tol=1e-4, alpha=1.01, beta=1.01, plot_trace=True, verbose=False): # Default values for alpha and beta are 1.01\n    \"\"\"\n    MAP Estimation for a mixture of K multivariate Bernoullis using (alpha, beta) weak symmetric priors.\n    alpha: prior for mixture proportions (Dirichlet),\n    beta: prior for Bernoulli parameters (Beta for each dimension).\n    \"\"\"\n    rng = np.random.default_rng() # Initialize random number generator\n    alpha = alpha\n    beta = beta\n    # Initialize\n    pi = np.ones(K) / K # Uniform prior for mixture proportions\n    P = rng.uniform(0.25, 0.75, size=(K, D)) # Uniform prior for Bernoulli parameters\n    log_trace = [] # Initialize log-trace to store log-posterior values\n\n    for it in range(max_iter): # Iterate over max_iter\n        # E-step: responsibilities as before\n        log_prob = log_bernoulli_pdf(X, P) + np.log(pi) # Calculate log-posterior\n        log_prob_max = np.max(log_prob, axis=1, keepdims=True) # Calculate log-posterior maximum\n        prob = np.exp(log_prob - log_prob_max)\n        gamma = prob / np.sum(prob, axis=1, keepdims=True) # Calculate responsibilities\n\n        # M-step: Update parameters with priors\n        Nk = np.sum(gamma, axis=0) # Effective counts\n        N = np.sum(Nk) # Total number of samples\n        N = N + 1e-12 # Avoid division by zero\n        Nk = Nk + 1e-12 # Avoid division by zero\n\n        # Dirichlet prior on pi: add (alpha-1) \"pseudo-counts\" to avoid log(0)\n        pi = (Nk + (alpha - 1)) / (N + K * (alpha - 1)) # Update mixture proportions\n\n        # Beta prior on Bernoulli parameters: each parameter beta[success, failure] = [beta, beta] to avoid log(0)\n        P = (gamma.T @ X + (beta - 1)) / (Nk[:, None] + 2 * (beta - 1)) # Update Bernoulli parameters\n        P = np.clip(P, 1e-6, 1-1e-6) # Clip Bernoulli parameters to avoid log(0)\n\n        # Log-posterior tracking\n        log_prior_pi = (alpha - 1) * np.sum(np.log(pi+1e-16)) # Add (alpha-1) \"pseudo-counts\" to avoid log(0)\n        log_prior_P = (beta - 1) * np.sum(np.log(P+1e-16) + np.log(1-P+1e-16)) # Add (beta-1) \"pseudo-counts\" to avoid log(0)\n        loglike = np.sum(np.log(np.sum(np.exp(log_bernoulli_pdf(X, P) + np.log(pi)), axis=1))) # Calculate log-likelihood\n        log_trace.append(loglike + log_prior_pi + log_prior_P) # Append log-posterior to log-trace\n        if it > 1 and np.abs(log_trace[-1] - log_trace[-2]) < tol: # Check if log-posterior has converged\n            break # Break if log-posterior has converged\n        if verbose:\n            print(f\"Iter {it}: log-posterior = {log_trace[-1]:.2f}\")\n\n    # Step 4: Plot log-trace\n    if plot_trace:\n        plt.plot(log_trace, marker='o') # Plot log-trace\n        plt.title(f\"EM Log-Posterior Trace, K={K}\")\n        plt.xlabel(\"Iteration\")\n        plt.ylabel(\"Log-Posterior\")\n        plt.show()\n\n    return loglike, pi, P, log_trace # Return log-likelihood, final mixture proportions, Bernoulli parameters, and log-trace\n\ndef estimated_model_bits(K, D, bits_per_float=32):\n    \"\"\"\n    Estimate the bits needed to encode the K-component mixture of D-dimensional Bernoullis.\n    bits_per_float can be 32 (single) or 64 (double).\n    \"\"\"\n    # Mixture weights: K-1 independent parameters (last is 1-sum)\n    header_bits = (K - 1) * bits_per_float\n    # Bernoulli parameters: K * D parameters\n    param_bits = K * D * bits_per_float\n    model_bits = header_bits + param_bits\n    return model_bits\n\n# Step 5: Run EM Algorithm for different values of K\nKs = [2, 3, 4, 7, 10]\nfor K in Ks:\n     loglike, pi, P, log_trace = em_map(X, K, max_iter=100, plot_trace=True) # Plot trace is True by default\n     print(f\"\\nK={K}, Final mixing proportions: {pi}\") # Print final mixing proportions\n     print(f\"Final Bernoulli parameters (first 3 components):\\n{P[:3]}\") # Print final Bernoulli parameters\n",
      "block_group": "e1b9f38a99ae41969c0fa03929ae2a53",
      "execution_count": 4,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzIOFsPBecgX",
        "cell_id": "58f8706861e8460f82b7fe3b9cd03ce7",
        "deepnote_cell_type": "markdown"
      },
      "source": "# Log-likelihoods nats to bits",
      "block_group": "27a38642f1fd45bb8f7fc0a18e83d26b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "2a5eefd4e2034755b99668dd1af7361b",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Gzip",
      "block_group": "bcc0c67370644085b473a03eb612d65e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvhK-Lu4lMPE",
        "executionInfo": {
          "user": {
            "userId": "04808690080138473040",
            "displayName": "Stephanie Jat"
          },
          "status": "ok",
          "elapsed": 4,
          "user_tz": 0,
          "timestamp": 1763774795001
        },
        "cell_id": "8b7f7f92408441759434fb5e25cc32fd",
        "deepnote_cell_type": "code"
      },
      "source": "# Bit-packing txt data for proper gziping\n# Ensure data is a numpy array of type uint8 [0, 1]\ndata = np.loadtxt('/datasets/t1cw-data/binarydigits.txt', dtype=int)\nmakebin = data.astype(np.uint8).flatten()  # Flatten so all binary digits are in one long vector\n# Pack bits: every 8 bits become one byte\npacked = np.packbits(makebin)\npacked.tofile('binarydigits_packed.raw')\n\n# Raw binary conversion for second Gzip option\n# Load .txt data (assume each row is whitespace-separated digits)\ntxt = np.loadtxt('/datasets/t1cw-data/binarydigits.txt', dtype=int)\n# Convert to uint8 (0/1)\nbin = txt.astype(np.uint8)\n# Save as raw binary (no separators, just N*D bytes)\nbin.tofile('binarydigits_bin.raw')",
      "block_group": "6278d1b672ef437e8f4f86f26e10c14d",
      "execution_count": 6,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "4c570f7d077140c89a3f0e217fe7c2d3",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Compare model costs",
      "block_group": "0beb5388a5384d5ab9fedba812966c2a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjZoxmlK0ICe",
        "colab": {
          "height": 1000,
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00715357-97f2-44be-9645-439aa4423d3e",
        "executionInfo": {
          "user": {
            "userId": "04808690080138473040",
            "displayName": "Stephanie Jat"
          },
          "status": "ok",
          "elapsed": 473,
          "user_tz": 0,
          "timestamp": 1763777866403
        },
        "cell_id": "812745fd3f0647538a8aea5bedf8c78e",
        "deepnote_cell_type": "code"
      },
      "source": "import os\nimport gzip # Import gzip module for compression\nimport numpy as np # Import numpy for array operations\nimport matplotlib.pyplot as plt # Import matplotlib for plotting\n\n# Calulate model costs and compare with naive encoding and Gzip compression (2\n# types, bit-packed and standard raw binary).\n\n# --- Gzip compression of raw data files ---\n# Gzip binarydigits_bin.raw\nwith open('/datasets/t1cw-data/binarydigits_bin.raw', 'rb') as f_in:\n    with gzip.open('/datasets/t1cw-data/binarydigits_bin.gz', 'wb') as f_out:\n        f_out.writelines(f_in)\n\n# Gzip binarydigits_packed.raw\nwith open('/datasets/t1cw-data/binarydigits_packed.raw', 'rb') as f_in:\n    with gzip.open('/datasets/t1cw-data/binarydigits_packed.gz', 'wb') as f_out:\n        f_out.writelines(f_in)\n# --- End of Gzip compression steps ---\n\n\n# For the EM model:\ntotal_costs = []\nmodel_bits_list = []\ndata_bits_list = []\n\nfor K in Ks:\n    loglike, pi, P, log_trace = em_map(X, K, max_iter=100, plot_trace=False)\n    # Data bits: negative log-likelihood in bits\n    data_bits = -loglike / np.log(2)\n    # Model bits: ≈ no of params × bits per parameter = header for mixture weights + Bernoulli params (sum to one)\n    model_bits = estimated_model_bits(K, D, bits_per_float=32)\n    # Total bits: model + data\n    total_cost = model_bits + data_bits\n    total_costs.append(total_cost)\n    model_bits_list.append(model_bits)\n    data_bits_list.append(data_bits)\n    print(f\"\\nK={K}\")\n    print(f\"Length of model-based coding in bits (total): {total_cost:.2f}\")\n    print(f\"Model-based coding bits per digit: {total_cost / (N*D):.4f}\")\n\n\n# Calculate baseline (i.e. naive encoding)\nnaive_bits = N * D # each digit uses 1 bit\nprint(\"\\nLength of naive encoding in bits:\", naive_bits)\n\n# For Gzip:\n# If bit-packed, then 8 digits per byte is the actual theoretical minimum, hence\n# 1 bit per digit, therefore file size (in bytes) × 8 = bits\npackedfilesize = os.path.getsize('/content/binarydigits_packed.gz')\ngzip_packed_bits = packedfilesize * 8\nprint(f\"Length of bit-packed Gzip encoding in bits: {gzip_packed_bits}\")\n\n\n# If not bit-packed, file has 1 digit per byte, which means 8 bits per digit is\n# the lower bound for gzip\nbinfilesize = os.path.getsize('/content/binarydigits_bin.gz')\ngzip_raw_bin_bits = binfilesize * 8\nprint(f\"Length of raw binary Gzip encoding in bits: {gzip_raw_bin_bits}\")",
      "block_group": "4501dd4d386a431cb32cf80318cc5c19",
      "execution_count": 8,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "9618453d0b5c449398bf3ea70c3d3415",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Plots",
      "block_group": "7f1de8c5358445b08af51d49c467c3b6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "8903b0027c9349ebbb70b43378547755",
        "deepnote_cell_type": "code"
      },
      "source": "# Plot to figure\nplt.figure(figsize=(8,6))\nplt.plot(Ks, total_costs, 'o-', label=\"Total Cost (Model+Data)\")\nplt.plot(Ks, model_bits_list, 's--', label=\"Model Header (bits)\")\nplt.plot(Ks, data_bits_list, 'x--', label=\"Data Bits (neg. loglike)\")\nplt.axhline(naive_bits, color='grey', linestyle=':', label=\"Naive Encoding (1 bit/digit)\")\nplt.axhline(gzip_packed_bits, color='red', linestyle='--', label=\"Bit-packed Gzip Encoding\")\nplt.axhline(gzip_raw_bin_bits, color='magenta', linestyle='-', label=\"Raw Binary Gzip Encoding\") # Adjusted label\nplt.xlabel(\"K\")\nplt.ylabel(\"Bits\")\nplt.title(\"Total Code Length vs. K (MDL Principle)\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# Plot \"bits per digit\" for completeness\nplt.figure(figsize=(8,6))\nplt.plot(Ks, np.array(total_costs)/(N*D), linestyle='dashdot', label=\"EM\")\nplt.axhline(1, color='grey', linestyle=':', label=\"Naive Baseline (1 bit/digit)\")\nplt.axhline(gzip_packed_bits/(N*D), color='red', linestyle='--', label=\"Bit-packed Gzip\")\nplt.axhline(gzip_raw_bin_bits/(N*D), color='magenta', linestyle='-', label=\"Raw Binary Gzip\")\nplt.xlabel(\"K\")\nplt.ylabel(\"Bits per Digit\")\nplt.title(\"Bits per Digit vs. K\")\nplt.legend()\nplt.tight_layout()\nplt.show()",
      "block_group": "414d79c32c7f496ab25e245d4a865723",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7h_PUuV93ix",
        "cell_id": "c4e8006e8cda47a599c0e4164ee3c433",
        "deepnote_cell_type": "markdown"
      },
      "source": "# Analysis Aids",
      "block_group": "316fe72b44be497493e75fd373c29b01"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS_ZzK2JReE_",
        "colab": {
          "height": 172,
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa94c81b-5e54-491a-c03d-b50f99e2f98d",
        "executionInfo": {
          "user": {
            "userId": "04808690080138473040",
            "displayName": "Stephanie Jat"
          },
          "status": "ok",
          "elapsed": 377,
          "user_tz": 0,
          "timestamp": 1762737083806
        },
        "cell_id": "cef0300c8e024250a844cf0f1de09df6",
        "deepnote_cell_type": "code"
      },
      "source": "# Collage figures for comparison\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# List of file names for your PNG plots\npng_files = ['x.png', 'x.png', 'x.png', 'x.png', 'x.png']\n\nfig, axes = plt.subplots(1, len(png_files), figsize=(5 * len(png_files), 5))\nfor i, png in enumerate(png_files):\n    img = mpimg.imread(png)\n    axes[i].imshow(img)\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.title(f\"EM Log-Posterior Traces, K = 10\")\nplt.show()\n",
      "block_group": "f5da408a4bc942a5bd1b340836eaaac2",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=cb182644-878e-48cb-992b-68a78a5afe3d' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_full_width": true,
    "deepnote_notebook_id": "90b1f80af6124b8aaf66c7c4c5db386e"
  }
}