{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av-SAIoa_2ap",
        "cell_id": "89cfc9e2b86541fc850554020880bd62",
        "deepnote_cell_type": "markdown"
      },
      "source": "# Kalman Filtering VS Smoothing",
      "block_group": "0d6b7d330c3e4c2eaea1e0c6f1752be0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3AuMnPUhC2L",
        "cellView": "form",
        "executionInfo": {
          "user": {
            "userId": "04808690080138473040",
            "displayName": "Stephanie Jat"
          },
          "status": "ok",
          "elapsed": 4,
          "user_tz": 0,
          "timestamp": 1763083138512
        },
        "cell_id": "6ac777e0c94f4444a7f3a0f53718bc24",
        "deepnote_cell_type": "code"
      },
      "source": "\"\"\"\n    File name: ssm_kalman.py\n    Description: a re-implementation of the Kalman filter for http://www.gatsby.ucl.ac.uk/teaching/courses/ml1\n    Author: Roman Pogodin / Maneesh Sahani (matlab version)\n    Date created: October 2018\n    Python version: 3.6\n\"\"\"\n\nimport numpy as np\n\n# function given on ssm_kalman.py\ndef run_ssm_kalman(X, y_init, Q_init, A, Q, C, R, mode='smooth'):\n    \"\"\"\n    Calculates kalman-smoother estimates of SSM state posterior.\n    :param X:       data, [d, t_max] numpy array\n    :param y_init:  initial latent state, [k,] numpy array\n    :param Q_init:  initial variance, [k, k] numpy array\n    :param A:       latent dynamics matrix, [k, k] numpy array\n    :param Q:       innovariations covariance matrix, [k, k] numpy array\n    :param C:       output loading matrix, [d, k] numpy array\n    :param R:       output noise matrix, [d, d] numpy array\n    :param mode:    'forw' or 'filt' for forward filtering, 'smooth' for also backward filtering\n    :return:\n    y_hat:      posterior mean estimates, [k, t_max] numpy array\n    V_hat:      posterior variances on y_t, [t_max, k, k] numpy array\n    V_joint:    posterior covariances between y_{t+1}, y_t, [t_max, k, k] numpy array\n    likelihood: conditional log-likelihoods log(p(x_t|x_{1:t-1})), [t_max,] numpy array\n    \"\"\"\n    d, k = C.shape\n    t_max = X.shape[1]\n\n    # dimension checks\n    assert np.all(X.shape == (d, t_max)), \"Shape of X must be (%d, %d), %s provided\" % (d, t_max, X.shape)\n    assert np.all(y_init.shape == (k,)), \"Shape of y_init must be (%d,), %s provided\" % (k, y_init.shape)\n    assert np.all(Q_init.shape == (k, k)), \"Shape of Q_init must be (%d, %d), %s provided\" % (k, k, Q_init.shape)\n    assert np.all(A.shape == (k, k)), \"Shape of A must be (%d, %d), %s provided\" % (k, k, A.shape)\n    assert np.all(Q.shape == (k, k)), \"Shape of Q must be (%d, %d), %s provided\" % (k, k, Q.shape)\n    assert np.all(C.shape == (d, k)), \"Shape of C must be (%d, %d), %s provided\" % (d, k, C.shape)\n    assert np.all(R.shape == (d, d)), \"Shape of R must be (%d, %d), %s provided\" % (d, k, R.shape)\n\n    y_filt = np.zeros((k, t_max))  # filtering estimate: \\hat(y)_t^t\n    V_filt = np.zeros((t_max, k, k))  # filtering variance: \\hat(V)_t^t\n    y_hat = np.zeros((k, t_max))  # smoothing estimate: \\hat(y)_t^T\n    V_hat = np.zeros((t_max, k, k))  # smoothing variance: \\hat(V)_t^T\n    K = np.zeros((t_max, k, X.shape[0]))  # Kalman gain\n    J = np.zeros((t_max, k, k))  # smoothing gain\n    likelihood = np.zeros(t_max)  # conditional log-likelihood: p(x_t|x_{1:t-1})\n\n    I_k = np.eye(k)\n\n    # forward pass\n\n    V_pred = Q_init\n    y_pred = y_init\n\n    for t in range(t_max):\n        x_pred_err = X[:, t] - C.dot(y_pred)\n        V_x_pred = C.dot(V_pred.dot(C.T)) + R\n        V_x_pred_inv = np.linalg.inv(V_x_pred)\n        likelihood[t] = -0.5 * (np.linalg.slogdet(2 * np.pi * (V_x_pred))[1] +\n                                x_pred_err.T.dot(V_x_pred_inv).dot(x_pred_err))\n\n        K[t] = V_pred.dot(C.T).dot(V_x_pred_inv)\n\n        y_filt[:, t] = y_pred + K[t].dot(x_pred_err)\n        V_filt[t] = V_pred - K[t].dot(C).dot(V_pred)\n\n        # symmetrise the variance to avoid numerical drift\n        V_filt[t] = (V_filt[t] + V_filt[t].T) / 2.0\n\n        y_pred = A.dot(y_filt[:, t])\n        V_pred = A.dot(V_filt[t]).dot(A.T) + Q\n\n    # backward pass\n\n    if mode == 'filt' or mode == 'forw':\n        # skip if filtering/forward pass only\n        y_hat = y_filt\n        V_hat = V_filt\n        V_joint = None\n    else:\n        V_joint = np.zeros_like(V_filt)\n        y_hat[:, -1] = y_filt[:, -1]\n        V_hat[-1] = V_filt[-1]\n\n        for t in range(t_max - 2, -1, -1):\n            J[t] = V_filt[t].dot(A.T).dot(np.linalg.inv(A.dot(V_filt[t]).dot(A.T) + Q))\n            y_hat[:, t] = y_filt[:, t] + J[t].dot((y_hat[:, t + 1] - A.dot(y_filt[:, t])))\n            V_hat[t] = V_filt[t] + J[t].dot(V_hat[t + 1] - A.dot(V_filt[t]).dot(A.T) - Q).dot(J[t].T)\n\n        V_joint[-2] = (I_k - K[-1].dot(C)).dot(A).dot(V_filt[-2])\n\n        for t in range(t_max - 3, -1, -1):\n            V_joint[t] = V_filt[t + 1].dot(J[t].T) + J[t + 1].dot(V_joint[t + 1] - A.dot(V_filt[t + 1])).dot(J[t].T)\n\n    return y_hat, V_hat, V_joint, likelihood\n",
      "block_group": "02fd989ee4f5446ca80ab23dd05e13dd",
      "execution_count": 55,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "a3907425b4da4274b6614e39bc91dd6a",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Initialise parameters",
      "block_group": "4e38d65ae0bc40ea93b9adb89c2b1c4f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQeHAXSUh9S9",
        "cellView": "form",
        "executionInfo": {
          "user": {
            "userId": "04808690080138473040",
            "displayName": "Stephanie Jat"
          },
          "status": "ok",
          "elapsed": 12,
          "user_tz": 0,
          "timestamp": 1763083139398
        },
        "cell_id": "5fe364e7744145d29f89938d5dd4f3f6",
        "deepnote_cell_type": "code"
      },
      "source": "data = np.loadtxt('/datasets/t1cw-data/ssm_spins.txt')  # data shape: (1000, 5)\ndata = data.T                    # Transpose to shape (5, 1000)\nX = data\n\n# Initial state mean and covariance\ny_init = np.zeros(4)        # [0, 0, 0, 0]\nQ_init = np.eye(4)          # 4x4 identity\n\n# System matrices\n# Latent dynamics (4x4)\nA = 0.99 * np.array([\n    [np.cos(2*np.pi/180), -np.sin(2*np.pi/180), 0, 0],\n    [np.sin(2*np.pi/180),  np.cos(2*np.pi/180), 0, 0],\n    [0, 0, np.cos(2*np.pi/90), -np.sin(2*np.pi/90)],\n    [0, 0, np.sin(2*np.pi/90),  np.cos(2*np.pi/90)],\n])\n\n# Numerically,\n# A = [[ 0.98939692 -0.03455050  0.          0.        ]\n#      [ 0.03455050  0.98939692  0.          0.        ]\n#      [ 0.          0.          0.98758841 -0.06905891]\n#      [ 0.          0.          0.06905891  0.98758841]]\n\n# Process noise covariance (4x4)\nQ = np.eye(4) - np.dot(A, A.T)\n\n# Numerically:\n# Q = [[ 1.99000000e-02 -4.00127539e-19  0.00000000e+00  0.00000000e+00]\n#      [-4.00127539e-19  1.99000000e-02  0.00000000e+00  0.00000000e+00]\n#      [ 0.00000000e+00  0.00000000e+00  1.99000000e-02  1.68953551e-18]\n#      [ 0.00000000e+00  0.00000000e+00  1.68953551e-18  1.99000000e-02]]\n\n# Observation matrix (5x4)\nC = np.array([\n    [1, 0, 1, 0],\n    [0, 1, 0, 1],\n    [1, 0, 0, 1],\n    [0, 0, 1, 1],\n    [0.5, 0.5, 0.5, 0.5],\n])\n\n# Numerically:\n# C = [[1. , 0. , 1. , 0. ],\n#      [0. , 1. , 0. , 1. ],\n#      [1. , 0. , 0. , 1. ],\n#      [0. , 0. , 1. , 1. ],\n#      [0.5, 0.5, 0.5, 0.5]]\n\n# Observation noise covariance as identity\nR = np.eye(5)\n\ntrue_params = {}\ntrue_params['A'] = A\ntrue_params['Q'] = Q\ntrue_params['C'] = C\ntrue_params['R'] = R",
      "block_group": "97e39751de694e80add47bb99abf0a70",
      "execution_count": 56,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "10c7a42b75d44ab2a59506814b8f4c17",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Log Determinants",
      "block_group": "9cdc750d5b6e4e61a57a6ea3f46afbe9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gHsICQTmD70",
        "cellView": "form",
        "executionInfo": {
          "user": {
            "userId": "04808690080138473040",
            "displayName": "Stephanie Jat"
          },
          "status": "ok",
          "elapsed": 205,
          "user_tz": 0,
          "timestamp": 1763083140431
        },
        "cell_id": "4d58a5633f7d4fbc91139a0d92f9e4cc",
        "deepnote_cell_type": "code"
      },
      "source": "# Log-determinant of covariance at each time step\ndef logdet(A):\n    # Safely computes log(det(A)) via Cholesky\n    return 2 * np.sum(np.log(np.diag(np.linalg.cholesky(A))))\n\n# Smoothing (mode='smooth')\ny, V, Vj, log = run_ssm_kalman(X, y_init, Q_init, A, Q, C, R, mode='smooth')\n\nlogdet_smooth = [logdet(V[t]) for t in range(V.shape[0])]\n\n# Filtering (mode='filt')\nYfilt, Vfilt, _, Lfilt = run_ssm_kalman(X, y_init, Q_init, A, Q, C, R, mode='filt')\n\nlogdet_filt = [logdet(Vfilt[t]) for t in range(Vfilt.shape[0])]",
      "block_group": "0d3300daa6df4f56b1f0401f7d9d1e0e",
      "execution_count": 57,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "06c165ae4d8b484cb94dc77f6625b380",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Plot results",
      "block_group": "7981f883a2784b94a5445e73080e09e4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02hIvUnWuqdK",
        "colab": {
          "height": 1000,
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "a7c227fe-8cc9-43de-f022-9645340caeed",
        "collapsed": true,
        "executionInfo": {
          "user": {
            "userId": "04808690080138473040",
            "displayName": "Stephanie Jat"
          },
          "status": "ok",
          "elapsed": 2125,
          "user_tz": 0,
          "timestamp": 1763083143632
        },
        "cell_id": "68605789b8ef4e008555a89ca3d05886",
        "deepnote_cell_type": "code"
      },
      "source": "import matplotlib.pyplot as plt\n\n# ----- Smoothed data -----\n\nplt.figure(figsize=(10,6))\nfor i in range(y.shape[0]): # number of latent dimensions (should be 4)\n    plt.plot(y[i, :], label=f\"Latent state y[{i}]\")\nplt.title(\"Smoothed Latent State Means\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Latent state\")\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(10,6))\nfor i in range(V.shape[1]): # number of latent dimensions (should be 4)\n    plt.plot(V[:, i, i], label=f\"Variance y[{i}]\")\nplt.title(\"Smoothed Variance of Latent States (Diagonal of Posterior Variance)\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Variance\")\nplt.legend()\nplt.show()\n\nplt.plot(Vj[:, 0, 1], label=\"Cov(y[0], y[1]) (t, t+1)\")\nplt.title(\"Smoothed Lag-1 Cross-Covariances between y[0] and y[1]\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Covariance\")\nplt.legend()\nplt.show()\n\nplt.plot(log, label=\"log p(x_t | x_{1:t-1})\")\nplt.title(\"Smoothed Kalman Filter Log-Likelihood\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Log-Likelihood\")\nplt.legend()\nplt.show()\n\nplt.figure()\nplt.plot(logdet_smooth)\nplt.title(\"Log-Determinant (Smoothing Covariance)\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"log(det(V_t))\")\nplt.show()\n\n# ----- Filtered data -----\n\nplt.figure(figsize=(10,6))\nfor i in range(Yfilt.shape[0]):\n    plt.plot(Yfilt[i, :], label=f\"y{str(i)}\")\nplt.title(\"Unsmoothed Latent States\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Latent State Value\")\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(10,6))\nfor i in range(Vfilt.shape[1]):\n    plt.plot(Vfilt[:, i, i], label=f\"Variance y[{i}]\")\nplt.title(\"Unsmoothed Variance of Latent States (Diagonal of Posterior Variance)\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Variance\")\nplt.legend()\nplt.show()\n\nplt.plot(Lfilt, label=\"log p(x_t | x_{1:t-1})\")\nplt.title(\"Kalman Filter Log-Likelihood (Unsmoothed)\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Log-Likelihood\")\nplt.legend()\nplt.show()\n\nplt.figure()\nplt.plot(logdet_filt)\nplt.title(\"Log-Determinant (Filtering Covariance)\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"log(det(Vfilt_t))\")\nplt.show()",
      "block_group": "57038fea8ae64104bb2049e6b73190fd",
      "execution_count": 58,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNx728eW_6_T",
        "cell_id": "0f3bcb45dba34c659a53ec2d176ac778",
        "deepnote_cell_type": "markdown"
      },
      "source": "# Learn parameters using EM",
      "block_group": "a0410e7017b348a1bf840414d59842cc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYxWrbDV_zhd",
        "cellView": "form",
        "executionInfo": {
          "user": {
            "userId": "04808690080138473040",
            "displayName": "Stephanie Jat"
          },
          "status": "ok",
          "elapsed": 4,
          "user_tz": 0,
          "timestamp": 1763082693112
        },
        "cell_id": "ede503e0e7e0497690caec3a59cbd40e",
        "deepnote_cell_type": "code"
      },
      "source": "def regularize_cov(V_hat, eps=1e-6):\n    \"\"\"Ensure covariance matrix is positive definite with regularization.\"\"\"\n    # Add a small constant times identity to diagonal if necessary\n    cov_reg = (V_hat + V_hat.T) / 2  # Force symmetry\n    min_eig = np.min(np.linalg.eigvalsh(cov_reg))\n    if min_eig < eps:\n        cov_reg += np.eye(cov_reg.shape[0]) * (eps - min_eig)\n    return cov_reg\n\ndef lgssm(X, y_init, Q_init, A, Q, C, R, n_iters=50, reg_eps=1e-6, verbose=True):\n    T = X.shape[1]\n    k = y_init.shape[0]\n    d = X.shape[0]\n    logs = []\n    A_chg, C_chg, Q_chg, R_chg = [], [], [], []\n\n    for em_step in range(n_iters):\n        # === E-Step ===\n        y, V, Vj, log= run_ssm_kalman(X, y_init, Q_init, A, Q, C, R, mode='smooth')\n\n        # Store log-likelihood for plotting\n        logs.append(np.sum(log))  # log is per iteration\n\n        # === Sufficient Statistics ===\n        # Observation stats\n        Exy = np.zeros((d, k))\n        Exx = np.zeros((d, d))\n        Eyy = np.zeros((k, k))\n        for t in range(T):\n            Exy += np.outer(X[:, t], y[:, t])       # X_t y_t^T\n            Exx += np.outer(X[:, t], X[:, t])           # X_t X_t^T\n            Eyy += V[t] + np.outer(y[:, t], y[:, t])  # E[y_t y_t^T]\n\n        # === M-Step: Update C and R ===\n        C_new = np.dot(Exy, np.linalg.inv(Eyy))\n        R_new = (Exx - np.dot(Exy, C_new.T)) / T\n        R_new = regularize_cov(R_new, reg_eps)\n\n        # Lag-1 sufficient statistics for A and Q\n        Ey_lag = y[:, :-1]\n        Ey_lead = y[:, 1:]\n        V_lag = V[:-1]\n        V_lead = V[1:]\n        Vj_lead = Vj[1:]  # shape [T-1, k, k]\n\n        Eyylead = np.zeros((k, k))\n        Eyylag = np.zeros((k, k))\n        Eyylag_self = np.zeros((k, k))\n        for t in range(T-1):\n            Eyylead += V_lead[t] + np.outer(Ey_lead[:, t], Ey_lead[:, t])\n            Eyylag += Vj_lead[t] + np.outer(Ey_lead[:, t], Ey_lag[:, t])\n            Eyylag_self += V_lag[t] + np.outer(Ey_lag[:, t], Ey_lag[:, t])\n\n        # Update A\n        A_new = np.dot(Eyylag, np.linalg.inv(Eyylag_self))\n        # Update Q\n        Q_new = (Eyylead - np.dot(Eyylag, A_new.T)) / (T-1)\n        Q_new = regularize_cov(Q_new, reg_eps)\n\n        # Track parameter changes\n        A_chg.append(np.linalg.norm(A_new - A))\n        C_chg.append(np.linalg.norm(C_new - C))\n        Q_chg.append(np.linalg.norm(Q_new - Q))\n        R_chg.append(np.linalg.norm(R_new - R))\n\n        A, Q, C, R = A_new, Q_new, C_new, R_new\n\n    return A, Q, C, R, logs, A_chg, Q_chg, C_chg, R_chg\n",
      "block_group": "5c66cf42feff434f9de740988177a21f",
      "execution_count": 52,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "ab3eef526ab440b58ec66e4df6c46509",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Training data likelihood: true parameters VS random initialisations",
      "block_group": "01b7451182324ad4b267003e2a6b5fdf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT-yFpO4VLLo",
        "colab": {
          "height": 339,
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed5446f8-1d1c-4e64-a8e8-16b29e83f082",
        "collapsed": true,
        "executionInfo": {
          "user": {
            "userId": "04808690080138473040",
            "displayName": "Stephanie Jat"
          },
          "status": "ok",
          "elapsed": 82240,
          "user_tz": 0,
          "timestamp": 1763083594166
        },
        "cell_id": "39a5d349f0c14235899e91cad91444c4",
        "deepnote_cell_type": "code"
      },
      "source": "# Run 50 EM iterations starting from both the true/generating parameters and\n# several random initialisations\n\nn_runs = 11  # 1 generating param, 10 random\nn_iters = 50\nlogtrace = []\nlabels = []\nem_runs = []       # List of dicts for each run\nparam_sets = {}    # Dictionary of parameter tuples for likelihood/test eval\n\n# Log-likeihood for true parameters\n_, _, _, log_true = run_ssm_kalman(X, y_init, Q_init, true_params['A'], true_params['Q'], true_params['C'], true_params['R'], mode='filt')\n\n# Add to param_sets\nparam_sets['true'] = (true_params['A'], true_params['Q'], true_params['C'], true_params['R'])\n\n\nfor i in range(n_runs):\n    if i == 0:\n        A0 = A.copy()\n        Q0 = Q.copy()\n        C0 = C.copy()\n        R0 = R.copy()\n        label = \"EM_true\"\n    else:\n        k = 4\n        d = 5\n        rng = np.random.default_rng(i)\n        A0 = rng.standard_normal((k, k))\n        Q0 = np.eye(k)\n        C0 = rng.standard_normal((d, k))\n        R0 = np.eye(d)\n        label = f\"EM_rand_{i}\"\n\n    A_final, Q_final, C_final, R_final, log, A_chg, Q_chg, C_chg, R_chg = lgssm(X, y_init, Q_init, A0, Q0, C0, R0, n_iters=n_iters, verbose=False)\n\n    em_params = {\n        'A': A_final,\n        'Q': Q_final,\n        'C': C_final,\n        'R': R_final,\n        'logs': log,\n        'A_chg': A_chg,\n        'Q_chg': Q_chg,\n        'C_chg': C_chg,\n        'R_chg': R_chg\n    }\n    em_runs.append(em_params)\n    param_sets[label] = (A_final, Q_final, C_final, R_final)\n    logtrace.append(log)\n    labels.append(label)  # sum across iterations\n\n# Show how the likelihood increases over the EM iterations (hand in a plot\nplt.figure(figsize=(12,7))\nfor likes, label in zip(logtrace, labels):\n    plt.plot(likes, label=label)\nplt.xlabel(\"EM Iteration\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Likelihood vs EM Iteration for Each Run\")\nplt.legend()\nplt.show()",
      "block_group": "0edbee1357ed4328b4da6fc57490b2be",
      "execution_count": 60,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "ff7647438d3b4c3da53514763c88924c",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## SSID function",
      "block_group": "25696288245d4b0d96155d1acf1ed95f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quY7SxHNnd3i",
        "executionInfo": {
          "user": {
            "userId": "04808690080138473040",
            "displayName": "Stephanie Jat"
          },
          "status": "ok",
          "elapsed": 6476,
          "user_tz": 0,
          "timestamp": 1763080772676
        },
        "cell_id": "e8ff1461483043309dbeb08b7ee256f2",
        "deepnote_cell_type": "code"
      },
      "source": "def ssid(X, k=4):\n    # X: observed data, shape (d, T)\n    # k: latent state dimension\n\n    # Estimate C: least squares on principal components\n    U, S, Vh = np.linalg.svd(X, full_matrices=False)\n    C_SSID = U[:, :k]\n\n    # Estimate latent Y by projecting onto C\n    Y_est = np.dot(C_SSID.T, X)\n\n    # Estimate A using linear regression between Y_t and Y_{t-1}\n    Y_t = Y_est[:, 1:]\n    Y_tm1 = Y_est[:, :-1]\n    A_SSID = np.linalg.lstsq(Y_tm1.T, Y_t.T, rcond=None)[0].T\n\n    # Estimate Q, R as sample variances\n    Q_SSID = np.cov(Y_t - np.dot(A_SSID, Y_tm1))\n    R_SSID = np.cov(X - np.dot(C_SSID, Y_est))\n\n    return A_SSID, Q_SSID, C_SSID, R_SSID\n\nA_SSID, Q_SSID, C_SSID, R_SSID = ssid(X, k=4)\nparam_sets[\"SSID\"] = (A_SSID, Q_SSID, C_SSID, R_SSID)\n\n# EM initialized at SSID solution\nA_EM_SSID, Q_EM_SSID, C_EM_SSID, R_EM_SSID, log, *_ = lgssm(X, y_init, Q_init, A_SSID, Q_SSID, C_SSID, R_SSID, n_iters=n_iters, verbose=False)\n\n# Add parameters to param_sets\nparam_sets[\"SSID+EM\"] = (A_EM_SSID, Q_EM_SSID, C_EM_SSID, R_EM_SSID)\n",
      "block_group": "d9f20399ece948279b27d6897e00ee22",
      "execution_count": 39,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "is_collapsed": false,
        "formattedRanges": [],
        "deepnote_app_block_visible": false,
        "cell_id": "982fadd7fa0b439082346ce8ae36649b",
        "deepnote_cell_type": "text-cell-h1"
      },
      "source": "# Test data likelihood: true parameters VS random initialisations",
      "block_group": "3f1da9836f9b4d5783f4a416b2ea72e1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wtu6lWIWXX3_",
        "collapsed": true,
        "executionInfo": {
          "user": {
            "userId": "04808690080138473040",
            "displayName": "Stephanie Jat"
          },
          "status": "ok",
          "elapsed": 966,
          "user_tz": 0,
          "timestamp": 1763083608321
        },
        "cell_id": "52720d5547364b50981c4c35811bea1c",
        "deepnote_cell_type": "code"
      },
      "source": "# Evaluate the likelihood of the test data under all of\n# parameters found above\n\n# Load the test data\ntestdata = np.loadtxt('/datasets/t1cw-data/ssm_spins_test.txt').T  # shape: (5, N)\nX = testdata\n\ntest_results = []\n    # Test set likelihood\nfor label, (A, Q, C, R) in param_sets.items():\n    # Run the filter (not smoother!), get conditional likelihoods\n    _, _, _, log_test = run_ssm_kalman(X, y_init, Q_init, A, Q, C, R, mode='filt')\n    test_results.append(np.sum(log_test))",
      "block_group": "983d31e5ac9a484b8cd1054504c9dfdc",
      "execution_count": 61,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "d1f48d5a94c347598c5c5e69d8e92f75",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Test VS Training results",
      "block_group": "ab52e07dc1e245aebab0db2ddae085d3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgM5ftXF6Egc",
        "colab": {
          "height": 249,
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aebd9bc3-a472-4f95-a4bb-1b3070b0df5f",
        "executionInfo": {
          "user": {
            "userId": "04808690080138473040",
            "displayName": "Stephanie Jat"
          },
          "status": "ok",
          "elapsed": 1529,
          "user_tz": 0,
          "timestamp": 1763083775650
        },
        "cell_id": "49fe4cf3296143fdacf83c2b165a198d",
        "deepnote_cell_type": "code"
      },
      "source": "# Plot test vs training log-likelihoods\n\n# Make sure true parameter log trace is a scalar\nlog_true = np.sum(log_true)\n\nfig, axs = plt.subplots(1, 2, figsize=(16,7))  # 1 row, 2 columns\n\n# Training data log-likelihood curves (EM trajectories)\nfor likes, label in zip(logtrace, labels):\n    axs[0].plot(likes, label=label)\naxs[0].axhline(log_true, color='black', linestyle='--', label='True')\naxs[0].set_title('Training Data Log-Likelihood')\naxs[0].set_xlabel('EM Iteration')\naxs[0].set_ylabel('Log-Likelihood')\naxs[0].legend()\n\n# Test data log-likelihoods (final values only, or dots/lines as you prefer)\nfor i, label in enumerate(labels):\n    # You might plot a flat line, or just the final values\n    axs[1].scatter(i, test_results[i], color='green', s=60)\n    axs[1].text(i+0.1, test_results[i], f'{label}\\n{test_results[i]:.1f}', fontsize=9, color='green', va='bottom')\naxs[1].set_title('Test Data Log-Likelihoods')\naxs[1].set_xlabel('Run')\naxs[1].set_ylabel('Log-Likelihood')\naxs[1].set_xticks(range(len(labels)))\naxs[1].set_xticklabels(labels, rotation=45)\n\nplt.tight_layout()\nplt.show()\n",
      "block_group": "9381cac3ba0f4cca9db03a42a251c585",
      "execution_count": 64,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=cb182644-878e-48cb-992b-68a78a5afe3d' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_full_width": true,
    "deepnote_notebook_id": "062fabb36f3b46c1be7d999082f2dea2"
  }
}