{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGrUe9v94HTn",
        "cell_id": "903a2491b1eb4f5fb5bd58927ada00d8",
        "deepnote_cell_type": "markdown"
      },
      "source": "# Gibbs Sampling on Toy Example",
      "block_group": "9bb7ee9e50fc4c13a6b7e07eddfa2b31"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFpGwbAphTed",
        "colab": {
          "height": 1000,
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da023b05-fe44-4784-83ad-93da2c2155a6",
        "collapsed": true,
        "executionInfo": {
          "user": {
            "userId": "04808690080138473040",
            "displayName": "Stephanie Jat"
          },
          "status": "ok",
          "elapsed": 6432,
          "user_tz": 0,
          "timestamp": 1763900869222
        },
        "cell_id": "3052dfb7fd5943a5b1a18fd5348b98f4",
        "deepnote_cell_type": "code"
      },
      "source": "\"\"\"\n    File name: gibbs_sampler.py\n    Description: a re-implementation of the Gibbs sampler for http://www.gatsby.ucl.ac.uk/teaching/courses/ml1\n    Author: python: Roman Pogodin, MATLAB (original): Yee Whye Teh and Maneesh Sahani\n    Date created: October 2018\n    Python version: 3.6\n\"\"\"\n# gibbs_sampler.py file provided. Complete sections marked todo\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import gammaln\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# todo: sample everything from self.rang_gen to control the random seed (works as numpy.random)\nclass GibbsSampler:\n    def __init__(self, n_docs, n_topics, n_words, alpha, beta, random_seed=None):\n        \"\"\"\n        :param n_docs:          number of documents\n        :param n_topics:        number of topics\n        :param n_words:         number of words in vocabulary\n        :param alpha:           dirichlet parameter on topic mixing proportions\n        :param beta:            dirichlet parameter on topic word distributions\n        :param random_seed:     random seed of the sampler\n        \"\"\"\n        self.n_docs = n_docs\n        self.n_topics = n_topics\n        self.n_words = n_words\n        self.alpha = alpha\n        self.beta = beta\n        self.rand_gen = np.random.RandomState(random_seed)\n\n        self.docs_words = np.zeros((self.n_docs, self.n_words))\n        self.docs_words_test = None\n        self.loglike = None\n        self.loglike_test = None\n        self.do_test = False\n\n        self.A_dk = np.zeros((self.n_docs, self.n_topics))  # number of words in document d assigned to topic k\n        self.B_kw = np.zeros((self.n_topics, self.n_words))  # number of occurrences of word w assigned to topic k\n        self.A_dk_test = np.zeros((self.n_docs, self.n_topics))\n        self.B_kw_test = np.zeros((self.n_topics, self.n_words))\n\n        self.theta = np.ones((self.n_docs, self.n_topics)\n                             ) / self.n_topics  # theta[d] is the distribution over topics in document d\n        self.phi = np.ones((self.n_topics, self.n_words)) / self.n_words  # phi[k] is the distribution words in topic k\n\n        self.topics_space = np.arange(self.n_topics)\n        self.topic_doc_words_distr = np.zeros((self.n_docs, self.n_words, self.n_topics))  # z_id|x_id, theta, phi\n\n    def init_sampling(self, docs_words, docs_words_test=None,\n                      theta=None, phi=None, n_iter=0, save_loglike=False):\n        # Update dimensions from actual data\n        self.n_docs = docs_words.shape[0]\n        self.n_words = docs_words.shape[1]\n\n        self.docs_words = docs_words\n        self.docs_words_test = docs_words_test\n\n        self.do_test = (docs_words_test is not None)\n\n        if save_loglike:\n            self.loglike = np.zeros(n_iter)\n\n            if self.do_test:\n                self.loglike_test = np.zeros(n_iter)\n\n        # Resize arrays to match actual data dimensions\n        self.A_dk = np.zeros((self.n_docs, self.n_topics))\n        self.B_kw = np.zeros((self.n_topics, self.n_words))\n        self.A_dk_test = np.zeros((self.n_docs, self.n_topics))\n        self.B_kw_test = np.zeros((self.n_topics, self.n_words))\n\n        # Resize topic_doc_words_distr - will be properly set in init_params\n        self.topic_doc_words_distr = np.zeros((self.n_docs, self.n_words, self.n_topics))\n\n        self.init_params(theta, phi)\n\n    def init_params(self, theta=None, phi=None):\n        D = self.n_docs\n        K = self.n_topics\n        W = self.n_words\n\n        if theta is None:\n            self.theta = np.ones((D, K)) / K\n        else:\n            self.theta = theta.copy()\n\n        if phi is None:\n            self.phi = np.ones((K, W)) / W\n        else:\n            self.phi = phi.copy()\n\n        self.update_topic_doc_words()\n        self.sample_counts()\n\n    def run(self, docs_words, docs_words_test=None,\n            n_iter=100, theta=None, phi=None, save_loglike=False):\n        \"\"\"\n        docs_words is a matrix n_docs * n_words; each entry\n        is a number of occurrences of a word in a document\n        docs_words_test does not influence the updates and is used\n        for validation\n        \"\"\"\n        self.init_sampling(docs_words, docs_words_test,\n                           theta, phi, n_iter, save_loglike)\n\n        # Run the Gibbs sampler\n        for iteration in range(n_iter):\n\n            self.update_params()\n\n            if save_loglike:\n                self.update_loglike(iteration)\n\n        return self.to_return_from_run()\n\n    def to_return_from_run(self):\n        return self.topic_doc_words_distr, self.theta, self.phi\n\n    def update_params(self):\n        \"\"\"\n        Samples theta and phi, then computes the distribution of\n        z_id and samples counts A_dk, B_kw from it\n        \"\"\"\n        D = self.n_docs\n        K = self.n_topics\n\n        # Zero out counts before collecting\n        self.A_dk.fill(0)\n        self.B_kw.fill(0)\n\n         # Sample theta (document-topic distributions)\n        for d in range(D):\n            self.theta[d, :] = self.rand_gen.dirichlet(self.A_dk[d, :] + self.alpha)\n\n        # Sample phi (topic-word distributions)\n        for k in range(K):\n            self.phi[k, :] = self.rand_gen.dirichlet(self.B_kw[k, :] + self.beta)\n\n        # Update the distribution of z_id|x_id, theta, phi and sample counts A_dk, B_kw from it\n        self.update_topic_doc_words()\n        self.sample_counts()\n\n    def update_topic_doc_words(self):\n        \"\"\"\n        Computes the distribution of z_id|x_id, theta, phi\n        \"\"\"\n        # Shape: theta (D x K), phi (K x W)\n        # Topic distribution for each doc-word pair: (D, W, K)\n        # We want topic_probs[d, w, k] = theta[d, k] * phi[k, w]\n        # Use broadcasting: theta[:, None, :] gives (D, 1, K), phi.T[None, :, :] gives (1, W, K)\n        # After broadcasting and transpose: (D, W, K)\n        topic_probs = self.theta[:, None, :] * self.phi.T[None, :, :]  # (D, 1, K) * (1, W, K) -> (D, W, K)\n        # Normalize over topics (axis 2) for each (doc, word) pair\n        self.topic_doc_words_distr = topic_probs / topic_probs.sum(axis=2, keepdims=True)\n\n    def sample_counts(self):\n        \"\"\"\n        For each document and each word, samples from z_id|x_id, theta, phi\n        and adds the results to the counts A_dk and B_kw\n        \"\"\"\n        D = self.n_docs\n        K = self.n_topics\n        W = self.n_words\n        z_id = self.topic_doc_words_distr  # shape (D, W, K)\n\n        # Reset and accumulate counts\n        self.A_dk.fill(0)\n        self.B_kw.fill(0)\n\n        for d in range(D):\n            for w in range(W):\n                n_occurrences = self.docs_words[d, w]\n                if n_occurrences > 0:\n                    # z_id[d, w, :] is the probability distribution over topics for this (doc, word) pair\n                    # Multiply by the number of occurrences to get expected counts\n                    for k in range(K):\n                        expected_count = z_id[d, w, k] * n_occurrences\n                        self.A_dk[d, k] += expected_count        # doc-topic assignments\n                        self.B_kw[k, w] += expected_count        # topic-word assignments\n\n    def update_loglike(self, iteration):\n        \"\"\"\n        Updates loglike of the data, omitting the constant additive term\n        with Gamma functions of hyperparameters\n        \"\"\"\n        # todo: implement log-like\n        loglike = 0.0\n        D = self.n_docs\n        W = self.n_words\n        K = self.n_topics\n        z_id = self.topic_doc_words_distr  # shape (D, W, K)\n\n        for d in range(D):\n            for w in range(W):\n                n_occurrences = self.docs_words[d, w]\n                if n_occurrences > 0:\n                    for k in range(K):\n                        # Weight by probability and number of occurrences\n                        loglike += z_id[d, w, k] * n_occurrences * (np.log(self.theta[d, k]) + np.log(self.phi[k, w]))\n        self.loglike[iteration] = loglike\n\n        # Compute test log-likelihood if test data is provided\n        if self.do_test and self.loglike_test is not None:\n            loglike_test = 0.0\n            for d in range(D):\n                for w in range(W):\n                    n_occurrences_test = self.docs_words_test[d, w]\n                    if n_occurrences_test > 0:\n                        # Use current theta and phi to compute test log-likelihood\n                        for k in range(K):\n                            loglike_test += z_id[d, w, k] * n_occurrences_test * (np.log(self.theta[d, k]) + np.log(self.phi[k, w]))\n            self.loglike_test[iteration] = loglike_test\n\n    def get_loglike(self):\n      \"\"\"Returns log-likelihood at each iteration.\"\"\"\n      if self.do_test:\n          return self.loglike, self.loglike_test\n      else:\n          return self.loglike\n\nclass GibbsSamplerCollapsed(GibbsSampler):\n    def __init__(self, n_docs, n_topics, n_words, alpha, beta, random_seed=None):\n        \"\"\"\n        :param n_docs:          number of documents\n        :param n_topics:        number of topics\n        :param n_words:         number of words in vocabulary\n        :param alpha:           dirichlet parameter on topic mixing proportions\n        :param beta:            dirichlet parameter on topic word distributions\n        :param random_seed:     random seed of the sampler\n        \"\"\"\n        super().__init__(n_docs, n_topics, n_words, alpha, beta, random_seed)\n\n        # topics assigned to each (doc, word)\n        self.doc_word_samples = np.ndarray((self.n_docs, self.n_words), dtype=object)\n        self.doc_word_samples_test = self.doc_word_samples.copy()\n\n    def init_params(self, theta=None, phi=None):\n        # z_id are initialized uniformly\n        for doc in range(self.n_docs):\n            for word in range(self.n_words):\n                n_train = self.docs_words[doc, word]\n                if self.do_test:\n                    n_test = self.docs_words_test[doc, word]\n                else:\n                    n_test = 0\n\n                # Initialize training samples\n                if n_train > 0:\n                    sampled_topics_train = self.rand_gen.choice(self.topics_space, size=n_train)\n                    self.doc_word_samples[doc, word] = sampled_topics_train.copy()\n\n                    sample, counts = np.unique(sampled_topics_train, return_counts=True)\n                    self.A_dk[doc, sample] += counts\n                    self.B_kw[sample, word] += counts\n                else:\n                    self.doc_word_samples[doc, word] = np.array([], dtype=int)\n\n                # Initialize test samples\n                if self.do_test:\n                    if n_test > 0:\n                        sampled_topics_test = self.rand_gen.choice(self.topics_space, size=n_test)\n                        self.doc_word_samples_test[doc, word] = sampled_topics_test.copy()\n\n                        sample, counts = np.unique(sampled_topics_test, return_counts=True)\n                        self.A_dk_test[doc, sample] += counts\n                        self.B_kw_test[sample, word] += counts\n                    else:\n                        self.doc_word_samples_test[doc, word] = np.array([], dtype=int)\n\n    def update_params(self):\n        \"\"\"\n        Computes the distribution of z_id.\n        Sampling of A_dk, B_kw is done automatically as\n        each new z_id updates these counters\n        \"\"\"\n        # todo: sample a topic for each (doc, word) and update A_dk, B_kw correspondingly\n        # Hint: you can update A_dk, B_kw after each sampling instead of re-computing the whole matrix\n\n        D = self.n_docs\n        W = self.n_words\n        K = self.n_topics\n\n        for d in range(D):\n            for w in range(W):\n                n_occurrences = self.docs_words[d, w]\n\n                # Remove old topic assignments for this (doc, word) pair\n                old_samples = self.doc_word_samples[d, w]\n                if old_samples is not None and len(old_samples) > 0:\n                    for old_topic in old_samples:\n                        self.A_dk[d, old_topic] -= 1\n                        self.B_kw[old_topic, w] -= 1\n\n                # Sample new topics if there are occurrences\n                if n_occurrences > 0:\n                    # Compute conditional probability for each topic k\n                    # M_k is the total count of words assigned to topic k (after removing old assignments)\n                    M_k = np.sum(self.B_kw, axis=1)  # shape (K,)\n                    probs = (self.A_dk[d, :] + self.alpha) * (self.B_kw[:, w] + self.beta) / (M_k + W * self.beta)\n                    probs = probs / probs.sum()\n\n                    # Sample topics for all occurrences of this word in this document\n                    sampled_topics = self.rand_gen.choice(np.arange(K), size=n_occurrences, p=probs)\n                    self.doc_word_samples[d, w] = sampled_topics\n\n                    # Update counts for sampled topics\n                    for topic in sampled_topics:\n                        self.A_dk[d, topic] += 1\n                        self.B_kw[topic, w] += 1\n                else:\n                    # No occurrences - set to empty array\n                    self.doc_word_samples[d, w] = np.array([], dtype=int)\n\n    def update_loglike(self, iteration):\n        \"\"\"\n        Updates loglike of the data, omitting the constant additive term\n        with Gamma functions of hyperparameters\n        \"\"\"\n        # todo: implement log-like\n        self.loglike[iteration] = 0.0\n        D = self.n_docs\n        K = self.n_topics\n\n        # First term: log-likelihood of doc-topic assignments (theta)\n        for d in range(D):\n            self.loglike[iteration] += np.sum(gammaln(self.A_dk[d, :] + self.alpha))\n            self.loglike[iteration] -= gammaln(np.sum(self.A_dk[d, :] + self.alpha))\n        # Second term: log-likelihood of topic-word assignments (phi)\n        for k in range(K):\n            self.loglike[iteration] += np.sum(gammaln(self.B_kw[k, :] + self.beta))\n            self.loglike[iteration] -= gammaln(np.sum(self.B_kw[k, :] + self.beta))\n\n        # Compute test log-likelihood if test data is provided\n        if self.do_test and self.loglike_test is not None:\n            loglike_test = 0.0\n            # Use test counts to compute test log-likelihood\n            for d in range(D):\n                loglike_test += np.sum(gammaln(self.A_dk_test[d, :] + self.alpha))\n                loglike_test -= gammaln(np.sum(self.A_dk_test[d, :] + self.alpha))\n            for k in range(K):\n                loglike_test += np.sum(gammaln(self.B_kw_test[k, :] + self.beta))\n                loglike_test -= gammaln(np.sum(self.B_kw_test[k, :] + self.beta))\n            self.loglike_test[iteration] = loglike_test\n\n    def get_loglike(self):\n      \"\"\"Returns log-likelihood at each iteration.\"\"\"\n      if self.do_test:\n          return self.loglike, self.loglike_test\n      else:\n          return self.loglike\n\n    def to_return_from_run(self):\n        # Compute theta and phi from the counts (posterior means)\n        D = self.n_docs\n        K = self.n_topics\n        W = self.n_words\n\n        # Compute theta: document-topic distributions (posterior mean)\n        theta = np.zeros((D, K))\n        for d in range(D):\n            theta[d, :] = (self.A_dk[d, :] + self.alpha) / (np.sum(self.A_dk[d, :]) + K * self.alpha)\n\n        # Compute phi: topic-word distributions (posterior mean)\n        phi = np.zeros((K, W))\n        for k in range(K):\n            phi[k, :] = (self.B_kw[k, :] + self.beta) / (np.sum(self.B_kw[k, :]) + W * self.beta)\n\n        return self.doc_word_samples, theta, phi\n\ndef display_topic_word_table(phi, word_labels=None, topic_labels=None, precision=2):\n    df = pd.DataFrame(np.round(phi, precision))\n    if word_labels:\n        df.columns = word_labels\n    if topic_labels:\n        df.index = topic_labels\n    print(df)\n\ndef display_doc_topic_table(theta, doc_labels=None, topic_labels=None, precision=2):\n    df = pd.DataFrame(np.round(theta, precision))\n    if doc_labels:\n        df.index = doc_labels\n    if topic_labels:\n        df.columns = topic_labels\n    print(df)\n\ndef read_data(filename):\n    \"\"\"\n    Reads the text data and splits into train/test.\n    Examples:\n    docs_words_train, docs_words_test = read_data('./code/toyexample.data')\n    nips_train, nips_test = read_data('./code/nips.data')\n    :param filename:    path to the file\n    :return:\n    docs_words_train:   training data, [n_docs, n_words] numpy array\n    docs_words_test:    test data, [n_docs, n_words] numpy array\n    \"\"\"\n    data = pd.read_csv(filename, dtype=int, sep=' ', names=['doc', 'word', 'train', 'test'])\n\n    n_docs = np.amax(data.loc[:, 'doc'])\n    n_words = np.amax(data.loc[:, 'word'])\n\n    docs_words_train = np.zeros((n_docs, n_words), dtype=int)\n    docs_words_test = np.zeros((n_docs, n_words), dtype=int)\n\n    docs_words_train[data.loc[:, 'doc'] - 1, data.loc[:, 'word'] - 1] = data.loc[:, 'train']\n    docs_words_test[data.loc[:, 'doc'] - 1, data.loc[:, 'word'] - 1] = data.loc[:, 'test']\n\n    return docs_words_train, docs_words_test\n\ndef main():\n    # run both standard and collapsed gibbs on the toy example consisting of\n    # 6 documents, 6 words, and 3 topics.  The true word distribution for each\n    # topic should be:\n    # [.5 .5 0 0 0 0], [0 0 .5 .5 0 0], [0 0 0 0 .5 .5]\n\n    n_topics = 10     # number of topics\n    alpha = 1      # dirichlet prior over topics\n    beta =  1        # dirichlet prior over words\n    n_iter = 1000    # number of iterations\n    random_seed = 0  # random seed\n\n    # Run the standard sampler\n    print('Running toyexample.data with the standard sampler')\n    docs_words_train, docs_words_test = read_data('/datasets/t1cw-data/toyexample.data')\n    n_docs, n_words = docs_words_train.shape\n\n    sampler = GibbsSampler(n_docs=n_docs, n_topics=n_topics, n_words=n_words,\n                           alpha=alpha, beta=beta, random_seed=random_seed)\n\n    topic_doc_words_distr, theta, phi = sampler.run(docs_words_train, docs_words_test,\n                                                  n_iter=n_iter, save_loglike=True)\n    # Print the topic-word distributions\n    print(phi * [phi > 1e-2]) # # highlight the \"active\" words for each topic in topic-word probabilities\n    print(theta * [theta > 1e-2]) # highlight the \"active\" words for each topic is the document-topic distributions\n    print(topic_doc_words_distr) # raw topic assignments for each (doc, word) pair\n\n    print(\"Topic-word distribution (phi) in Standard Gibbs:\")\n    display_topic_word_table(phi)\n    print(\"\\nDoc-topic distribution (theta) in Standard Gibbs:\")\n    display_doc_topic_table(theta)\n\n    # ensure log-likelihoods for the standard sampler are stored if not done by run() method\n    like_train, like_test = sampler.get_loglike()\n\n    # Plot the log-likelihood\n    plt.subplots(figsize=(15, 6))\n    plt.plot(like_train, label='Joint')\n    plt.plot(like_test, label='Predictive')\n    plt.title('Standard Gibbs Log-likelihood')\n    plt.ylabel('loglike')\n    plt.xlabel('iteration')\n    plt.legend()\n    plt.show()\n\n    # Heatmap\n    plt.figure(figsize=(8,3))\n    sns.heatmap(phi, annot=True, cmap=\"Blues\", fmt=\".2f\", annot_kws={'size': 7})\n    plt.title(\"Standard Topic-Word Distribution (phi)\")\n    plt.ylabel(\"Topic\")\n    plt.xlabel(\"Word\")\n    plt.show()\n\n    plt.figure(figsize=(6,4))\n    sns.heatmap(theta, annot=True, cmap=\"Greens\", fmt=\".2f\", annot_kws={'size': 7})\n    plt.title(\"Standard Doc-Topic Distribution (theta)\")\n    plt.ylabel(\"Document\")\n    plt.xlabel(\"Topic\")\n    plt.show()\n\n    # Run the collapsed sampler\n    print('Running toyexample.data with the collapsed sampler')\n\n    sampler_collapsed = GibbsSamplerCollapsed(n_docs=n_docs, n_topics=n_topics, n_words=n_words,\n                                              alpha=alpha, beta=beta, random_seed=random_seed)\n\n    doc_word_samples, theta, phi = sampler_collapsed.run(docs_words_train, docs_words_test,\n                                             n_iter=n_iter, save_loglike=True)\n\n    for d in range(n_docs):\n        for w in range(n_words):\n            print(f\"doc {d}, word {w}: {sampler_collapsed.doc_word_samples[d, w]}\")\n\n    # Print the topic-word distributions\n    print(phi * [phi > 1e-2]) # highlight the \"active\" words for each topic in topic-word probabilities\n    print(theta * [theta > 1e-2]) # highlight the \"active\" words for each topic is the document-topic distributions\n    print(doc_word_samples) # raw topic assignments for each (doc, word) pair\n\n    # Print topic counts as heatmap\n    topic_counts = np.zeros((n_topics, n_words))\n    for doc in range(doc_word_samples.shape[0]):\n        for word in range(doc_word_samples.shape[1]):\n            for topic in doc_word_samples[doc, word]:\n                topic_counts[topic, word] += 1\n    plt.figure(figsize=(8,3))\n    sns.heatmap(topic_counts, annot=True, cmap=\"Reds\", fmt=\".2f\", annot_kws={'size': 7})  # two decimal places\n    plt.title(\"Collapsed Gibbs: Assignment Counts\")\n    plt.ylabel(\"Topic\")\n    plt.xlabel(\"Word\")\n    plt.show()\n\n    print(\"\\nTopic-word distribution (phi) in Collapsed Gibbs:\")\n    display_topic_word_table(phi)\n    print(\"\\nDoc-topic distribution (theta) in Collapsed Gibbs:\")\n    display_doc_topic_table(theta)\n\n    # ensure log-likelihoods for the collapsed sampler are stored if not done by run() method\n    like_train_collapsed, like_test_collapsed = sampler_collapsed.get_loglike()\n\n    # Plot the log-likelihood for collapsed sampler\n    plt.subplots(figsize=(15, 6))\n    plt.plot(like_train_collapsed, label='Joint')\n    plt.plot(like_test_collapsed, label='Predictive')\n    plt.title('Collapsed Gibbs Log-likelihood')\n    plt.ylabel('loglike')\n    plt.xlabel('iteration')\n    plt.legend()\n    plt.show()\n\n    # Heatmap\n    plt.figure(figsize=(8,3))\n    sns.heatmap(phi, annot=True, cmap=\"Blues\", fmt=\".2f\", annot_kws={'size': 7})\n    plt.title(\"Collapsed Topic-Word Distribution (phi)\")\n    plt.ylabel(\"Topic\")\n    plt.xlabel(\"Word\")\n    plt.show()\n\n    plt.figure(figsize=(6,4))\n    sns.heatmap(theta, annot=True, cmap=\"Greens\", fmt=\".2f\", annot_kws={'size': 7})\n    plt.title(\"Collapsed Doc-Topic Distribution (theta)\")\n    plt.ylabel(\"Document\")\n    plt.xlabel(\"Topic\")\n    plt.show()\n\n    return sampler, sampler_collapsed\n\n\nif __name__ == '__main__':\n    # Capture the returned sampler objects into global variables\n    sampler, sampler_collapsed = main()\n",
      "block_group": "d9e8c31c8d8e42c598424fec40663883",
      "execution_count": 8,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhKYjjdK4IrM",
        "cell_id": "915619fc5d4440fcbb9c881c61444b1c",
        "deepnote_cell_type": "markdown"
      },
      "source": "## Autocorrelation after Burn-in",
      "block_group": "206c5481719c4805a69395b2e93f18f2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glxuCAMQmTdl",
        "colab": {
          "height": 379,
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4513315e-9e11-4ad2-a99b-8c047e45ce7a",
        "collapsed": true,
        "executionInfo": {
          "user": {
            "userId": "04808690080138473040",
            "displayName": "Stephanie Jat"
          },
          "status": "ok",
          "elapsed": 1023,
          "user_tz": 0,
          "timestamp": 1763900977059
        },
        "cell_id": "497b99e1c41e4cdb840747566c01e10f",
        "deepnote_cell_type": "code"
      },
      "source": "import matplotlib.pyplot as plt\nfrom statsmodels.tsa.stattools import acf\n\nburnin = 200\nmax_lag = 50\n\n# Use samples after burn-in only:\npredstd = sampler.loglike_test[burnin:]\npredcol = sampler_collapsed.loglike_test[burnin:]\njointstd = sampler.loglike[burnin:]\njointcol = sampler_collapsed.loglike[burnin:]\n\n\n# Remove NaNs or inf before autocorrelation\ndef fin(arr, nlags, fft=True):\n    arr = np.array(arr)\n    arr = arr[np.isfinite(arr)]\n    if arr.size < nlags + 1:\n        raise ValueError(\"Not enough finite values to compute autocorrelation.\")\n    if np.nanstd(arr) == 0:\n        print(\"Warning: input array has zero variance after burn-in. Autocorrelation is undefined.\")\n        return np.zeros(nlags + 1)\n    return acf(arr, nlags=nlags, fft=fft)\n\n# Compute autocorrelation (using statsmodels)\nacf_predstd = fin(predstd, nlags=max_lag)\nacf_predcol = fin(predcol, nlags=max_lag)\nacf_jointstd = fin(jointstd, nlags=max_lag)\nacf_jointcol = fin(jointcol, nlags=max_lag)\n\n# Plot\nplt.figure(figsize=(10,8))\nplt.subplot(221); plt.stem(acf_predstd); plt.title('Standard Gibbs Log Predictive')\nplt.subplot(222); plt.stem(acf_jointstd); plt.title('Standard Gibbs Log Joint')\nplt.subplot(223); plt.stem(acf_predcol); plt.title('Collapsed Gibbs Log Predictive')\nplt.subplot(224); plt.stem(acf_jointcol); plt.title('Collapsed Gibbs Log Joint')\nplt.tight_layout(); plt.show()",
      "block_group": "5c6b088e65bb46e0b42919360aaad488",
      "execution_count": 14,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvZC5zqV4KfZ",
        "cell_id": "6e4d913ca54a4117aa3d3201ad1c73a6",
        "deepnote_cell_type": "markdown"
      },
      "source": "## Recover Posteriors",
      "block_group": "aa587d47af8740beb8efa7c34d735d74"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoF9jvu1u-on",
        "cell_id": "c0d27514b7b741a3ad0646c15dc17e2c",
        "deepnote_cell_type": "code"
      },
      "source": "# Inspect Posterior Recovery\nprint('Estimated topic-word distributions (standard sampler):')\nprint(np.round(sampler.phi, 2))\nprint('Estimated doc-topic distributions (standard sampler):')\nprint(np.round(sampler.theta, 2))\n\nprint('Estimated topic-word distributions (collapsed sampler):')\nprint(np.round(sampler_collapsed.phi, 2))\nprint('Estimated doc-topic distributions (collapsed sampler):')\nprint(np.round(sampler_collapsed.theta, 2))\n",
      "block_group": "0d27a8728dde434aa578eaab54c42e23",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0X0XnGzLUvp",
        "cell_id": "77b51a025f824d1480ef58f23251c438",
        "deepnote_cell_type": "markdown"
      },
      "source": "# Gibbs Sampling on NeurIPS data\n",
      "block_group": "9e2a986cbd2240df8925444d5ade2f10"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "0a3adc68ee6c41fc827cbe63be9b99fb",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Prune data using tf-idf",
      "block_group": "de77ea72524e46728173326e67f792ef"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk9LU5d4LaiI",
        "cell_id": "5bacfaaf34cb4d4dad877b73308ee5e1",
        "deepnote_cell_type": "code"
      },
      "source": "import pandas as pd\nimport numpy as np\n\n# Step 1: Load .data file (4 cols: doc, word, train, test)\ndata = pd.read_csv('/datasets/t1cw-data/nips.data', sep=' ', header=None, names=['doc', 'word', 'train', 'test'])\n\n# Convert to zero-based indices for arrays\ndata['doc'] -= 1\ndata['word'] -= 1\n\n# Step 2: Load vocab\nwith open('/datasets/t1cw-data/nips.vocab') as f:\n  vocab = [line.strip() for line in f]\n\n# Step 3: Compute informativeness metric (tf-idf)\nN = int(data['doc'].max()) + 1 # Number of docs\nW = int(data['word'].max()) + 1 # Number of words\n# Term frequency (tf): total count of each word in all documents\ntf = data.groupby('word')['train'].sum()\n# Document frequency (df): number of docs each word appears in\ndf = data[data['train'] > 0].groupby('word')['doc'].nunique()\n# tf-idf informativeness metric for each word\ntfidf = tf * np.log(N / (1 + df)).reindex(tf.index, fill_value=0)\ntop = tfidf.nlargest(500).index.to_list()   # List of top 500 word indices (ints)\n\n# Step 4: Filter .data using indices, then renumber for contiguous matrix\nfiltered = data[data['word'].isin(top)].copy()\ntop = sorted(top)\nmapping = {old_idx: new_idx for new_idx, old_idx in enumerate(top)}\nfiltered['word'] = filtered['word'].map(mapping)\n\n# Step 5: Build train/test matrices\nN = int(filtered['doc'].max()) + 1\nW = int(filtered['word'].max()) + 1\nnips_train = np.zeros((N, W), dtype=float)\nnips_test = np.zeros((N, W), dtype=float)\nnips_train[filtered['doc'], filtered['word']] = filtered['train']\nnips_test[filtered['doc'], filtered['word']] = filtered['test']\n\n# Step 6: Save new .data file (word and doc indices as 1-based)\npruned = filtered.copy()\npruned['doc'] += 1\npruned['word'] += 1\npruned[['doc', 'word', 'train', 'test']].to_csv(\n    '/datasets/t1cw-data/prunednips.data', sep=' ', header=False, index=False)\n\n# Step 7: Save new filtered vocab, aligned to new word indices\nkeywords = [vocab[idx] for idx in top]\nwith open('/datasets/t1cw-data/prunednips.vocab', 'w') as f:\n    for word in keywords:\n        f.write(word + '\\n')\n",
      "block_group": "82aff845d24b44cb9573b01676176c2e",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "c9e5714848304a099bf4d55b2ffe4644",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Run Collapsed Sampler on pruned data",
      "block_group": "2b0a1eaed9024665babbab4206311a55"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFw0kUHE2uQs",
        "colab": {
          "height": 1000,
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f1b29e5-3702-49f6-8beb-9f7a4fe25324",
        "executionInfo": {
          "user": {
            "userId": "04808690080138473040",
            "displayName": "Stephanie Jat"
          },
          "status": "ok",
          "elapsed": 3782942,
          "user_tz": 0,
          "timestamp": 1763767971681
        },
        "cell_id": "2a2acd730c364999800aa107f86c1174",
        "deepnote_cell_type": "code"
      },
      "source": "# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import gammaln\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nclass GibbsSamplerCollapsed:\n    def __init__(self, n_docs, n_topics, n_words, alpha, beta, random_seed=None):\n        \"\"\"Initialize sampler with shapes.\"\"\"\n        self.n_docs = n_docs\n        self.n_topics = n_topics\n        self.n_words = n_words\n        self.alpha = alpha\n        self.beta = beta\n        self.rand_gen = np.random.RandomState(random_seed)\n\n        # Placeholders (shapes will be updated in init_sampling)\n        self.nips_samples = None  # topic assignments for train\n        self.nips_samples_test = None  # topic assignments for test\n        self.nips = None\n        self.nips_test = None\n        self.A_dk = None  # document-topic count\n        self.B_kw = None  # topic-word count\n        self.A_dk_test = None\n        self.B_kw_test = None\n        self.theta = None\n        self.phi = None\n        self.topics_space = np.arange(self.n_topics)\n        self.topic_nips_distr = None\n        self.loglike = None\n        self.loglike_test = None\n        self.do_test = False\n\n    def init_sampling(self, nips, nips_test=None,\n                      theta=None, phi=None, n_iter=0, save_loglike=False):\n        self.nips = nips\n        self.n_docs, self.n_words = nips.shape\n        self.n_topics = getattr(self, 'n_topics', 10)\n        if nips_test is not None:\n            self.do_test = True\n            self.nips_test = nips_test\n        else:\n            self.do_test = False\n            self.nips_test = None\n\n        self.nips_samples = np.empty((self.n_docs, self.n_words), dtype=object)\n        self.nips_samples_test = np.empty((self.n_docs, self.n_words), dtype=object)\n        self.A_dk = np.zeros((self.n_docs, self.n_topics), dtype=int)\n        self.B_kw = np.zeros((self.n_topics, self.n_words), dtype=int)\n        self.A_dk_test = np.zeros((self.n_docs, self.n_topics), dtype=int)\n        self.B_kw_test = np.zeros((self.n_topics, self.n_words), dtype=int)\n        self.theta = np.ones((self.n_docs, self.n_topics)) / self.n_topics\n        self.phi = np.ones((self.n_topics, self.n_words)) / self.n_words\n        self.topics_space = np.arange(self.n_topics)\n        self.topic_nips_distr = np.zeros((self.n_docs, self.n_topics, self.n_words))\n\n        if save_loglike:\n            self.loglike = np.zeros(n_iter)\n            self.loglike_test = np.zeros(n_iter) if self.do_test else None\n\n        self.init_params(theta, phi)\n\n    def init_params(self, theta=None, phi=None):\n        for d in range(self.n_docs):\n            for w in range(self.n_words):\n                n_train = int(self.nips[d, w])\n                n_test = int(self.nips_test[d, w]) if (self.do_test and self.nips_test is not None) else 0\n                if n_train > 0:\n                    sampled_topics_train = self.rand_gen.choice(self.topics_space, size=n_train)\n                    self.nips_samples[d, w] = sampled_topics_train.copy()\n                    for k, c in zip(*np.unique(sampled_topics_train, return_counts=True)):\n                        self.A_dk[d, k] += c\n                        self.B_kw[k, w] += c\n                else:\n                    self.nips_samples[d, w] = np.array([], dtype=int)\n\n                if self.do_test and n_test > 0:\n                    sampled_topics_test = self.rand_gen.choice(self.topics_space, size=n_test)\n                    self.nips_samples_test[d, w] = sampled_topics_test.copy()\n                    for k, c in zip(*np.unique(sampled_topics_test, return_counts=True)):\n                        self.A_dk_test[d, k] += c\n                        self.B_kw_test[k, w] += c\n                elif self.do_test:\n                    self.nips_samples_test[d, w] = np.array([], dtype=int)\n\n    def sample_counts(self):\n        for d in range(self.n_docs):\n            for w in range(self.n_words):\n                n_occurrences = int(self.nips[d, w])\n                # Remove old topic assignments\n                old_samples = self.nips_samples[d, w]\n                for old_topic in old_samples:\n                    self.A_dk[d, old_topic] -= 1\n                    self.B_kw[old_topic, w] -= 1\n                if n_occurrences > 0:\n                    M_k = np.sum(self.B_kw, axis=1)  # total words per topic\n                    p = (self.A_dk[d, :] + self.alpha) * (self.B_kw[:, w] + self.beta) / (M_k + self.n_words * self.beta)\n                    p = p / p.sum()\n                    sampled_topics = self.rand_gen.choice(self.topics_space, size=n_occurrences, p=p)\n                    self.nips_samples[d, w] = sampled_topics\n                    for topic in sampled_topics:\n                        self.A_dk[d, topic] += 1\n                        self.B_kw[topic, w] += 1\n                else:\n                    self.nips_samples[d, w] = np.array([], dtype=int)\n\n    def update_pd(self):\n        # Recalculate topic_nips_distr\n        # Note: not generally used in collapsed samplers, but keeping for reference\n        self.topic_nips_distr = self.theta[:, :, None] * self.phi[None, :, :]  # shape (N, K, W)\n        self.topic_nips_distr /= np.sum(self.topic_nips_distr, axis=1, keepdims=True)  # normalize over K\n\n    def update_params(self):\n        # Posterior means for theta/phi after sampling\n        self.theta = (self.A_dk + self.alpha).astype(float)\n        self.theta /= np.sum(self.theta, axis=1, keepdims=True)\n        self.phi = (self.B_kw + self.beta).astype(float)\n        self.phi /= np.sum(self.phi, axis=1, keepdims=True)\n        self.update_pd()\n        self.sample_counts()\n        return self.nips_samples, self.theta, self.phi\n\n    def update_loglike(self, iteration):\n        \"\"\"Collapsed log-likelihood per iteration.\"\"\"\n        loglike = 0.0\n        for d in range(self.n_docs):\n            loglike += np.sum(gammaln(self.A_dk[d, :] + self.alpha))\n            loglike -= gammaln(np.sum(self.A_dk[d, :] + self.n_topics * self.alpha))\n        for k in range(self.n_topics):\n            loglike += np.sum(gammaln(self.B_kw[k, :] + self.beta))\n            loglike -= gammaln(np.sum(self.B_kw[k, :] + self.n_words * self.beta))\n        self.loglike[iteration] = loglike\n        # Compute test log-like if relevant\n        if self.do_test and self.loglike_test is not None:\n            loglike_test = 0.0\n            for d in range(self.n_docs):\n                loglike_test += np.sum(gammaln(self.A_dk_test[d, :] + self.alpha))\n                loglike_test -= gammaln(np.sum(self.A_dk_test[d, :] + self.n_topics * self.alpha))\n            for k in range(self.n_topics):\n                loglike_test += np.sum(gammaln(self.B_kw_test[k, :] + self.beta))\n                loglike_test -= gammaln(np.sum(self.B_kw_test[k, :] + self.n_words * self.beta))\n            self.loglike_test[iteration] = loglike_test\n\n    def get_loglike(self):\n        if self.do_test:\n            return self.loglike, self.loglike_test\n        else:\n            return self.loglike, None\n\n    def run(self, nips, nips_test=None, n_iter=100, theta=None, phi=None, save_loglike=False):\n        self.init_sampling(nips, nips_test, theta, phi, n_iter, save_loglike)\n        for iteration in range(n_iter):\n            self.update_params()\n            if save_loglike:\n                self.update_loglike(iteration)\n        return self.nips_samples, self.theta, self.phi\n\n    @staticmethod\n    def display_topic_word_table(phi, word_labels=None, topic_labels=None, precision=2):\n        df = pd.DataFrame(np.round(phi, precision))\n        if word_labels is not None:\n            df.columns = word_labels\n        if topic_labels is not None:\n            df.index = topic_labels\n        print(df)\n\n    @staticmethod\n    def display_doc_topic_table(theta, doc_labels=None, topic_labels=None, precision=2):\n        df = pd.DataFrame(np.round(theta, precision))\n        if doc_labels is not None:\n            df.index = doc_labels\n        if topic_labels is not None:\n            df.columns = topic_labels\n        print(df)\n\ndef read_data(filename):\n    \"\"\"\n    Reads the text data and splits into train/test.\n    Examples:\n    docs_words_train, docs_words_test = read_data('./code/toyexample.data')\n    nips_train, nips_test = read_data('./code/nips.data')\n    :param filename:    path to the file\n    :return:\n    nips_train:   training data, [n_docs, n_words] numpy array\n    nips_words_test:    test data, [n_docs, n_words] numpy array\n    \"\"\"\n    data = pd.read_csv(filename, dtype=int, sep=' ', names=['doc', 'word', 'train', 'test'])\n\n    n_docs = np.amax(data.loc[:, 'doc'])\n    n_words = np.amax(data.loc[:, 'word'])\n\n    nips_train = np.zeros((n_docs, n_words), dtype=int)\n    nips_test = np.zeros((n_docs, n_words), dtype=int)\n\n    nips_train[data.loc[:, 'doc'] - 1, data.loc[:, 'word'] - 1] = data.loc[:, 'train']\n    nips_test[data.loc[:, 'doc'] - 1, data.loc[:, 'word'] - 1] = data.loc[:, 'test']\n\n    return nips_train, nips_test\n\ndef main():\n    nips_train, nips_test = read_data('/datasets/t1cw-data/prunednips.data')\n    K= 3\n    alpha = 1\n    beta = 1\n    n_iter = 200\n    seed = 0\n\n    sampler = GibbsSamplerCollapsed(\n        n_docs=N, n_topics=K, n_words=W, alpha=alpha, beta=beta, random_seed=seed\n    )\n    nips_samples, theta, phi = sampler.run(\n        nips_train, nips_test, n_iter=n_iter, save_loglike=True\n    )\n    like_train, like_test = sampler.get_loglike()\n\n    # Print topic assignments for first few doc/word pairs (truncate printing for readability)\n    for d in range(min(3, N)):\n        for w in range(min(3, W)):\n            print(f\"doc {d}, word {w}: {sampler.nips_samples[d, w]}\")\n\n    print(\"\\nActive topic-word probabilities (>1e-2):\\n\", np.round(phi * (phi > 1e-2), 2))\n    print(\"\\nActive doc-topic probabilities (>1e-2):\\n\", np.round(theta * (theta > 1e-2), 2))\n\n    # Topic assignment heatmap\n    topic_counts = np.zeros((K, N))\n    for doc in range(N):\n        for word in range(W):\n            for topic in sampler.nips_samples[doc, word]:\n                topic_counts[topic, doc] += 1\n\n    plt.figure(figsize=(8, 3))\n    sns.heatmap(topic_counts, annot=True, cmap=\"Reds\", fmt=\".0f\", annot_kws={'size': 7})\n    plt.title(\"Topic Assignments (Counts per Topic, Document)\")\n    plt.ylabel(\"Topic\")\n    plt.xlabel(\"Document\")\n    plt.show()\n\n    print(\"\\nTopic-word distribution (phi):\")\n    GibbsSamplerCollapsed.display_topic_word_table(phi)\n    print(\"\\nDoc-topic distribution (theta):\")\n    GibbsSamplerCollapsed.display_doc_topic_table(theta)\n\n    # Plot log-likelihood\n    plt.figure(figsize=(15, 6))\n    plt.plot(like_train, label='Joint log-like')\n    if like_test is not None:\n        plt.plot(like_test, label='Predictive log-like')\n    plt.title('Collapsed Gibbs Log-likelihood')\n    plt.ylabel('loglikelihood')\n    plt.xlabel('Iteration')\n    plt.legend()\n    plt.show()\n\n    plt.figure(figsize=(8, 3))\n    sns.heatmap(phi, annot=True, cmap=\"Blues\", fmt=\".2f\", annot_kws={'size': 7})\n    plt.title(\"Collapsed Topic-Word Distribution (phi)\")\n    plt.ylabel(\"Topic\")\n    plt.xlabel(\"Word\")\n    plt.show()\n\n    plt.figure(figsize=(6, 4))\n    sns.heatmap(theta, annot=True, cmap=\"Greens\", fmt=\".2f\", annot_kws={'size': 7})\n    plt.title(\"Collapsed Doc-Topic Distribution (theta)\")\n    plt.ylabel(\"Document\")\n    plt.xlabel(\"Topic\")\n    plt.show()\n\n    return sampler\n\nif __name__ == '__main__':\n    nips_sampler = main()\n",
      "block_group": "576ccedbf6f3401f8b795dda993ab67b",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "is_collapsed": false,
        "formattedRanges": [],
        "deepnote_app_block_visible": false,
        "cell_id": "07f5274ace344023b5540c7f6b8e5c05",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Get topics",
      "block_group": "44fcbe7e07444c299da82a7c13facc6b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KBXfUU20HGm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec10df9a-eed6-44fd-94cb-98477b3472cb",
        "executionInfo": {
          "user": {
            "userId": "04808690080138473040",
            "displayName": "Stephanie Jat"
          },
          "status": "ok",
          "elapsed": 10,
          "user_tz": 0,
          "timestamp": 1763771195228
        },
        "cell_id": "b27b276ae9804563be36f1ace1a67209",
        "deepnote_cell_type": "code"
      },
      "source": "phi = nips_sampler.phi\nK = phi.shape[0]\ntop_n = 10  # Number of top words per topic\nsummary = []\n\nfor k in range(K):\n    keyword_indices = phi[k, :].argsort()[::-1][:top_n]\n    topkeywords = [keywords[i] for i in keyword_indices]\n    summary.append(topkeywords)\n\n# Print summary as a markdown-style table:\nprint(\"| Topic | Top 10 Words                |\")\nprint(\"|-------|-----------------------------|\")\nfor k, words in enumerate(summary):\n    print(f\"| {k+1:5d} | {' '.join(words)} |\")\n",
      "block_group": "c0fac2aafd1d43969117ddad59e6b0ea",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=cb182644-878e-48cb-992b-68a78a5afe3d' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_full_width": true,
    "deepnote_notebook_id": "ac4b22334267468282c9f496b15f2baf"
  }
}