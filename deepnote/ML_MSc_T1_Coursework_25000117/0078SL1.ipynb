{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "3ad2550edc164d29b6863c14af6fd29d",
        "deepnote_cell_type": "code"
      },
      "source": "import numpy as np\nimport pandas as pd\nfrom sklearn.neighbors import KNeighborsClassifier\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import cdist\nfrom sklearn.model_selection import KFold",
      "block_group": "4b080f28991c42a3957be4a3ebefb96c",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "f16cc6e1dfff4fbf93b8a97b88505039",
        "deepnote_cell_type": "text-cell-h1"
      },
      "source": "# Part I ",
      "block_group": "c790d667b91443ef95f144e882abdc0a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "17e9a305f0f54a62828cf0eb2a97d3be",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Linear Regression",
      "block_group": "6c625a2ebf5145cc8598a9a1ae6c809d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "is_collapsed": false,
        "formattedRanges": [],
        "deepnote_app_block_visible": false,
        "cell_id": "c5f52abacbf54d4396541f611c032ec8",
        "deepnote_cell_type": "text-cell-h3"
      },
      "source": "### Matrix building functions",
      "block_group": "528fa8d63f26455b87405b46a3c2a04b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "e97c9217ac6f43939cd96fbf3e1736b8",
        "deepnote_cell_type": "code"
      },
      "source": "def matrix_design(x, ks):\n    x = np.asarray(x).reshape(-1)\n    ks = np.asarray(ks)\n    n = x.shape[0]\n    K = ks.shape[0]\n    X = np.zeros((n, K))\n    for j in range(K):\n        X[:, j] = x ** ks[j]\n    return X\n\ndef sin_matrix(x, ks):\n    x = np.asarray(x).reshape(-1)\n    ks = np.asarray(ks)\n    n = x.shape[0]\n    K = ks.shape[0]\n    X = np.zeros((n, K))\n    for j in range(K):\n        X[:, j] = np.sin(ks[j] * np.pi * x)\n    return X",
      "block_group": "ee86e0cc83cd463395999b5a83932836",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "is_collapsed": false,
        "formattedRanges": [],
        "deepnote_app_block_visible": false,
        "cell_id": "fd403f2f93be480abf235e525e97ad75",
        "deepnote_cell_type": "text-cell-h3"
      },
      "source": "### Q1. Polynomial bases",
      "block_group": "90a3bc79d5ed4eea9b769ea108a972fa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "536ad49e2dfb4fb59560c6a6344a82fe",
        "deepnote_cell_type": "code"
      },
      "source": "# Data\nx = np.array([1, 2, 3, 4], dtype=float).reshape(-1, 1)\ny = np.array([3, 2, 0, 5], dtype=float).reshape(-1, 1)\n\nxx = np.linspace(0.0, 5.0, 200).reshape(-1, 1)\n\n# Colors and line styles for each fit\nstyles = [\"b-\", \"r--\", \"g-.\", \"m:\"]\norder = np.arange(1, 5)  # k = 1,2,3,4\n\nformulas = [None] * len(order)\nsses = np.zeros(len(order))\n\nplt.figure()\nplt.plot(x, y, \"ko\", markerfacecolor=\"k\", label=\"Data\")\n\nfor ki, k in enumerate(order):\n    ks = np.arange(0, k)  # [0], [0 1], [0 1 2], [0 1 2 3]\n\n    X = matrix_design(x, ks)\n    # Least squares solve\n    w, *_ = np.linalg.lstsq(X, y, rcond=None)\n\n    X_fit = matrix_design(xx, ks)\n    y_fit = X_fit @ w\n    ytrain_fit = X @ w\n\n    sses[ki] = np.sum((y - ytrain_fit) ** 2)\n\n    plt.plot(xx, y_fit, styles[ki], linewidth=2, label=f\"k={k}\")\n\n    # Build explicit formula string\n    terms = []\n    for d in range(k):\n        coeff = w[d, 0]\n        if abs(coeff) > 1e-12:\n            if ks[d] == 0:\n                terms.append(f\"{coeff:.3g}\")\n            elif ks[d] == 1:\n                terms.append(f\"{coeff:.3g}x\")\n            else:\n                terms.append(f\"{coeff:.3g}x^{ks[d]}\")\n    formulas[ki] = \"y = \" + \" + \".join(terms) if terms else \"y = 0\"\n\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend(loc=\"upper left\")\nplt.title(\"Polynomial Regression Fits: k=1,2,3,4\")\nplt.show()\n\n# Display formulas\nprint(\"\\nPolynomial formulas:\")\nfor k, fml in zip(order, formulas):\n    print(f\"k={k}: {fml}\")\n\n# Display SSE vector\nprint(\"Sum of Squared Errors (SSE):\")\nprint(sses)",
      "block_group": "536ad49e2dfb4fb59560c6a6344a82fe",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "is_collapsed": false,
        "formattedRanges": [],
        "deepnote_app_block_visible": false,
        "cell_id": "9905879103a94f2692493df8771c8e50",
        "deepnote_cell_type": "text-cell-h3"
      },
      "source": "### Q2. (a) Polynomial bases with sine function",
      "block_group": "393908c8c5eb4e89bb8df933f3f5e9ad"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "f9c3e76ac1cf41d4ad801b4c3bb539f5",
        "deepnote_cell_type": "code"
      },
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\nsigma = 0.07  # noise standard deviation\nn = 30        # number of sample points\n\n# Sample xi uniformly from [0,1]\nx_data = np.random.rand(n, 1)\n\ndef g_sigma(x):\n    x = np.asarray(x)\n    return np.sin(2 * np.pi * x) ** 2 + sigma * np.random.randn(*x.shape)\n\n# Noisy outputs\ny_data = g_sigma(x_data)\n\n# For plotting the underlying function (smooth curve):\nxx = np.linspace(0.0, 1.0, 300).reshape(-1, 1)\ntrue_func = np.sin(2 * np.pi * xx) ** 2\n\nplt.figure()\nplt.plot(xx, true_func, \"b-\", linewidth=2, label=r\"sin^2(2πx)\")\nplt.plot(x_data, y_data, \"ro\", markerfacecolor=\"r\", label=\"Noisy Data Points\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend(loc=\"upper left\")\nplt.title(r\"sin^2(2πx) and Noisy Data Points\")\nplt.show()",
      "block_group": "7eb0015e80bb49b38a1f59310c8f3c1a",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "is_collapsed": false,
        "formattedRanges": [],
        "deepnote_app_block_visible": false,
        "cell_id": "daa689dadc88497baced25cb4af0c79e",
        "deepnote_cell_type": "text-cell-h3"
      },
      "source": "### (b) training error",
      "block_group": "57ac9decab334889bb80c9c1c1ea31ce"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "db0361a9c7ca40d48b39c79a96a6c0c0",
        "deepnote_cell_type": "code"
      },
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\nsigma = 0.07  # noise standard deviation\nntrain = 30   # number of sample points\n\nx_train = np.random.rand(ntrain, 1)\nmtrain = x_train.shape[0]\n\ndef g_sigma(x):\n    x = np.asarray(x)\n    return np.sin(2 * np.pi * x) ** 2 + sigma * np.random.randn(*x.shape)\n\ny_train = g_sigma(x_train)\n\nxx = np.linspace(0.0, 1.0, 400).reshape(-1, 1)\ntrue_func = np.sin(2 * np.pi * xx) ** 2\n\nstyles = [\"b-\", \"r--\", \"g-.\", \"m:\", \"c-.\"]\nbases = np.array([2, 5, 10, 14, 18])\n\ntek = np.zeros_like(bases, dtype=float)\nssestrain = np.zeros_like(bases, dtype=float)\n\nplt.figure()\nplt.plot(xx, true_func, \"k-\", linewidth=1.5, label=\"True function\")\nplt.plot(x_train, y_train, \"ro\", markerfacecolor=\"r\", label=\"Noisy data\")\n\nfor ki, k in enumerate(bases):\n    ks = np.arange(0, k)  # basis exponents\n\n    Xtrain = matrix_design(x_train, ks)\n    w, *_ = np.linalg.lstsq(Xtrain, y_train, rcond=None)\n\n    X_fit = matrix_design(xx, ks)\n    y_fit = X_fit @ w\n    ytrain_fit = Xtrain @ w\n\n    ssestrain[ki] = np.sum((y_train - ytrain_fit) ** 2)\n    tek[ki] = ssestrain[ki] / mtrain\n\n    plt.plot(xx, y_fit, styles[ki], linewidth=2,\n             label=f\"Poly degree {k-1}\")\n\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend(loc=\"upper left\")\nplt.title(\"Polynomial Regression Fits: k=2,5,10,14,18\")\nplt.ylim([-1, 2])\nplt.show()\n\nplt.figure()\nplt.plot(bases, np.log(tek), \"bo-\", linewidth=2, markerfacecolor=\"b\")\nplt.xlabel(\"Polynomial Dimension k\")\nplt.ylabel(\"ln(Training MSE)\")\nplt.title(\"Natural Log of Training Error vs Polynomial Dimension\")\nplt.grid(True)\nplt.show()",
      "block_group": "6963e444be8e4058ac2c298ca1491f38",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "769b7a0f28e84bc2ad15fe9e35259459",
        "deepnote_cell_type": "text-cell-h3"
      },
      "source": "### (c) test error",
      "block_group": "114ffaa755344ca59cee2cb9b7a7a040"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "3f213d4b8afb4b778942d5747435593d",
        "deepnote_cell_type": "code"
      },
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\nsigma = 0.07  # noise standard deviation\n\ndef g_sigma(x):\n    x = np.asarray(x)\n    return np.sin(2 * np.pi * x) ** 2 + sigma * np.random.randn(*x.shape)\n\nxx = np.linspace(0.0, 1.0, 400).reshape(-1, 1)  # for smooth plotting\ntrue_func = np.sin(2 * np.pi * xx) ** 2\n\nstyles = [\"b-\", \"r--\", \"g-.\", \"m:\", \"c-.\"]\nbases = np.array([2, 5, 10, 14, 18])\n\n# -------------------------------\nntrain = 30\nx_train = np.random.rand(ntrain, 1)\nmtrain = x_train.shape[0]\ny_train = g_sigma(x_train)\n\ntek = np.zeros_like(bases, dtype=float)\nssestrain = np.zeros_like(bases, dtype=float)\n\n# -------------------------------\nntest = 1000\nx_test = np.random.rand(ntest, 1)\nmtest = x_test.shape[0]\ny_test = g_sigma(x_test)\ntest_true = np.sin(2 * np.pi * x_test) ** 2\n\ntseks = np.zeros_like(bases, dtype=float)\nssestest = np.zeros_like(bases, dtype=float)\n\nplt.figure()\nplt.plot(xx, true_func, \"k-\", linewidth=2, label=\"True function\")\nplt.plot(x_test, y_test, \"go\", markerfacecolor=\"g\", linestyle=\"\",\n         label=\"Test (noisy)\")\nplt.plot(x_test, test_true, \"rx\", markersize=6, linestyle=\"\",\n         label=\"Test (true)\")\n\nfor ki, k in enumerate(bases):\n    ks = np.arange(0, k)\n\n    Xtrain = matrix_design(x_train, ks)\n    w, *_ = np.linalg.lstsq(Xtrain, y_train, rcond=None)\n\n    X_fit = matrix_design(xx, ks)\n    y_fit = X_fit @ w\n    plt.plot(xx, y_fit, styles[ki], linewidth=2,\n             label=f\"Poly degree {k-1}\")\n\n    ytrain_fit = Xtrain @ w\n    ssestrain[ki] = np.sum((y_train - ytrain_fit) ** 2)\n    tek[ki] = ssestrain[ki] / mtrain\n\n    Xtest = matrix_design(x_test, ks)\n    ytest_fit = Xtest @ w\n    ssestest[ki] = np.sum((y_test - ytest_fit) ** 2)\n    tseks[ki] = ssestest[ki] / mtest\n\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend(loc=\"upper left\")\nplt.title(\"True Func, Test Data (Noisy + True), Fitted Polynomials\")\nplt.show()\n\nplt.figure()\nplt.plot(bases, np.log(tek), \"bo-\", linewidth=2, markerfacecolor=\"b\", label=\"Training error\")\nplt.plot(bases, np.log(tseks), \"ro-\", linewidth=2, markerfacecolor=\"r\", label=\"Test error\")\nplt.xlabel(\"Polynomial Dimension k\")\nplt.ylabel(\"ln(MSE)\")\nplt.title(\"Training (blue) and Test (red) Error vs Polynomial Dimension\")\nplt.legend(loc=\"upper left\")\nplt.grid(True)\nplt.show()",
      "block_group": "42e720afe2bc4a3f8ef2ca430fec7dbd",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "is_collapsed": false,
        "formattedRanges": [],
        "deepnote_app_block_visible": false,
        "cell_id": "60ad3bdde80f4c8bad7ad117035f3170",
        "deepnote_cell_type": "text-cell-h3"
      },
      "source": "### (d) log of average MSE",
      "block_group": "0e21c966d3c8473b8630792165ce22dd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "b4d696ad65a348c2a8bdc884c32c3181",
        "deepnote_cell_type": "code"
      },
      "source": "mse_test_all = np.zeros((num_runs, len(bases)))\n\nfor run in range(num_runs):\n    # Random training and test sets\n    x_train = np.random.rand(ntrain, 1)\n    y_train = np.sin(2 * np.pi * x_train) ** 2 + 0.07 * np.random.randn(ntrain, 1)\n\n    x_test = np.random.rand(ntest, 1)\n    y_test = np.sin(2 * np.pi * x_test) ** 2 + 0.07 * np.random.randn(ntest, 1)\n\n    for ki, k in enumerate(bases):\n        ks = np.arange(0, k)  # 0..k-1\n        Xtrain = matrix_design(x_train, ks)\n        wtrain, *_ = np.linalg.lstsq(Xtrain, y_train, rcond=None)\n\n        Xtest = matrix_design(x_test, ks)\n        ytest_pred = Xtest @ wtrain\n\n        mse = np.mean((y_test - ytest_pred) ** 2)\n        mse_test_all[run, ki] = mse\n\navg_mse = mse_test_all.mean(axis=0)  # shape (len(bases),)\n\nplt.figure()\nplt.plot(bases, np.log(avg_mse), \"ro-\", linewidth=2, markerfacecolor=\"r\")\nplt.xlabel(\"Polynomial Dimension k\")\nplt.ylabel(\"ln(avg Test MSE over {} runs)\".format(num_runs))\nplt.title(\"Log Average Test Error vs Polynomial Dimension\")\nplt.grid(True)\nplt.show()",
      "block_group": "e15a491eb3d049d08c0d5f0ba86dc8c8",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "is_collapsed": false,
        "formattedRanges": [],
        "deepnote_app_block_visible": false,
        "cell_id": "7aa881e95c124d61b26df76c8fc31bba",
        "deepnote_cell_type": "text-cell-h3"
      },
      "source": "### Q3. Sine basis",
      "block_group": "e7862916cb1146df9fb1476667c66a16"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "cc73dc9528014e758777a4db2b50f325",
        "deepnote_cell_type": "code"
      },
      "source": "sigma = 0.07\n\ndef g_sigma(x):\n    x = np.asarray(x)\n    return np.sin(2 * np.pi * x) ** 2 + sigma * np.random.randn(*x.shape)\n\nxx = np.linspace(0.0, 1.0, 400).reshape(-1, 1)\ntrue_func = np.sin(2 * np.pi * xx) ** 2\n\nbases = np.array([2, 5, 10, 14, 18])\nruns = 100\nntest = 1000\nntrain = 30\nmax_k = len(bases)\n\nmse_all = np.zeros((runs, max_k))\nmse_train = np.zeros((runs, max_k))\n\n# Visualisation test/train sets\nx_test_vis = np.random.rand(ntest, 1)\ny_test_vis = g_sigma(x_test_vis)\ntest_true_vis = np.sin(2 * np.pi * x_test_vis) ** 2\n\nx_train_vis = np.random.rand(ntrain, 1)\ny_train_vis = g_sigma(x_train_vis)\n\nplt.figure()\nplt.plot(xx, true_func, \"k-\", linewidth=2, label=\"True function\")\nplt.plot(x_test_vis, y_test_vis, \"go\", markerfacecolor=\"g\",\n         linestyle=\"\", label=\"Test (noisy)\")\nplt.plot(x_test_vis, test_true_vis, \"rx\", markersize=6,\n         linestyle=\"\", label=\"Test (true)\")\nplt.plot(x_train_vis, y_train_vis, \"bo\", markerfacecolor=\"b\",\n         linestyle=\"\", label=\"Training data\")\nstyles = [\"b-\", \"r--\", \"g-.\", \"m:\", \"c-.\"]\nplt.legend(loc=\"upper left\")\nplt.title(\"True Functions of Sine Basis\")\nplt.show()\n\nplt.figure()\nfor ki, k in enumerate(bases):\n    ks = np.arange(1, k + 1)\n    Xtrain_vis = sin_matrix(x_train_vis, ks)\n    wvis, *_ = np.linalg.lstsq(Xtrain_vis, y_train_vis, rcond=None)\n\n    Xvis = sin_matrix(xx, ks)\n    y_vis = Xvis @ wvis\n\n    plt.plot(xx, y_vis, styles[ki], linewidth=2,\n             label=f\"Sine basis k={k}\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend(loc=\"upper left\")\nplt.title(\"Sine Basis Fits: k=2,5,10,14,18\")\nplt.show()\n\n# Monte Carlo\nfor run in range(runs):\n    x_train = np.random.rand(ntrain, 1)\n    y_train = g_sigma(x_train)\n    x_test = np.random.rand(ntest, 1)\n    y_test = g_sigma(x_test)\n\n    for ki, k in enumerate(bases):\n        ks = np.arange(1, k + 1)\n\n        Xtrain = sin_matrix(x_train, ks)\n        w, *_ = np.linalg.lstsq(Xtrain, y_train, rcond=None)\n        ytrain_fit = Xtrain @ w\n        mse_train[run, ki] = np.mean((y_train - ytrain_fit) ** 2)\n\n        Xtest = sin_matrix(x_test, ks)\n        ytest_fit = Xtest @ w\n        mse_all[run, ki] = np.mean((y_test - ytest_fit) ** 2)\n\navg_train = mse_train.mean(axis=0)\navg_mse = mse_all.mean(axis=0)\n\nplt.figure()\nplt.plot(bases, np.log(avg_train), \"bo-\", linewidth=2,\n         markerfacecolor=\"b\", label=\"Train MSE\")\nplt.plot(bases, np.log(avg_mse), \"ro-\", linewidth=2,\n         markerfacecolor=\"r\", label=\"Test MSE\")\nplt.xlabel(\"Basis Dimension k\")\nplt.ylabel(\"ln(MSE)\")\nplt.title(\"Train (blue) and Test (red) Error vs Sine Basis Dimension\")\nplt.legend(loc=\"upper left\")\nplt.grid(True)\nplt.show()",
      "block_group": "44849eff1af7416d84e40c32aae975eb",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "is_collapsed": false,
        "formattedRanges": [],
        "deepnote_app_block_visible": false,
        "cell_id": "0e4a605e23db45a4a87a0eea8caae8e5",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Filtered Boston housing and kernels",
      "block_group": "b3e168fad0f94671a64c21ee67c6594f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "is_collapsed": false,
        "formattedRanges": [],
        "deepnote_app_block_visible": false,
        "cell_id": "ef4b3e7c37654e5cadf8a3ca082ebd66",
        "deepnote_cell_type": "text-cell-h3"
      },
      "source": "### Q4 Baseline versus full linear regression",
      "block_group": "b1257a3c481d40cabcc5358abd5136fe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "8cc6f35ca72f4ac99f0574dfbed210f6",
        "deepnote_cell_type": "code"
      },
      "source": "# Hyperparameter grids\ngamma_vals = 2.0 ** np.arange(-40, -25)        # 15 gamma values\nsigma_vals = 2.0 ** np.arange(7, 13.5, 0.5)    # 13 sigma values\n\n# Load data\nT = pd.read_csv(\"/datasets/t1cw-data/Boston-filtered.txt\", sep=None, engine=\"python\")\nX = T.iloc[:, :-1].to_numpy()\ny = T.iloc[:, -1].to_numpy()\nn, d = X.shape\nattribute_names = T.columns[:-1].to_list()\n\nruns = 20\n\nmse_krr_train = np.zeros(runs)\nmse_krr_test = np.zeros(runs)\nbest_gamma_all = np.zeros(runs)\nbest_sigma_all = np.zeros(runs)\n\nmse_naive_train = np.zeros(runs)\nmse_naive_test = np.zeros(runs)\n\nmse_linear_train = np.zeros((runs, d))\nmse_linear_test = np.zeros((runs, d))\n\nmse_linear_all_train = np.zeros(runs)\nmse_linear_all_test = np.zeros(runs)\n\nfor r in range(runs):\n    idx = np.random.permutation(n)\n    ntrain = int(round(2 * n / 3))\n    train_idx = idx[:ntrain]\n    test_idx = idx[ntrain:]\n\n    X_train = X[train_idx, :]\n    y_train = y[train_idx]\n    X_test = X[test_idx, :]\n    y_test = y[test_idx]\n",
      "block_group": "8026d9e35ceb419cb889c226fe16c214",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "is_collapsed": false,
        "formattedRanges": [],
        "deepnote_app_block_visible": false,
        "cell_id": "086fd15331f1453aa80b2425c8c4074e",
        "deepnote_cell_type": "text-cell-h3"
      },
      "source": "### (a) Naive regression",
      "block_group": "210633aa01934e5eb6db5a0ff9dd7faf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "ed681834bd1c48cb841f6118fa7dd530",
        "deepnote_cell_type": "code"
      },
      "source": "    w_const = y_train.mean()\n    ytrain_fit = np.full_like(y_train, w_const, dtype=float)\n    ytest_fit = np.full_like(y_test, w_const, dtype=float)\n    mse_naive_train[r] = np.mean((y_train - ytrain_fit) ** 2)\n    mse_naive_test[r] = np.mean((y_test - ytest_fit) ** 2)\n\nprint(f\"{'Naive Regression':40s} \"\n      f\"{naive_train_mean:6.2f} ± {naive_train_std:4.2f} \"\n      f\"{naive_test_mean:6.2f} ± {naive_test_std:4.2f}\")\n",
      "block_group": "25b8703a0a3d486ea4535d2af4deac80",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "is_collapsed": false,
        "formattedRanges": [],
        "deepnote_app_block_visible": false,
        "cell_id": "5765112f7d6148d5a6ae20c6d90940ca",
        "deepnote_cell_type": "text-cell-h3"
      },
      "source": "### (c) Single‑attribute linear + bias",
      "block_group": "a669fa85788d4631972c815a46b62860"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "29d8e5004e3841e994353d5f67bdf163",
        "deepnote_cell_type": "code"
      },
      "source": "for j in range(d):\n        xj_train = X_train[:, j:j+1]\n        xj_test = X_test[:, j:j+1]\n\n        Xj_train = np.hstack([xj_train, np.ones((ntrain, 1))])\n        Xj_test = np.hstack([xj_test, np.ones((xj_test.shape[0], 1))])\n\n        wj, *_ = np.linalg.lstsq(Xj_train, y_train, rcond=None)\n        ytrain_fit = Xj_train @ wj\n        ytest_fit = Xj_test @ wj\n\n        mse_linear_train[r, j] = np.mean((y_train - ytrain_fit) ** 2)\n        mse_linear_test[r, j] = np.mean((y_test - ytest_fit) ** 2)\n\n\nfor j, name in enumerate(attribute_names):\n    label = f\"Linear Regression ({name})\"\n    print(f\"{label:40s} \"\n          f\"{linear_train_mean[j]:6.2f} ± {linear_train_std[j]:4.2f} \"\n          f\"{linear_test_mean[j]:6.2f} ± {linear_test_std[j]:4.2f}\")",
      "block_group": "27ff64e7f9234e92a120adc7cea3638d",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "is_collapsed": false,
        "formattedRanges": [],
        "deepnote_app_block_visible": false,
        "cell_id": "6478585d704e48efa8e8e650c6d687f8",
        "deepnote_cell_type": "text-cell-h3"
      },
      "source": "### (d) Full linear regression with bias",
      "block_group": "9d776d3d568d4ff7a4bfb0957df39f6a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "110714c41e494bceb8e40780d81fb8a5",
        "deepnote_cell_type": "code"
      },
      "source": "for r in range(runs):\n    idx = np.random.permutation(n)\n    ntrain = int(round(2 * n / 3))\n    train_idx = idx[:ntrain]\n    test_idx = idx[ntrain:]\n\n    Xaug = np.hstack([X, np.ones((n, 1))])\n    Xtrain_aug = Xaug[train_idx, :]\n    Xtest_aug = Xaug[test_idx, :]\n\n    w_full, *_ = np.linalg.lstsq(Xtrain_aug, y_train, rcond=None)\n    ytrain_fit = Xtrain_aug @ w_full\n    ytest_fit = Xtest_aug @ w_full\n\n    mse_linear_all_train[r] = np.mean((y_train - ytrain_fit) ** 2)\n    mse_linear_all_test[r] = np.mean((y_test - ytest_fit) ** 2)\n\nprint(f\"{'Linear Regression (all attributes)':40s} \"\n      f\"{linear_all_train_mean:6.2f} ± {linear_all_train_std:4.2f} \"\n      f\"{linear_all_test_mean:6.2f} ± {linear_all_test_std:4.2f}\")",
      "block_group": "65cab41616974930acb4f3e935b045dc",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "76cafbe854694bedb6191898af1c94ca",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Q5. Kernelised ridge regression",
      "block_group": "a78b6a2e05a44f55b1ffbfc14677fbef"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "90bee3d3b7b043edb23d4907603709a0",
        "deepnote_cell_type": "text-cell-h3"
      },
      "source": "### Kernel function",
      "block_group": "cadd4bb715c64be8a7304cbfeba88d21"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "2cc62cf9cbbe49f9861df830af545567",
        "deepnote_cell_type": "code"
      },
      "source": "def kernel_func(X1, X2, sigma):\n    X1 = np.asarray(X1)\n    X2 = np.asarray(X2)\n    D2 = cdist(X1, X2, metric=\"euclidean\") ** 2\n    K = np.exp(-D2 / (2.0 * sigma ** 2))\n    return K",
      "block_group": "d6b2547fdd214e0f8d2c59b10d144395",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "1fc47f99af384152ba50a5604dcd59a1",
        "deepnote_cell_type": "code"
      },
      "source": "# Hyperparameter grids\ngamma_vals = 2.0 ** np.arange(-40, -25)   # 15 gamma values\nsigma_vals = 2.0 ** np.arange(7, 13.5, 0.5)  # 13 sigma values\n\nruns = 20\ntrain_mse_all = np.zeros(runs)\ntest_mse_all = np.zeros(runs)\nbest_gamma_all = np.zeros(runs)\nbest_sigma_all = np.zeros(runs)\n\n# For plotting CV error for the first run\ncv_error_grid = None\n\n# Load data\nT = pd.read_csv(\"/datasets/t1cw-data/Boston-filtered.txt\", sep=None, engine=\"python\")\nX = T.iloc[:, :-1].to_numpy()\ny = T.iloc[:, -1].to_numpy()\nn = y.shape[0]\n\nfor run in range(runs):\n    idx = np.random.permutation(n)\n    ntrain = int(round(2 * n / 3))\n    train_idx = idx[:ntrain]\n    test_idx = idx[ntrain:]\n\n    X_train = X[train_idx, :]\n    y_train = y[train_idx]\n    X_test = X[test_idx, :]\n    y_test = y[test_idx]",
      "block_group": "e709491d0aa54a93a47e1cf82b52d9b6",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "ea52a70b7d424ff2bd793fa4dd712c9e",
        "deepnote_cell_type": "text-cell-h3"
      },
      "source": "### (a) kFold CV",
      "block_group": "27bb4741a34446928a9db339560f7d98"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "66dca9371ea5459081a43a1e32f230f4",
        "deepnote_cell_type": "code"
      },
      "source": "    # --- Cross‑validation over gamma/sigma grid ---\n    kf = KFold(n_splits=5, shuffle=True, random_state=run)\n    mean_cv_error = np.zeros((len(gamma_vals), len(sigma_vals)))\n\n    for gi, gamma in enumerate(gamma_vals):\n        for si, sigma in enumerate(sigma_vals):\n            mse_fold = []\n\n            for train_fold_idx, val_fold_idx in kf.split(X_train):\n                Xtr = X_train[train_fold_idx]\n                ytr = y_train[train_fold_idx]\n                Xval = X_train[val_fold_idx]\n                yval = y_train[val_fold_idx]\n\n                Ktr = kernel_func(Xtr, Xtr, sigma)\n                Kval = kernel_func(Xval, Xtr, sigma)\n\n                # (K + gamma * n * I)^{-1} y\n                n_tr = ytr.shape[0]\n                alpha = np.linalg.solve(\n                    Ktr + gamma * n_tr * np.eye(Ktr.shape[0]),\n                    ytr,\n                )\n                yval_fit = Kval @ alpha\n                mse_fold.append(np.mean((yval - yval_fit) ** 2))\n\n            mean_cv_error[gi, si] = np.mean(mse_fold)",
      "block_group": "e75966952b104f52a7b7edd0eeda132d",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "692b8071af5b4f06b5dc9c2766087210",
        "deepnote_cell_type": "text-cell-h3"
      },
      "source": "### (b) Cross-validation error",
      "block_group": "3ad0567ca71e481e8ff30d3d1af14958"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "35a924327439428fa414fb600bbbb306",
        "deepnote_cell_type": "code"
      },
      "source": "    if run == 0:\n        cv_error_grid = mean_cv_error.copy()\n        \nplt.figure()\nplt.imshow(np.log10(cv_error_grid),\n           aspect=\"auto\",\n           origin=\"lower\")\nplt.colorbar(label=\"log10(CV MSE)\")\nplt.xlabel(\"sigma index\")\nplt.ylabel(\"gamma index\")\nplt.title(\"Log10 Cross‑Validation Error Surface\")\n\nxticks = np.arange(len(sigma_vals))\nyticks = np.arange(len(gamma_vals))\nplt.xticks(xticks, [f\"{np.log2(s):.1f}\" for s in sigma_vals], rotation=45)\nplt.yticks(yticks, [f\"{int(np.log2(g))}\" for g in gamma_vals])\nplt.tight_layout()\nplt.show()",
      "block_group": "ac5a8d8d12f14055b27fa0bbec169516",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "8441a7a9d2df4378b7234567cf074f90",
        "deepnote_cell_type": "text-cell-h3"
      },
      "source": "### (c) Best gamma, sigma and final KRR fit ",
      "block_group": "5a01a8a997564f4d94517e7ff2f1150f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "ab3664cd8b234fffb9d01e7370ea5410",
        "deepnote_cell_type": "code"
      },
      "source": "    ind = np.argmin(mean_cv_error)\n    best_g_idx, best_s_idx = np.unravel_index(ind, mean_cv_error.shape)\n    best_gamma = gamma_vals[best_g_idx]\n    best_sigma = sigma_vals[best_s_idx]\n    best_gamma_all[r] = best_gamma\n    best_sigma_all[r] = best_sigma\n\n    Ktr = kernel_func(X_train, X_train, best_sigma)\n    n_tr = y_train.shape[0]\n    alpha = np.linalg.solve(\n        Ktr + best_gamma * n_tr * np.eye(Ktr.shape[0]),\n        y_train,\n    )\n    Ktest = kernel_func(X_test, X_train, best_sigma)\n    y_train_fit = Ktr @ alpha\n    y_test_fit = Ktest @ alpha\n\n    mse_krr_train[r] = np.mean((y_train - y_train_fit) ** 2)\n    mse_krr_test[r] = np.mean((y_test - y_test_fit) ** 2)",
      "block_group": "17fdf637ed214090aadd4c465a14b6a4",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "dfad8390faac45869b82ed347a25bd06",
        "deepnote_cell_type": "text-cell-h3"
      },
      "source": "### (d) Aggregate statistics",
      "block_group": "ab68adef9fee49b48a5b9441c8e92fd7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "873bcda2d6cc4a39a5052080f37f212a",
        "deepnote_cell_type": "code"
      },
      "source": "naive_train_mean = mse_naive_train.mean()\nnaive_test_mean = mse_naive_test.mean()\nnaive_train_std = mse_naive_train.std()\nnaive_test_std = mse_naive_test.std()\n\nlinear_train_mean = mse_linear_train.mean(axis=0)\nlinear_test_mean = mse_linear_test.mean(axis=0)\nlinear_train_std = mse_linear_train.std(axis=0)\nlinear_test_std = mse_linear_test.std(axis=0)\n\nlinear_all_train_mean = mse_linear_all_train.mean()\nlinear_all_test_mean = mse_linear_all_test.mean()\nlinear_all_train_std = mse_linear_all_train.std()\nlinear_all_test_std = mse_linear_all_test.std()\n\nkrr_train_mean = mse_krr_train.mean()\nkrr_test_mean = mse_krr_test.mean()\nkrr_train_std = mse_krr_train.std()\nkrr_test_std = mse_krr_test.std()\n\nprint(f\"{'Method':40s} {'MSE train':>20s} {'MSE test':>20s}\")\nprint(\"-\" * 80)\n\nprint(f\"{'Naive Regression':40s} \"\n      f\"{naive_train_mean:6.2f} ± {naive_train_std:4.2f} \"\n      f\"{naive_test_mean:6.2f} ± {naive_test_std:4.2f}\")\n\nfor j, name in enumerate(attribute_names):\n    label = f\"Linear Regression ({name})\"\n    print(f\"{label:40s} \"\n          f\"{linear_train_mean[j]:6.2f} ± {linear_train_std[j]:4.2f} \"\n          f\"{linear_test_mean[j]:6.2f} ± {linear_test_std[j]:4.2f}\")\n\nprint(f\"{'Linear Regression (all attributes)':40s} \"\n      f\"{linear_all_train_mean:6.2f} ± {linear_all_train_std:4.2f} \"\n      f\"{linear_all_test_mean:6.2f} ± {linear_all_test_std:4.2f}\")\n\nprint(f\"{'Kernel Ridge Regression':40s} \"\n      f\"{krr_train_mean:6.2f} ± {krr_train_std:4.2f} \"\n      f\"{krr_test_mean:6.2f} ± {krr_test_std:4.2f}\")",
      "block_group": "f9e7f62475a941f793a446404c57ddca",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "176feb9dea464f48aa518b394afe0670",
        "deepnote_cell_type": "text-cell-h1"
      },
      "source": "# Part II",
      "block_group": "039295f80b6240999b46e039f68b01f0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "802f206f3e274aac8a32889a6039f782",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## k-NN",
      "block_group": "03215d6bb950420ca3d366d8ebc9fbe4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "7fdf2a3523fa4a78ada423375e9a0f92",
        "deepnote_cell_type": "code"
      },
      "source": "# helper functions\n\ndef sample_pH(N):\n    centers = np.random.rand(N, 2)\n    labels = np.random.randint(0, 2, size=N)\n    return centers, labels\n\n\ndef voted_center(x, centers, labels, v):\n    x = np.asarray(x).reshape(1, -1)\n    centers = np.asarray(centers)\n    labels = np.asarray(labels).reshape(-1)\n    dists = np.sqrt(np.sum((centers - x) ** 2, axis=1))\n    idx = np.argsort(dists)\n    nearest_labels = labels[idx[:v]]\n    count0 = np.sum(nearest_labels == 0)\n    count1 = np.sum(nearest_labels == 1)\n    if count0 > count1:\n        return 0\n    elif count1 > count0:\n        return 1\n    else:\n        return np.nan",
      "block_group": "a766001775774c709a66fc878851fb9b",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "is_collapsed": false,
        "formattedRanges": [],
        "deepnote_app_block_visible": false,
        "cell_id": "e18e39e223db4b0c8a218d46dbdca699",
        "deepnote_cell_type": "text-cell-h3"
      },
      "source": "### Q6. Visualisation of a voted‑center hypothesis",
      "block_group": "76823496225d4fbe84350af96f95d208"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "8dc3357c9f094704a2e4897da3a10ace",
        "deepnote_cell_type": "code"
      },
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\nN = 100       # number of centers\nv = 3         # number of neighbors in the vote\ngridRes = 200 # grid resolution\n\ncenters, labels = sample_pH(N)\n\nxg, yg = np.meshgrid(\n    np.linspace(0.0, 1.0, gridRes),\n    np.linspace(0.0, 1.0, gridRes),\n)\nhg = np.full_like(xg, fill_value=np.nan, dtype=float)\n\nfor i in range(xg.size):\n    x = np.array([xg.flat[i], yg.flat[i]])\n\n    dists = np.sqrt(np.sum((centers - x) ** 2, axis=1))\n    idx = np.argsort(dists)\n    nearest_labels = labels[idx[:v]]\n\n    c0 = np.sum(nearest_labels == 0)\n    c1 = np.sum(nearest_labels == 1)\n    if c0 > c1:\n        hg.flat[i] = 0\n    elif c1 > c0:\n        hg.flat[i] = 1\n    else:\n        hg.flat[i] = np.nan\n\nhg_plot = hg + 1  # 0→1, 1→2\nhg_plot[np.isnan(hg)] = 3  # ambiguous→3\n\nplt.figure()\nplt.imshow(\n    hg_plot,\n    extent=(0, 1, 0, 1),\n    origin=\"lower\",\n    aspect=\"equal\",\n)\ncmap = np.array([\n    [255, 255, 255],  # class 0 -> white\n    [64, 224, 208],   # class 1 -> turquoise\n    [128, 128, 128],  # ambiguous -> gray\n]) / 255.0\nplt.colormaps.register(cmap=plt.matplotlib.colors.ListedColormap(cmap))\nplt.set_cmap(\"h_{S,v} sampled from p_H\")\nplt.clim(1, 3)\nplt.colorbar()\n\n# Plot centers\nplt.plot(centers[labels == 0, 0], centers[labels == 0, 1],\n         \"ro\", markerfacecolor=\"r\", markersize=6, label=\"Class 0\")\nplt.plot(centers[labels == 1, 0], centers[labels == 1, 1],\n         \"bo\", markerfacecolor=\"b\", markersize=6, label=\"Class 1\")\n\nplt.legend()\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Visualisation of a voted-center hypothesis h_{S,v} sampled from p_H\")\nplt.show()",
      "block_group": "fcab55187dd943e1864bcf0d1f868067",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "is_collapsed": false,
        "formattedRanges": [],
        "deepnote_app_block_visible": false,
        "cell_id": "180d216b892d430c9d90ce32b28fcf36",
        "deepnote_cell_type": "text-cell-h3"
      },
      "source": "### Q7. Protocol A (estimated generalization error as a function of k)",
      "block_group": "5a2c50b53fac4acf960a7eadbe000593"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "15c085c3a3f54c6387ebfc24df01e7eb",
        "deepnote_cell_type": "code"
      },
      "source": "K_max = 49\nruns = 100\nn_train = 4000\nn_test = 1000\n\nerrors = np.zeros((K_max, runs))\n\nfor k in range(1, K_max + 1):\n    for run in range(runs):\n        centers, labels = sample_pH(100)\n\n        X_train = np.random.rand(n_train, 2)\n        Y_train = np.zeros(n_train, dtype=int)\n        for i in range(n_train):\n            if np.random.rand() < 0.8:\n                y_val = voted_center(X_train[i, :], centers, labels, 3)\n                if np.isnan(y_val):\n                    y_val = np.random.randint(0, 2)\n                Y_train[i] = int(y_val)\n            else:\n                Y_train[i] = np.random.randint(0, 2)\n\n        X_test = np.random.rand(n_test, 2)\n        Y_test = np.zeros(n_test, dtype=int)\n        for i in range(n_test):\n            if np.random.rand() < 0.8:\n                y_val = voted_center(X_test[i, :], centers, labels, 3)\n                if np.isnan(y_val):\n                    y_val = np.random.randint(0, 2)\n                Y_test[i] = int(y_val)\n            else:\n                Y_test[i] = np.random.randint(0, 2)\n\n        knn = KNeighborsClassifier(n_neighbors=k)\n        knn.fit(X_train, Y_train)\n        pred = knn.predict(X_test)\n\n        errors[k - 1, run] = np.mean(pred != Y_test)\n\nmean_errors = errors.mean(axis=1)\n\nplt.figure()\nplt.plot(np.arange(1, K_max + 1), mean_errors, \"-o\")\nplt.xlabel(\"k\")\nplt.ylabel(\"Estimated Generalisation Error\")\nplt.title(\"k‑NN Generalisation Error vs. k (Protocol A)\")\nplt.grid(True)\nplt.show()",
      "block_group": "331867c5a44d4583854a07430f3eab4c",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "is_collapsed": false,
        "formattedRanges": [],
        "deepnote_app_block_visible": false,
        "cell_id": "a6ca3190fc3144f4851efe44c0c8721b",
        "deepnote_cell_type": "text-cell-h3"
      },
      "source": "### Q8. Protocol B (optimal k as a function of m)",
      "block_group": "ee43635fdb3a43748e06e803049d35f8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "fcddaecc61a64892b92582dd8ee819bc",
        "deepnote_cell_type": "code"
      },
      "source": "M_vals = np.array([100, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000])\nK_max = 49\nruns = 100\nn_test = 1000\n\nmean_optimal = np.zeros(len(M_vals))\n\nfor m_idx, m in enumerate(M_vals):\n    optimal = np.zeros(runs, dtype=int)\n\n    for run in range(runs):\n        centers, labels = sample_pH(100)\n\n        X_train = np.random.rand(m, 2)\n        Y_train = np.zeros(m, dtype=int)\n        for i in range(m):\n            y_val = voted_center(X_train[i, :], centers, labels, 3)\n            if np.isnan(y_val):\n                y_val = np.random.randint(0, 2)\n\n            if np.random.rand() < 0.8:\n                Y_train[i] = int(y_val)\n            else:\n                Y_train[i] = np.random.randint(0, 2)\n\n        X_test = np.random.rand(n_test, 2)\n        Y_test = np.zeros(n_test, dtype=int)\n        for i in range(n_test):\n            y_val = voted_center(X_test[i, :], centers, labels, 3)\n            if np.isnan(y_val):\n                y_val = np.random.randint(0, 2)\n\n            if np.random.rand() < 0.8:\n                Y_test[i] = int(y_val)\n            else:\n                Y_test[i] = np.random.randint(0, 2)\n\n        errs = np.zeros(K_max)\n        for k in range(1, K_max + 1):\n            knn = KNeighborsClassifier(n_neighbors=k)\n            knn.fit(X_train, Y_train)\n            pred = knn.predict(X_test)\n            errs[k - 1] = np.mean(pred != Y_test)\n\n        best_k = np.argmin(errs) + 1\n        optimal[run] = best_k\n\n    mean_optimal[m_idx] = optimal.mean()\n\nplt.figure()\nplt.plot(M_vals, mean_optimal, \"-o\")\nplt.xlabel(\"Training set size m\")\nplt.ylabel(\"Mean estimated optimal k\")\nplt.title(\"Protocol B: m vs mean optimal k\")\nplt.grid(True)\nplt.show()",
      "block_group": "a41f4199dc6b4c75bc9eaeadfc0af3de",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "f37385ebaf4541898a7d85fa5a82e195",
        "deepnote_cell_type": "text-cell-h1"
      },
      "source": "# Part III",
      "block_group": "eedec814f21c4ca1a991c6c64c9436a9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "is_collapsed": false,
        "formattedRanges": [],
        "deepnote_app_block_visible": false,
        "cell_id": "e522f0982c3a413582f7a2ca695d5311",
        "deepnote_cell_type": "text-cell-h2"
      },
      "source": "## Q11. Gaussian elimination mod 2 (whack‑a‑mole)",
      "block_group": "f0a8bcec7b9c46d2a12091161ee5e352"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "c509a61194d240fcaaaa7c8d1ea208f7",
        "deepnote_cell_type": "code"
      },
      "source": "A = np.array([\n    [1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    [1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    [0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n    [1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n    [0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n    [0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n    [0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n    [0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0],\n    [0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0],\n    [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0],\n    [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1],\n    [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1],\n    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1],\n], dtype=int)\n\nb = np.array([0, 1, 0, 0, 1, 1, 1, 0,\n              0, 0, 0, 1, 1, 1, 0, 1], dtype=int).reshape(-1, 1)\n\nAb = np.hstack([A, b])\nm, n_tot = Ab.shape\nn = n_tot - 1  # number of variables\n\n# Forward elimination mod 2\nrow = 0\nfor col in range(n):\n    # pivot: find row with a 1 in this column at or below current row\n    pivot_rows = np.where(Ab[row:, col] % 2 != 0)[0]\n    if pivot_rows.size == 0:\n        continue\n    p = row + pivot_rows[0]\n\n    if p != row:\n        Ab[[row, p], :] = Ab[[p, row], :]\n\n    # eliminate below\n    for r in range(row + 1, m):\n        if Ab[r, col] % 2 != 0:\n            Ab[r, :] = (Ab[r, :] + Ab[row, :]) % 2\n\n    row += 1\n    if row == m:\n        break\n\n# Back substitution mod 2\nx = np.zeros((n, 1), dtype=int)\n\nfor i in range(m - 1, -1, -1):\n    pivot_cols = np.where(Ab[i, :n] % 2 != 0)[0]\n    if pivot_cols.size == 0:\n        continue\n    j = pivot_cols[0]\n    rhs = (Ab[i, n] - (Ab[i, j+1:n] @ x[j+1:n, 0]) % 2) % 2\n    x[j, 0] = rhs\n\nprint(\"Solution vector x (which moles to press):\")\nprint(x.reshape(-1))",
      "block_group": "3bee2a507505436b9acb610b8d7c21b5",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=cb182644-878e-48cb-992b-68a78a5afe3d' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_full_width": true,
    "deepnote_notebook_id": "45ce755f52a543aaa7de90ced3c04f4d"
  }
}